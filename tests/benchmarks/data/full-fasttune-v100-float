nohup: ignoring input
------- Single CUDA FLOAT FastTune NN -------
1_2x2^1  &  0.001 & 0.001 & 0.000 & 47.523
4_2x2^1  &  0.006 & 0.006 & 0.000 & 61.416
16_2x2^1  &  0.023 & 0.025 & 0.000 & 60.025
64_2x2^1  &  0.094 & 0.098 & 0.002 & 58.922
256_2x2^1  &  0.377 & 0.384 & 0.007 & 58.781
1024_2x2^1  &  1.455 & 1.429 & 0.026 & 54.729
1_2x2^2  &  0.003 & 0.003 & 0.000 & 43.698
4_2x2^2  &  0.013 & 0.013 & 0.000 & 46.248
16_2x2^2  &  0.054 & 0.053 & 0.001 & 47.315
64_2x2^2  &  0.215 & 0.215 & 0.005 & 47.121
256_2x2^2  &  0.861 & 0.860 & 0.018 & 46.955
1024_2x2^2  &  3.137 & 3.137 & 0.068 & 46.074
1_2x2^3  &  0.007 & 0.007 & 0.000 & 38.962
4_2x2^3  &  0.028 & 0.026 & 0.001 & 38.671
16_2x2^3  &  0.114 & 0.111 & 0.003 & 40.673
64_2x2^3  &  0.444 & 0.448 & 0.011 & 41.107
256_2x2^3  &  1.846 & 1.690 & 0.023 & 74.639
1024_2x2^3  &  6.443 & 6.444 & 0.170 & 37.848
1_2x2^4  &  0.013 & 0.014 & 0.000 & 35.235
4_2x2^4  &  0.056 & 0.058 & 0.002 & 37.113
16_2x2^4  &  0.221 & 0.225 & 0.006 & 37.161
64_2x2^4  &  0.930 & 0.889 & 0.025 & 35.566
256_2x2^4  &  3.556 & 3.721 & 0.088 & 42.225
1024_2x2^4  &  13.061 & 12.997 & 0.406 & 32.000
1_2x2^5  &  0.029 & 0.027 & 0.001 & 30.563
4_2x2^5  &  0.104 & 0.114 & 0.003 & 33.828
16_2x2^5  &  0.444 & 0.467 & 0.014 & 34.185
64_2x2^5  &  1.835 & 1.869 & 0.054 & 34.654
256_2x2^5  &  5.925 & 7.240 & 0.218 & 33.245
1024_2x2^5  &  26.126 & 26.226 & 0.832 & 31.525
1_2x2^6  &  0.058 & 0.278 & 0.002 & 148.436
4_2x2^6  &  0.225 & 1.091 & 0.007 & 149.251
16_2x2^6  &  0.916 & 4.366 & 0.028 & 154.020
64_2x2^6  &  3.795 & 18.450 & 0.117 & 158.241
256_2x2^6  &  14.491 & 69.779 & 0.458 & 152.202
1024_2x2^6  &  57.744 & 240.000 & 1.873 & 128.157
1_2x2^7  &  0.116 & 0.343 & 0.004 & 86.372
4_2x2^7  &  0.478 & 1.443 & 0.015 & 95.883
16_2x2^7  &  1.801 & 5.831 & 0.057 & 101.510
64_2x2^7  &  7.594 & 23.830 & 0.243 & 97.874
256_2x2^7  &  30.583 & 92.371 & 0.941 & 98.127
1024_2x2^7  &  83.549 & 257.842 & 3.963 & 65.060
1_2x2^8  &  0.224 & 0.833 & 0.008 & 111.004
4_2x2^8  &  0.955 & 3.479 & 0.025 & 138.918
16_2x2^8  &  3.678 & 13.333 & 0.124 & 107.136
64_2x2^8  &  14.503 & 54.486 & 0.489 & 111.530
256_2x2^8  &  58.348 & 208.980 & 2.066 & 101.172
1024_2x2^8  &  106.532 & 640.156 & 8.211 & 77.964
1_2x2^9  &  0.470 & 1.856 & 0.017 & 111.526
4_2x2^9  &  1.800 & 7.270 & 0.051 & 142.316
16_2x2^9  &  7.579 & 29.388 & 0.263 & 111.792
64_2x2^9  &  30.557 & 123.871 & 1.024 & 120.964
256_2x2^9  &  83.787 & 434.845 & 3.230 & 134.641
1024_2x2^9  &  140.702 & 980.589 & 11.403 & 85.997
1_2x2^10  &  0.957 & 4.257 & 0.034 & 124.030
4_2x2^10  &  3.819 & 16.661 & 0.135 & 123.756
16_2x2^10  &  15.535 & 69.565 & 0.536 & 129.759
64_2x2^10  &  57.919 & 266.753 & 2.067 & 129.056
256_2x2^10  &  226.533 & 793.991 & 8.759 & 90.649
1024_2x2^10  &  421.833 & 1153.803 & 25.084 & 45.997
1_2x2^11  &  1.934 & 9.167 & 0.056 & 164.918
4_2x2^11  &  7.893 & 37.459 & 0.274 & 136.784
16_2x2^11  &  28.795 & 142.222 & 0.864 & 164.617
64_2x2^11  &  118.071 & 502.857 & 3.352 & 150.000
256_2x2^11  &  341.867 & 1099.094 & 15.195 & 72.334
1024_2x2^11  &  337.874 & 1403.682 & 46.073 & 30.466
1_2x2^12  &  3.879 & 19.586 & 0.118 & 166.573
4_2x2^12  &  14.825 & 78.392 & 0.503 & 155.744
16_2x2^12  &  61.932 & 289.897 & 1.776 & 163.209
64_2x2^12  &  226.716 & 896.934 & 8.871 & 101.106
256_2x2^12  &  422.631 & 1250.787 & 34.804 & 35.938
1024_2x2^12  &  371.028 & 1603.695 & 51.991 & 30.845
1_2x2^13  &  7.482 & 30.588 & 0.287 & 106.674
4_2x2^13  &  28.591 & 124.179 & 0.862 & 144.003
16_2x2^13  &  124.878 & 475.429 & 4.670 & 101.813
64_2x2^13  &  343.979 & 989.855 & 17.555 & 56.386
256_2x2^13  &  335.315 & 1278.511 & 46.282 & 27.624
1024_2x2^13  &  394.211 & 1590.086 & 53.016 & 29.993
1_2x2^14  &  15.448 & 63.099 & 0.581 & 108.592
4_2x2^14  &  56.530 & 265.481 & 2.253 & 117.831
16_2x2^14  &  237.745 & 843.294 & 6.864 & 122.849
64_2x2^14  &  461.707 & 1184.793 & 36.840 & 32.161
256_2x2^14  &  378.134 & 1573.224 & 52.140 & 30.173
1024_2x2^14  &  405.688 & 1739.674 & 53.504 & 32.515
1_2x2^15  &  30.236 & 140.146 & 1.145 & 122.437
4_2x2^15  &  124.473 & 533.333 & 4.748 & 112.324
16_2x2^15  &  382.565 & 1181.538 & 18.923 & 62.439
64_2x2^15  &  346.629 & 1461.118 & 50.511 & 28.927
256_2x2^15  &  395.176 & 1763.617 & 53.000 & 33.276
1024_2x2^15  &  410.130 & 1870.320 & 53.466 & 34.981
1_2x2^16  &  61.502 & 305.672 & 2.238 & 136.584
4_2x2^16  &  242.009 & 925.650 & 9.861 & 93.866
16_2x2^16  &  467.447 & 1270.078 & 34.003 & 37.352
64_2x2^16  &  377.620 & 1672.904 & 52.554 & 31.832
256_2x2^16  &  405.262 & 1865.462 & 52.303 & 35.666
1024_2x2^16  &  412.772 & 1917.922 & 52.449 & 36.568
1_2x2^17  &  119.233 & 576.424 & 4.846 & 118.946
4_2x2^17  &  382.173 & 1256.895 & 19.119 & 65.741
16_2x2^17  &  347.725 & 1547.378 & 51.240 & 30.199
64_2x2^17  &  395.552 & 1833.627 & 53.047 & 34.566
256_2x2^17  &  410.196 & 1926.363 & 51.590 & 37.340
1024_2x2^17  &  414.237 & 1951.117 & 51.373 & 37.979
1_2x2^18  &  236.611 & 1001.739 & 9.936 & 100.821
4_2x2^18  &  427.905 & 1370.409 & 37.753 & 36.299
16_2x2^18  &  377.753 & 1750.220 & 52.679 & 33.224
64_2x2^18  &  405.879 & 1927.214 & 53.398 & 36.092
256_2x2^18  &  412.843 & 1975.629 & 51.002 & 38.736
1024_2x2^18  &  414.961 & 1988.806 & 50.721 & 39.211
1_2x2^19  &  381.490 & 1147.847 & 17.934 & 64.005
4_2x2^19  &  347.739 & 1390.956 & 50.562 & 27.510
16_2x2^19  &  395.573 & 1641.857 & 53.632 & 30.613
64_2x2^19  &  410.221 & 1745.055 & 53.748 & 32.468
256_2x2^19  &  414.331 & 1771.716 & 51.164 & 34.628
1024_2x2^19  &  415.400 & 1775.363 & 50.674 & 35.035
1_2x4^1  &  0.003 & 0.003 & 0.000 & 49.092
4_2x4^1  &  0.012 & 0.012 & 0.000 & 56.254
16_2x4^1  &  0.047 & 0.047 & 0.001 & 58.755
64_2x4^1  &  0.185 & 0.185 & 0.003 & 58.065
256_2x4^1  &  0.727 & 0.727 & 0.013 & 55.928
1024_2x4^1  &  2.909 & 2.909 & 0.053 & 55.219
1_2x4^2  &  0.010 & 0.010 & 0.000 & 44.172
4_2x4^2  &  0.039 & 0.039 & 0.001 & 45.952
16_2x4^2  &  0.152 & 0.152 & 0.002 & 76.002
64_2x4^2  &  0.645 & 0.645 & 0.013 & 49.897
256_2x4^2  &  2.449 & 2.449 & 0.051 & 48.268
1024_2x4^2  &  8.975 & 8.975 & 0.197 & 45.587
1_2x4^3  &  0.033 & 0.033 & 0.001 & 38.451
4_2x4^3  &  0.131 & 0.131 & 0.003 & 41.870
16_2x4^3  &  0.526 & 0.526 & 0.013 & 41.546
64_2x4^3  &  2.121 & 2.121 & 0.048 & 43.897
256_2x4^3  &  8.358 & 8.358 & 0.203 & 41.109
1024_2x4^3  &  29.097 & 29.097 & 0.823 & 35.335
1_2x4^4  &  0.111 & 0.111 & 0.003 & 36.632
4_2x4^4  &  0.429 & 0.429 & 0.011 & 37.279
16_2x4^4  &  1.667 & 1.667 & 0.046 & 36.270
64_2x4^4  &  6.857 & 6.857 & 0.189 & 36.367
256_2x4^4  &  27.907 & 27.907 & 0.740 & 37.727
1024_2x4^4  &  83.128 & 83.128 & 2.966 & 28.023
1_2x4^5  &  0.359 & 0.359 & 0.011 & 33.030
4_2x4^5  &  1.483 & 1.483 & 0.041 & 36.561
16_2x4^5  &  5.611 & 5.611 & 0.164 & 34.279
64_2x4^5  &  21.950 & 21.950 & 0.657 & 33.423
256_2x4^5  &  74.868 & 74.868 & 2.666 & 28.084
1024_2x4^5  &  175.587 & 175.587 & 10.862 & 16.165
1_2x4^6  &  1.230 & 1.230 & 0.040 & 31.010
4_2x4^6  &  4.701 & 4.701 & 0.153 & 30.805
16_2x4^6  &  18.806 & 18.806 & 0.598 & 31.449
64_2x4^6  &  74.667 & 74.667 & 2.426 & 30.780
256_2x4^6  &  195.728 & 195.728 & 9.663 & 20.256
1024_2x4^6  &  262.031 & 262.031 & 37.415 & 7.003
1_2x4^7  &  4.178 & 4.178 & 0.138 & 30.215
4_2x4^7  &  16.076 & 16.076 & 0.537 & 29.939
16_2x4^7  &  67.508 & 67.508 & 2.144 & 31.485
64_2x4^7  &  193.986 & 193.986 & 8.624 & 22.493
256_2x4^7  &  266.056 & 266.056 & 35.026 & 7.596
1024_2x4^7  &  300.481 & 300.481 & 91.163 & 3.296
1_2x4^8  &  14.571 & 14.571 & 0.514 & 28.338
4_2x4^8  &  58.790 & 58.790 & 1.908 & 30.819
16_2x4^8  &  180.931 & 180.931 & 6.371 & 28.400
64_2x4^8  &  263.014 & 263.014 & 30.897 & 8.513
256_2x4^8  &  303.134 & 303.134 & 90.839 & 3.337
1024_2x4^8  &  317.819 & 317.819 & 95.042 & 3.344
1_2x4^9  &  51.100 & 51.100 & 1.873 & 27.284
4_2x4^9  &  171.046 & 171.046 & 7.348 & 23.279
16_2x4^9  &  250.990 & 250.990 & 29.562 & 8.490
64_2x4^9  &  297.512 & 297.512 & 89.494 & 3.324
256_2x4^9  &  314.897 & 314.897 & 95.185 & 3.308
1024_2x4^9  &  318.694 & 318.694 & 96.345 & 3.308
1_2x4^10  &  159.844 & 159.844 & 7.032 & 22.730
4_2x4^10  &  245.581 & 245.581 & 26.130 & 9.399
16_2x4^10  &  295.451 & 295.451 & 89.230 & 3.311
64_2x4^10  &  311.642 & 311.642 & 94.139 & 3.310
256_2x4^10  &  316.773 & 316.773 & 94.445 & 3.354
1024_2x4^10  &  317.568 & 317.568 & -1.000 & -317.568
1_2x4^11  &  240.824 & 240.824 & 24.979 & 9.641
4_2x4^11  &  290.483 & 290.483 & 88.355 & 3.288
16_2x4^11  &  308.472 & 308.472 & 94.317 & 3.271
64_2x4^11  &  313.679 & 313.679 & 94.985 & 3.302
256_2x4^11  &  315.155 & 315.155 & -1.000 & -315.155
1_2x4^12  &  262.763 & 262.763 & 87.966 & 2.987
4_2x4^12  &  314.320 & 314.320 & 94.159 & 3.338
16_2x4^12  &  320.235 & 320.235 & 95.686 & 3.347
64_2x4^12  &  321.870 & 321.870 & -1.000 & -321.870
1_2x4^13  &  270.149 & 270.149 & 93.976 & 2.875
4_2x4^13  &  316.255 & 316.255 & 95.472 & 3.313
16_2x4^13  &  319.649 & 319.649 & -1.000 & -319.649
1_2x4^14  &  273.952 & 273.952 & 95.390 & 2.872
4_2x4^14  &  313.101 & 313.101 & -1.000 & -313.101
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 15 -p 2 -q 4 -r 10 -w 20 -t float --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32768] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x F_11 [2, 4] x F_12 [2, 4] x F_13 [2, 4] x F_14 [2, 4] x to produce Y[1, 1073741824]
Matmul: 1 x 1073741824 x 32768, Num KP Factors: 15
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Failed: Cuda error /home/parasail/fastkron/src/kernel_db/cuda_kernel_db.cu:240 'out of memory'
1_2x4^15  &  1.000 & 1.000 & -1.000 & -1.000
1_2x8^1  &  0.006 & 0.006 & 0.000 & 49.117
4_2x8^1  &  0.024 & 0.024 & 0.000 & 59.297
16_2x8^1  &  0.096 & 0.096 & 0.002 & 57.603
64_2x8^1  &  0.400 & 0.400 & 0.006 & 63.414
256_2x8^1  &  1.356 & 1.356 & 0.027 & 51.091
1024_2x8^1  &  5.614 & 5.614 & 0.107 & 52.391
1_2x8^2  &  0.032 & 0.032 & 0.001 & 42.285
4_2x8^2  &  0.129 & 0.129 & 0.003 & 44.503
16_2x8^2  &  0.516 & 0.516 & 0.011 & 45.774
64_2x8^2  &  2.127 & 2.127 & 0.044 & 48.213
256_2x8^2  &  8.513 & 8.513 & 0.178 & 47.796
1024_2x8^2  &  32.323 & 32.323 & 0.719 & 44.956
1_2x8^3  &  0.190 & 0.190 & 0.005 & 37.184
4_2x8^3  &  0.740 & 0.740 & 0.018 & 41.049
16_2x8^3  &  3.110 & 3.110 & 0.076 & 40.876
64_2x8^3  &  11.834 & 11.834 & 0.312 & 37.938
256_2x8^3  &  46.355 & 46.355 & 1.213 & 38.203
1024_2x8^3  &  126.811 & 126.811 & 5.027 & 25.224
1_2x8^4  &  1.258 & 1.258 & 0.036 & 35.212
4_2x8^4  &  5.030 & 5.030 & 0.134 & 37.650
16_2x8^4  &  19.322 & 19.322 & 0.524 & 36.904
64_2x8^4  &  73.913 & 73.913 & 2.107 & 35.084
256_2x8^4  &  185.034 & 185.034 & 8.320 & 22.239
1024_2x8^4  &  277.197 & 277.197 & 26.741 & 10.366
1_2x8^5  &  7.445 & 7.445 & 0.234 & 31.787
4_2x8^5  &  30.583 & 30.583 & 0.923 & 33.139
16_2x8^5  &  122.883 & 122.883 & 3.340 & 36.789
64_2x8^5  &  223.149 & 223.149 & 14.734 & 15.145
256_2x8^5  &  290.599 & 290.599 & 57.198 & 5.081
1024_2x8^5  &  322.721 & 322.721 & 147.773 & 2.184
1_2x8^6  &  50.743 & 50.743 & 1.677 & 30.253
4_2x8^6  &  174.441 & 174.441 & 6.495 & 26.857
16_2x8^6  &  262.500 & 262.500 & 25.960 & 10.112
64_2x8^6  &  309.568 & 309.568 & 99.720 & 3.104
256_2x8^6  &  326.275 & 326.275 & 150.783 & 2.164
1024_2x8^6  &  332.128 & 332.128 & 154.569 & 2.149
1_2x8^7  &  208.435 & 208.435 & 12.287 & 16.964
4_2x8^7  &  285.542 & 285.542 & 45.602 & 6.262
16_2x8^7  &  321.294 & 321.294 & 141.693 & 2.268
64_2x8^7  &  331.331 & 331.331 & 150.807 & 2.197
256_2x8^7  &  334.594 & 334.594 & 155.149 & 2.157
1_2x8^8  &  272.212 & 272.212 & 84.960 & 3.204
4_2x8^8  &  327.665 & 327.665 & 146.511 & 2.236
16_2x8^8  &  335.593 & 335.593 & 150.951 & 2.223
64_2x8^8  &  336.656 & 336.656 & 152.376 & 2.209
1_2x8^9  &  286.190 & 286.190 & 149.787 & 1.911
4_2x8^9  &  331.668 & 331.668 & 150.963 & 2.197
1_2x8^10  &  287.412 & 287.412 & 109.417 & 2.627
1_2x16^1  &  0.012 & 0.012 & 0.000 & 50.208
4_2x16^1  &  0.047 & 0.047 & 0.001 & 57.362
16_2x16^1  &  0.185 & 0.185 & 0.003 & 60.661
64_2x16^1  &  0.755 & 0.755 & 0.013 & 58.045
256_2x16^1  &  3.019 & 3.019 & 0.053 & 57.388
1024_2x16^1  &  11.228 & 11.228 & 0.213 & 52.810
1_2x16^2  &  0.118 & 0.118 & 0.003 & 43.042
4_2x16^2  &  0.489 & 0.489 & 0.010 & 49.597
16_2x16^2  &  1.837 & 1.837 & 0.039 & 46.811
64_2x16^2  &  7.502 & 7.502 & 0.161 & 46.517
256_2x16^2  &  29.378 & 29.378 & 0.651 & 45.127
1024_2x16^2  &  84.725 & 84.725 & 2.589 & 32.723
1_2x16^3  &  1.393 & 1.393 & 0.036 & 38.866
4_2x16^3  &  5.177 & 5.177 & 0.132 & 39.353
16_2x16^3  &  22.290 & 22.290 & 0.544 & 40.993
64_2x16^3  &  75.355 & 75.355 & 1.552 & 48.544
256_2x16^3  &  179.714 & 179.714 & 8.580 & 20.946
1024_2x16^3  &  261.371 & 261.371 & 30.625 & 8.534
1_2x16^4  &  17.411 & 17.411 & 0.489 & 35.584
4_2x16^4  &  67.630 & 67.630 & 1.794 & 37.696
16_2x16^4  &  200.000 & 200.000 & 7.394 & 27.049
64_2x16^4  &  281.504 & 281.504 & 29.840 & 9.434
256_2x16^4  &  321.098 & 321.098 & 112.795 & 2.847
1024_2x16^4  &  338.250 & 338.250 & 216.494 & 1.562
1_2x16^5  &  176.641 & 176.641 & 6.722 & 26.278
4_2x16^5  &  270.578 & 270.578 & 25.737 & 10.513
16_2x16^5  &  319.386 & 319.386 & 98.665 & 3.237
64_2x16^5  &  336.536 & 336.536 & 212.313 & 1.585
256_2x16^5  &  342.480 & 342.480 & 222.270 & 1.541
1024_2x16^5  &  343.762 & 343.762 & 222.856 & 1.543
1_2x16^6  &  282.102 & 282.102 & 89.941 & 3.137
4_2x16^6  &  334.852 & 334.852 & 208.468 & 1.606
16_2x16^6  &  340.949 & 340.949 & 216.240 & 1.577
64_2x16^6  &  342.188 & 342.188 & 218.904 & 1.563
1_2x16^7  &  308.009 & 308.009 & 208.620 & 1.476
4_2x16^7  &  342.394 & 342.394 & 218.366 & 1.568
1_2x32^1  &  0.025 & 0.025 & 0.000 & 50.383
4_2x32^1  &  0.098 & 0.098 & 0.002 & 58.678
16_2x32^1  &  0.377 & 0.377 & 0.007 & 56.872
64_2x32^1  &  1.538 & 1.538 & 0.026 & 58.230
256_2x32^1  &  6.038 & 6.038 & 0.106 & 56.846
1024_2x32^1  &  17.778 & 17.778 & 0.420 & 42.351
1_2x32^2  &  0.443 & 0.443 & 0.010 & 42.585
4_2x32^2  &  1.753 & 1.753 & 0.038 & 45.559
16_2x32^2  &  7.312 & 7.312 & 0.149 & 49.180
64_2x32^2  &  28.343 & 28.343 & 0.608 & 46.587
256_2x32^2  &  89.917 & 89.917 & 2.418 & 37.180
1024_2x32^2  &  185.216 & 185.216 & 9.699 & 19.096
1_2x32^3  &  10.187 & 10.187 & 0.264 & 38.555
4_2x32^3  &  39.009 & 39.009 & 0.963 & 40.491
16_2x32^3  &  128.471 & 128.471 & 3.930 & 32.686
64_2x32^3  &  232.960 & 232.960 & 15.305 & 15.222
256_2x32^3  &  314.245 & 314.245 & 64.820 & 4.848
1024_2x32^3  &  340.668 & 340.668 & 237.751 & 1.433
1_2x32^4  &  195.045 & 195.045 & 7.115 & 27.412
4_2x32^4  &  282.783 & 282.783 & 26.037 & 10.861
16_2x32^4  &  324.682 & 324.682 & 103.553 & 3.135
64_2x32^4  &  340.000 & 340.000 & 240.855 & 1.412
256_2x32^4  &  345.269 & 345.269 & 255.215 & 1.353
1024_2x32^4  &  346.292 & 346.292 & 252.485 & 1.372
1_2x32^5  &  297.152 & 297.152 & 183.883 & 1.616
4_2x32^5  &  342.630 & 342.630 & 198.255 & 1.728
16_2x32^5  &  346.359 & 346.359 & 246.113 & 1.407
1_2x32^6  &  310.868 & 310.868 & 234.037 & 1.328
1_2x64^1  &  0.047 & 0.047 & 0.001 & 48.862
4_2x64^1  &  0.185 & 0.185 & 0.003 & 57.673
16_2x64^1  &  0.769 & 0.769 & 0.013 & 58.875
64_2x64^1  &  3.019 & 3.019 & 0.044 & 68.180
256_2x64^1  &  11.228 & 11.228 & 0.211 & 53.335
1024_2x64^1  &  24.381 & 24.381 & 0.831 & 29.326
1_2x64^2  &  1.793 & 1.793 & 0.039 & 45.606
4_2x64^2  &  6.735 & 6.735 & 0.148 & 45.444
16_2x64^2  &  26.947 & 26.947 & 0.590 & 45.671
64_2x64^2  &  91.034 & 91.034 & 2.218 & 41.047
256_2x64^2  &  194.654 & 194.654 & 9.438 & 20.625
1024_2x64^2  &  258.361 & 258.361 & 36.885 & 7.004
1_2x64^3  &  77.153 & 77.153 & 2.002 & 38.536
4_2x64^3  &  211.400 & 211.400 & 7.552 & 27.994
16_2x64^3  &  284.235 & 284.235 & 30.757 & 9.241
64_2x64^3  &  335.389 & 335.389 & 109.528 & 3.062
256_2x64^3  &  349.783 & 349.783 & 252.016 & 1.388
1024_2x64^3  &  353.784 & 353.784 & 263.646 & 1.342
1_2x64^4  &  293.620 & 293.620 & 107.085 & 2.742
4_2x64^4  &  342.662 & 342.662 & 131.208 & 2.612
16_2x64^4  &  347.927 & 347.927 & 253.754 & 1.371
64_2x64^4  &  349.099 & 349.099 & 250.886 & 1.391
1_2x64^5  &  313.651 & 313.651 & 197.381 & 1.589
1_2x128^1  &  0.091 & 0.091 & 0.002 & 47.370
4_2x128^1  &  0.385 & 0.385 & 0.007 & 57.854
16_2x128^1  &  1.600 & 1.600 & 0.026 & 61.439
64_2x128^1  &  5.926 & 5.926 & 0.103 & 57.781
256_2x128^1  &  17.778 & 17.778 & 0.402 & 44.235
1024_2x128^1  &  30.296 & 30.296 & 1.307 & 23.175
1_2x128^2  &  6.842 & 6.842 & 0.156 & 43.966
4_2x128^2  &  26.008 & 26.008 & 0.557 & 46.666
16_2x128^2  &  91.228 & 91.228 & 2.138 & 42.662
64_2x128^2  &  201.942 & 201.942 & 8.994 & 22.453
256_2x128^2  &  267.538 & 267.538 & 36.526 & 7.325
1024_2x128^2  &  327.559 & 327.559 & 140.575 & 2.330
1_2x128^3  &  225.528 & 225.528 & 16.196 & 13.925
4_2x128^3  &  311.103 & 311.103 & 60.215 & 5.167
16_2x128^3  &  346.209 & 346.209 & 228.671 & 1.514
64_2x128^3  &  355.926 & 355.926 & 261.835 & 1.359
256_2x128^3  &  358.822 & 358.822 & 269.682 & 1.331
1_2x128^4  &  316.408 & 316.408 & 47.578 & 6.650
4_2x128^4  &  356.813 & 356.813 & 133.513 & 2.673
1_4x2^1  &  0.003 & 0.003 & 0.000 & 48.670
4_4x2^1  &  0.012 & 0.012 & 0.000 & 58.258
16_4x2^1  &  0.046 & 0.046 & 0.001 & 56.963
64_4x2^1  &  0.182 & 0.182 & 0.003 & 55.149
256_4x2^1  &  0.769 & 0.769 & 0.013 & 59.793
1024_4x2^1  &  2.909 & 2.909 & 0.050 & 58.504
1_4x2^2  &  0.010 & 0.010 & 0.000 & 53.029
4_4x2^2  &  0.040 & 0.040 & 0.001 & 48.040
16_4x2^2  &  0.156 & 0.156 & 0.003 & 46.879
64_4x2^2  &  0.632 & 0.632 & 0.013 & 48.050
256_4x2^2  &  2.581 & 2.581 & 0.052 & 49.903
1024_4x2^2  &  10.323 & 10.323 & 0.214 & 48.226
1_4x2^3  &  0.033 & 0.033 & 0.001 & 38.820
4_4x2^3  &  0.131 & 0.131 & 0.003 & 41.576
16_4x2^3  &  0.530 & 0.530 & 0.012 & 43.366
64_4x2^3  &  2.105 & 2.105 & 0.051 & 41.059
256_4x2^3  &  8.233 & 8.233 & 0.206 & 40.023
1024_4x2^3  &  32.000 & 32.000 & 0.797 & 40.143
1_4x2^4  &  0.110 & 0.110 & 0.003 & 35.640
4_4x2^4  &  0.439 & 0.439 & 0.012 & 37.768
16_4x2^4  &  1.667 & 1.667 & 0.046 & 36.482
64_4x2^4  &  6.977 & 6.977 & 0.185 & 37.811
256_4x2^4  &  28.230 & 28.230 & 0.740 & 38.157
1024_4x2^4  &  89.302 & 89.302 & 3.025 & 29.524
1_4x2^5  &  0.349 & 0.349 & 0.010 & 33.396
4_4x2^5  &  1.336 & 1.336 & 0.042 & 31.826
16_4x2^5  &  5.848 & 5.848 & 0.171 & 34.264
64_4x2^5  &  23.619 & 23.619 & 0.665 & 35.514
256_4x2^5  &  81.311 & 81.311 & 2.639 & 30.810
1024_4x2^5  &  177.527 & 177.527 & 10.522 & 16.872
1_4x2^6  &  1.235 & 1.235 & 0.039 & 31.835
4_4x2^6  &  4.809 & 4.809 & 0.146 & 32.935
16_4x2^6  &  20.000 & 20.000 & 0.475 & 42.103
64_4x2^6  &  80.000 & 80.000 & 2.377 & 33.660
256_4x2^6  &  192.919 & 192.919 & 8.831 & 21.847
1024_4x2^6  &  275.692 & 275.692 & 37.143 & 7.423
1_4x2^7  &  4.276 & 4.276 & 0.137 & 31.142
4_4x2^7  &  17.162 & 17.162 & 0.525 & 32.661
16_4x2^7  &  69.352 & 69.352 & 2.138 & 32.442
64_4x2^7  &  184.727 & 184.727 & 5.891 & 31.357
256_4x2^7  &  276.698 & 276.698 & 34.660 & 7.983
1024_4x2^7  &  324.958 & 324.958 & 82.447 & 3.941
1_4x2^8  &  14.530 & 14.530 & 0.492 & 29.508
4_4x2^8  &  59.475 & 59.475 & 1.965 & 30.261
16_4x2^8  &  172.881 & 172.881 & 6.272 & 27.566
64_4x2^8  &  279.452 & 279.452 & 30.737 & 9.092
256_4x2^8  &  324.857 & 324.857 & 63.152 & 5.144
1024_4x2^8  &  344.804 & 344.804 & 61.896 & 5.571
1_4x2^9  &  53.648 & 53.648 & 1.865 & 28.763
4_4x2^9  &  166.517 & 166.517 & 7.260 & 22.938
16_4x2^9  &  265.239 & 265.239 & 28.593 & 9.276
64_4x2^9  &  325.170 & 325.170 & 75.538 & 4.305
256_4x2^9  &  345.868 & 345.868 & 45.607 & 7.584
1024_4x2^9  &  352.996 & 352.996 & 43.084 & 8.193
1_4x2^10  &  156.482 & 156.482 & 6.823 & 22.935
4_4x2^10  &  260.637 & 260.637 & 26.634 & 9.786
16_4x2^10  &  317.056 & 317.056 & 90.342 & 3.510
64_4x2^10  &  337.898 & 337.898 & 76.539 & 4.415
256_4x2^10  &  344.120 & 344.120 & 44.847 & 7.673
1024_4x2^10  &  346.115 & 346.115 & 43.413 & 7.973
1_4x2^11  &  250.015 & 250.015 & 25.287 & 9.887
4_4x2^11  &  308.835 & 308.835 & 90.334 & 3.419
16_4x2^11  &  332.634 & 332.634 & 94.672 & 3.514
64_4x2^11  &  339.437 & 339.437 & 75.973 & 4.468
256_4x2^11  &  341.643 & 341.643 & 52.426 & 6.517
1_4x2^12  &  300.550 & 300.550 & 92.199 & 3.260
4_4x2^12  &  336.885 & 336.885 & 94.794 & 3.554
16_4x2^12  &  345.313 & 345.313 & 95.011 & 3.634
64_4x2^12  &  347.383 & 347.383 & 91.666 & 3.790
1_4x2^13  &  319.629 & 319.629 & 107.315 & 2.978
4_4x2^13  &  340.804 & 340.804 & 95.893 & 3.554
16_4x2^13  &  348.374 & 348.374 & 94.741 & 3.677
1_4x2^14  &  325.736 & 325.736 & 108.848 & 2.993
4_4x2^14  &  341.326 & 341.326 & 82.480 & 4.138
1_4x2^15  &  327.470 & 327.470 & 109.377 & 2.994
1_4x4^1  &  0.006 & 0.006 & 0.000 & 50.767
4_4x4^1  &  0.023 & 0.024 & 0.000 & 57.877
16_4x4^1  &  0.096 & 0.096 & 0.002 & 58.275
64_4x4^1  &  0.377 & 0.377 & 0.006 & 59.172
256_4x4^1  &  1.509 & 1.455 & 0.026 & 54.897
1024_4x4^1  &  5.520 & 5.427 & 0.103 & 52.588
1_4x4^2  &  0.026 & 0.026 & 0.001 & 42.153
4_4x4^2  &  0.102 & 0.106 & 0.002 & 52.005
16_4x4^2  &  0.400 & 0.426 & 0.009 & 47.165
64_4x4^2  &  1.702 & 1.739 & 0.036 & 48.855
256_4x4^2  &  6.811 & 6.882 & 0.141 & 48.773
1024_4x4^2  &  22.857 & 23.279 & 0.576 & 40.409
1_4x4^3  &  0.106 & 0.283 & 0.003 & 95.843
4_4x4^3  &  0.462 & 1.111 & 0.011 & 100.430
16_4x4^3  &  1.702 & 4.364 & 0.042 & 103.138
64_4x4^3  &  7.274 & 18.812 & 0.173 & 108.554
256_4x4^3  &  29.091 & 73.846 & 0.692 & 106.775
1024_4x4^3  &  101.053 & 274.439 & 2.746 & 99.936
1_4x4^4  &  0.440 & 1.538 & 0.013 & 117.515
4_4x4^4  &  1.778 & 6.041 & 0.051 & 119.409
16_4x4^4  &  7.485 & 22.069 & 0.193 & 114.195
64_4x4^4  &  29.767 & 98.521 & 0.796 & 123.797
256_4x4^4  &  114.413 & 379.479 & 3.024 & 125.480
1024_4x4^4  &  384.657 & 1122.192 & 12.847 & 87.353
1_4x4^5  &  1.869 & 7.273 & 0.056 & 130.336
4_4x4^5  &  7.513 & 28.070 & 0.215 & 130.587
16_4x4^5  &  30.184 & 116.298 & 0.828 & 140.385
64_4x4^5  &  119.070 & 456.888 & 3.512 & 130.078
256_4x4^5  &  389.354 & 1296.715 & 14.044 & 92.330
1024_4x4^5  &  585.169 & 1905.393 & 55.453 & 34.361
1_4x4^6  &  7.300 & 20.000 & 0.236 & 84.887
4_4x4^6  &  30.356 & 82.581 & 0.931 & 88.740
16_4x4^6  &  110.890 & 313.569 & 3.647 & 85.988
64_4x4^6  &  387.672 & 1193.010 & 14.812 & 80.541
256_4x4^6  &  601.639 & 1616.842 & 58.917 & 27.443
1024_4x4^6  &  723.896 & 2321.313 & 154.820 & 14.994
1_4x4^7  &  31.003 & 90.505 & 1.000 & 90.544
4_4x4^7  &  122.740 & 377.263 & 3.848 & 98.029
16_4x4^7  &  427.940 & 1156.129 & 15.932 & 72.567
64_4x4^7  &  632.238 & 1803.270 & 61.523 & 29.310
256_4x4^7  &  723.355 & 2262.091 & 169.806 & 13.322
1024_4x4^7  &  775.115 & 2506.157 & 176.012 & 14.239
1_4x4^8  &  123.746 & 390.095 & 4.087 & 95.436
4_4x4^8  &  378.383 & 1231.880 & 16.374 & 75.235
16_4x4^8  &  631.977 & 1856.544 & 64.112 & 28.958
64_4x4^8  &  725.357 & 2420.536 & 173.257 & 13.971
256_4x4^8  &  776.263 & 2668.132 & 165.527 & 16.119
1024_4x4^8  &  791.647 & 2743.706 & 164.995 & 16.629
1_4x4^9  &  347.774 & 1123.902 & 17.011 & 66.068
4_4x4^9  &  634.492 & 1793.869 & 61.952 & 28.956
16_4x4^9  &  699.507 & 2102.010 & 175.475 & 11.979
64_4x4^9  &  750.842 & 2340.571 & 174.580 & 13.407
256_4x4^9  &  767.114 & 2412.615 & 151.376 & 15.938
1024_4x4^9  &  771.532 & 2430.948 & 147.992 & 16.426
1_4x4^10  &  512.000 & 1891.917 & 71.099 & 26.610
4_4x4^10  &  705.599 & 2214.054 & 176.536 & 12.542
16_4x4^10  &  759.441 & 2449.028 & 184.399 & 13.281
64_4x4^10  &  774.817 & 2523.770 & 178.159 & 14.166
256_4x4^10  &  778.840 & 2542.896 & 153.865 & 16.527
1024_4x4^10  &  778.946 & 2548.281 & -1.000 & -2548.281
1_4x4^11  &  606.203 & 2346.667 & 180.438 & 13.005
4_4x4^11  &  758.439 & 2554.105 & 184.965 & 13.809
16_4x4^11  &  772.395 & 2627.651 & 187.527 & 14.012
64_4x4^11  &  777.092 & 2648.406 & 178.663 & 14.823
256_4x4^11  &  778.314 & 2652.814 & -1.000 & -2652.814
1_4x4^12  &  643.746 & 2491.468 & 189.222 & 13.167
4_4x4^12  &  783.377 & 2573.719 & 187.783 & 13.706
16_4x4^12  &  787.168 & 2608.344 & 187.829 & 13.887
64_4x4^12  &  788.351 & 2605.158 & -1.000 & -2605.158
1_4x4^13  &  652.418 & 2280.047 & 191.722 & 11.892
4_4x4^13  &  765.777 & 2302.305 & 187.855 & 12.256
16_4x4^13  &  767.020 & 2308.099 & -1.000 & -2308.099
1_4x4^14  &  653.571 & 2222.114 & 191.980 & 11.575
4_4x4^14  &  771.254 & 2231.551 & -1.000 & -2231.551
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 15 -p 4 -q 4 -r 10 -w 20 -t float --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1073741824] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x F_11 [4, 4] x F_12 [4, 4] x F_13 [4, 4] x F_14 [4, 4] x to produce Y[1, 1073741824]
Matmul: 1 x 1073741824 x 1073741824, Num KP Factors: 15
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
Aborted (core dumped)
1_4x4^15  &  1.000 & 1.000 & -1.000 & -1.000
1_4x8^1  &  0.012 & 0.012 & 0.000 & 48.051
4_4x8^1  &  0.050 & 0.050 & 0.001 & 61.034
16_4x8^1  &  0.189 & 0.189 & 0.003 & 58.447
64_4x8^1  &  0.755 & 0.755 & 0.011 & 66.526
256_4x8^1  &  3.077 & 3.077 & 0.052 & 58.687
1024_4x8^1  &  11.636 & 11.636 & 0.212 & 54.952
1_4x8^2  &  0.078 & 0.078 & 0.002 & 42.615
4_4x8^2  &  0.323 & 0.323 & 0.007 & 48.166
16_4x8^2  &  1.291 & 1.291 & 0.026 & 48.828
64_4x8^2  &  5.161 & 5.161 & 0.104 & 49.768
256_4x8^2  &  20.204 & 20.204 & 0.397 & 50.954
1024_4x8^2  &  77.576 & 77.576 & 1.734 & 44.729
1_4x8^3  &  0.497 & 0.497 & 0.014 & 36.468
4_4x8^3  &  2.170 & 2.170 & 0.050 & 43.377
16_4x8^3  &  7.465 & 7.465 & 0.175 & 42.754
64_4x8^3  &  32.708 & 32.708 & 0.668 & 48.951
256_4x8^3  &  129.884 & 129.884 & 3.107 & 41.800
1024_4x8^3  &  336.526 & 336.526 & 13.108 & 25.674
1_4x8^4  &  3.530 & 3.530 & 0.098 & 35.931
4_4x8^4  &  13.408 & 13.408 & 0.366 & 36.639
16_4x8^4  &  55.491 & 55.491 & 1.482 & 37.433
64_4x8^4  &  192.030 & 192.030 & 5.090 & 37.726
256_4x8^4  &  429.088 & 429.088 & 22.882 & 18.752
1024_4x8^4  &  581.818 & 581.818 & 96.123 & 6.053
1_4x8^5  &  22.143 & 22.143 & 0.693 & 31.958
4_4x8^5  &  89.369 & 89.369 & 2.648 & 33.748
16_4x8^5  &  307.597 & 307.597 & 10.475 & 29.365
64_4x8^5  &  513.657 & 513.657 & 42.780 & 12.007
256_4x8^5  &  616.688 & 616.688 & 163.137 & 3.780
1024_4x8^5  &  666.891 & 666.891 & 296.283 & 2.251
1_4x8^6  &  149.333 & 149.333 & 4.952 & 30.155
4_4x8^6  &  390.508 & 390.508 & 19.116 & 20.428
16_4x8^6  &  567.887 & 567.887 & 77.077 & 7.368
64_4x8^6  &  646.089 & 646.089 & 278.830 & 2.317
256_4x8^6  &  677.158 & 677.158 & 302.778 & 2.236
1024_4x8^6  &  687.727 & 687.727 & 307.180 & 2.239
1_4x8^7  &  381.149 & 381.149 & 36.185 & 10.533
4_4x8^7  &  610.841 & 610.841 & 133.754 & 4.567
16_4x8^7  &  664.358 & 664.358 & 291.334 & 2.280
64_4x8^7  &  686.881 & 686.881 & 300.265 & 2.288
256_4x8^7  &  692.723 & 692.723 & 308.958 & 2.242
1_4x8^8  &  509.502 & 509.502 & 252.687 & 2.016
4_4x8^8  &  676.827 & 676.827 & 297.850 & 2.272
16_4x8^8  &  689.323 & 689.323 & 301.246 & 2.288
64_4x8^8  &  692.683 & 692.683 & -1.000 & -692.683
1_4x8^9  &  538.642 & 538.642 & 299.157 & 1.801
4_4x8^9  &  692.549 & 692.549 & 303.875 & 2.279
1_4x8^10  &  540.921 & 540.921 & -1.000 & -540.921
1_4x16^1  &  0.025 & 0.025 & 0.000 & 52.079
4_4x16^1  &  0.093 & 0.093 & 0.002 & 57.188
16_4x16^1  &  0.364 & 0.364 & 0.007 & 54.815
64_4x16^1  &  1.509 & 1.509 & 0.027 & 56.609
256_4x16^1  &  5.926 & 5.926 & 0.106 & 56.026
1024_4x16^1  &  22.857 & 22.857 & 0.420 & 54.372
1_4x16^2  &  0.266 & 0.266 & 0.006 & 43.760
4_4x16^2  &  1.010 & 1.010 & 0.023 & 44.415
16_4x16^2  &  4.212 & 4.212 & 0.086 & 48.924
64_4x16^2  &  16.842 & 16.842 & 0.364 & 46.290
256_4x16^2  &  65.979 & 65.979 & 1.424 & 46.318
1024_4x16^2  &  185.549 & 185.549 & 5.791 & 32.041
1_4x16^3  &  3.256 & 3.256 & 0.082 & 39.685
4_4x16^3  &  12.824 & 12.824 & 0.307 & 41.779
16_4x16^3  &  52.093 & 52.093 & 1.234 & 42.215
64_4x16^3  &  178.050 & 178.050 & 4.945 & 36.004
256_4x16^3  &  413.588 & 413.588 & 19.137 & 21.612
1024_4x16^3  &  556.400 & 556.400 & 78.724 & 7.068
1_4x16^4  &  39.535 & 39.535 & 1.108 & 35.675
4_4x16^4  &  151.111 & 151.111 & 4.266 & 35.422
16_4x16^4  &  387.189 & 387.189 & 16.160 & 23.960
64_4x16^4  &  574.142 & 574.142 & 68.527 & 8.378
256_4x16^4  &  654.929 & 654.929 & 264.788 & 2.473
1024_4x16^4  &  684.277 & 684.277 & 432.253 & 1.583
1_4x16^5  &  308.249 & 308.249 & 15.487 & 19.903
4_4x16^5  &  549.031 & 549.031 & 46.767 & 11.740
16_4x16^5  &  649.524 & 649.524 & 229.321 & 2.832
64_4x16^5  &  681.534 & 681.534 & 424.391 & 1.606
256_4x16^5  &  692.362 & 692.362 & 441.992 & 1.566
1024_4x16^5  &  695.283 & 695.283 & 443.962 & 1.566
1_4x16^6  &  525.948 & 525.948 & 200.357 & 2.625
4_4x16^6  &  682.767 & 682.767 & 419.289 & 1.628
16_4x16^6  &  694.211 & 694.211 & 432.192 & 1.606
64_4x16^6  &  698.090 & 698.090 & 434.885 & 1.605
1_4x16^7  &  598.863 & 598.863 & 421.366 & 1.421
4_4x16^7  &  696.145 & 696.145 & 430.772 & 1.616
1_4x32^1  &  0.044 & 0.044 & 0.001 & 46.575
4_4x32^1  &  0.185 & 0.185 & 0.003 & 59.689
16_4x32^1  &  0.755 & 0.755 & 0.012 & 60.400
64_4x32^1  &  3.019 & 3.019 & 0.053 & 56.696
256_4x32^1  &  11.852 & 11.852 & 0.185 & 64.153
1024_4x32^1  &  35.068 & 35.068 & 0.835 & 41.983
1_4x32^2  &  0.938 & 0.938 & 0.022 & 43.550
4_4x32^2  &  3.831 & 3.831 & 0.081 & 47.427
16_4x32^2  &  14.850 & 14.850 & 0.324 & 45.892
64_4x32^2  &  57.618 & 57.618 & 1.279 & 45.045
256_4x32^2  &  198.674 & 198.674 & 5.096 & 38.985
1024_4x32^2  &  415.194 & 415.194 & 20.415 & 20.337
1_4x32^3  &  22.462 & 22.462 & 0.565 & 39.769
4_4x32^3  &  85.902 & 85.902 & 2.162 & 39.740
16_4x32^3  &  268.506 & 268.506 & 8.387 & 32.014
64_4x32^3  &  466.071 & 466.071 & 33.446 & 13.935
256_4x32^3  &  649.453 & 649.453 & 131.312 & 4.946
1024_4x32^3  &  698.128 & 698.128 & 467.835 & 1.492
1_4x32^4  &  321.649 & 321.649 & 15.398 & 20.889
4_4x32^4  &  562.162 & 562.162 & 57.287 & 9.813
16_4x32^4  &  654.832 & 654.832 & 226.245 & 2.894
64_4x32^4  &  688.235 & 688.235 & 468.905 & 1.468
256_4x32^4  &  696.315 & 696.315 & 497.839 & 1.399
1024_4x32^4  &  699.206 & 699.206 & 496.678 & 1.408
1_4x32^5  &  561.545 & 561.545 & 339.935 & 1.652
4_4x32^5  &  693.803 & 693.803 & 382.802 & 1.812
16_4x32^5  &  701.294 & 701.294 & 483.293 & 1.451
1_4x32^6  &  600.652 & 600.652 & 458.283 & 1.311
1_4x64^1  &  0.094 & 0.094 & 0.002 & 50.118
4_4x64^1  &  0.364 & 0.364 & 0.007 & 53.833
16_4x64^1  &  1.569 & 1.569 & 0.025 & 61.839
64_4x64^1  &  6.154 & 6.154 & 0.104 & 59.316
256_4x64^1  &  22.456 & 22.456 & 0.420 & 53.518
1024_4x64^1  &  49.231 & 49.231 & 1.712 & 28.751
1_4x64^2  &  3.696 & 3.696 & 0.083 & 44.644
4_4x64^2  &  13.204 & 13.204 & 0.305 & 43.291
16_4x64^2  &  58.495 & 58.495 & 1.227 & 47.683
64_4x64^2  &  196.036 & 196.036 & 4.853 & 40.392
256_4x64^2  &  428.768 & 428.768 & 18.937 & 22.641
1024_4x64^2  &  534.834 & 534.834 & 77.831 & 6.872
1_4x64^3  &  157.122 & 157.122 & 4.258 & 36.904
4_4x64^3  &  420.000 & 420.000 & 8.320 & 50.482
16_4x64^3  &  590.270 & 590.270 & 62.969 & 9.374
64_4x64^3  &  684.170 & 684.170 & 244.739 & 2.796
256_4x64^3  &  707.458 & 707.458 & 494.356 & 1.431
1024_4x64^3  &  716.869 & 716.869 & 514.113 & 1.394
1_4x64^4  &  545.273 & 545.273 & 211.679 & 2.576
4_4x64^4  &  694.095 & 694.095 & 263.929 & 2.630
16_4x64^4  &  704.145 & 704.145 & 497.269 & 1.416
64_4x64^4  &  706.413 & 706.413 & 488.992 & 1.445
1_4x64^5  &  592.364 & 592.364 & 381.846 & 1.551
1_4x128^1  &  0.182 & 0.182 & 0.004 & 51.070
4_4x128^1  &  0.784 & 0.784 & 0.012 & 66.966
16_4x128^1  &  3.077 & 3.077 & 0.053 & 58.026
64_4x128^1  &  12.308 & 12.308 & 0.213 & 57.854
256_4x128^1  &  35.556 & 35.556 & 0.840 & 42.314
1024_4x128^1  &  60.952 & 60.952 & 3.422 & 17.812
1_4x128^2  &  13.333 & 13.333 & 0.322 & 41.374
4_4x128^2  &  55.597 & 55.597 & 1.162 & 47.833
16_4x128^2  &  190.324 & 190.324 & 4.678 & 40.681
64_4x128^2  &  431.020 & 431.020 & 18.924 & 22.776
256_4x128^2  &  544.182 & 544.182 & 76.419 & 7.121
1024_4x128^2  &  668.487 & 668.487 & 295.765 & 2.260
1_4x128^3  &  408.502 & 408.502 & 32.208 & 12.683
4_4x128^3  &  639.395 & 639.395 & 122.526 & 5.218
16_4x128^3  &  692.405 & 692.405 & 459.843 & 1.506
64_4x128^3  &  705.723 & 705.723 & 514.067 & 1.373
256_4x128^3  &  710.775 & 710.775 & 529.042 & 1.344
1_4x128^4  &  607.629 & 607.629 & 95.335 & 6.374
4_4x128^4  &  708.041 & 708.041 & 262.980 & 2.692
1_8x2^1  &  0.006 & 0.006 & 0.000 & 48.595
4_8x2^1  &  0.025 & 0.025 & 0.000 & 60.853
16_8x2^1  &  0.096 & 0.096 & 0.002 & 58.770
64_8x2^1  &  0.392 & 0.392 & 0.007 & 59.751
256_8x2^1  &  1.538 & 1.538 & 0.015 & 103.448
1024_8x2^1  &  6.038 & 6.038 & 0.107 & 56.209
1_8x2^2  &  0.034 & 0.034 & 0.001 & 44.757
4_8x2^2  &  0.133 & 0.133 & 0.003 & 49.479
16_8x2^2  &  0.532 & 0.532 & 0.011 & 46.741
64_8x2^2  &  2.151 & 2.151 & 0.045 & 47.939
256_8x2^2  &  8.513 & 8.513 & 0.173 & 49.084
1024_8x2^2  &  32.333 & 32.333 & 0.710 & 45.540
1_8x2^3  &  0.200 & 0.200 & 0.005 & 39.040
4_8x2^3  &  0.820 & 0.820 & 0.018 & 44.402
16_8x2^3  &  3.183 & 3.183 & 0.075 & 42.585
64_8x2^3  &  13.020 & 13.020 & 0.305 & 42.708
256_8x2^3  &  48.000 & 48.000 & 1.223 & 39.251
1024_8x2^3  &  147.692 & 147.692 & 4.734 & 31.199
1_8x2^4  &  1.265 & 1.265 & 0.034 & 36.805
4_8x2^4  &  4.971 & 4.971 & 0.132 & 37.708
16_8x2^4  &  20.363 & 20.363 & 0.520 & 39.164
64_8x2^4  &  70.845 & 70.845 & 2.064 & 34.317
256_8x2^4  &  201.505 & 201.505 & 8.296 & 24.290
1024_8x2^4  &  347.049 & 347.049 & 33.210 & 10.450
1_8x2^5  &  7.857 & 7.857 & 0.238 & 33.046
4_8x2^5  &  31.869 & 31.869 & 0.936 & 34.034
16_8x2^5  &  118.609 & 118.609 & 3.017 & 39.317
64_8x2^5  &  259.192 & 259.192 & 14.919 & 17.373
256_8x2^5  &  431.304 & 431.304 & 58.677 & 7.351
1024_8x2^5  &  517.463 & 517.463 & 99.383 & 5.207
1_8x2^6  &  53.740 & 53.740 & 1.730 & 31.064
4_8x2^6  &  173.885 & 173.885 & 6.333 & 27.457
16_8x2^6  &  338.605 & 338.605 & 26.264 & 12.892
64_8x2^6  &  475.558 & 475.558 & 96.460 & 4.930
256_8x2^6  &  534.394 & 534.394 & 42.383 & 12.609
1024_8x2^6  &  551.363 & 551.363 & 39.177 & 14.074
1_8x2^7  &  236.920 & 236.920 & 7.836 & 30.234
4_8x2^7  &  403.771 & 403.771 & 46.266 & 8.727
16_8x2^7  &  505.941 & 505.941 & 150.106 & 3.371
64_8x2^7  &  543.425 & 543.425 & 99.223 & 5.477
256_8x2^7  &  554.098 & 554.098 & 41.199 & 13.449
1_8x2^8  &  453.451 & 453.451 & 85.364 & 5.312
4_8x2^8  &  526.623 & 526.623 & 154.244 & 3.414
16_8x2^8  &  547.901 & 547.901 & 153.332 & 3.573
64_8x2^8  &  553.914 & 553.914 & 141.181 & 3.923
1_8x2^9  &  541.185 & 541.185 & 221.610 & 2.442
4_8x2^9  &  553.230 & 553.230 & 158.418 & 3.492
1_8x2^10  &  553.358 & 553.358 & 225.927 & 2.449
1_8x4^1  &  0.012 & 0.012 & 0.000 & 47.866
4_8x4^1  &  0.049 & 0.049 & 0.001 & 61.068
16_8x4^1  &  0.192 & 0.192 & 0.003 & 58.015
64_8x4^1  &  0.741 & 0.741 & 0.013 & 55.841
256_8x4^1  &  2.963 & 2.963 & 0.049 & 60.366
1024_8x4^1  &  11.429 & 11.429 & 0.211 & 54.204
1_8x4^2  &  0.077 & 0.077 & 0.002 & 41.705
4_8x4^2  &  0.309 & 0.309 & 0.007 & 45.306
16_8x4^2  &  1.277 & 1.277 & 0.028 & 45.661
64_8x4^2  &  4.898 & 4.898 & 0.107 & 45.577
256_8x4^2  &  20.211 & 20.211 & 0.420 & 48.081
1024_8x4^2  &  76.040 & 76.040 & 1.734 & 43.859
1_8x4^3  &  0.543 & 0.543 & 0.014 & 39.636
4_8x4^3  &  2.121 & 2.121 & 0.051 & 41.287
16_8x4^3  &  8.684 & 8.684 & 0.208 & 41.789
64_8x4^3  &  32.464 & 32.464 & 0.785 & 41.363
256_8x4^3  &  123.613 & 123.613 & 3.271 & 37.795
1024_8x4^3  &  347.961 & 347.961 & 13.025 & 26.714
1_8x4^4  &  3.296 & 3.296 & 0.099 & 33.156
4_8x4^4  &  14.199 & 14.199 & 0.367 & 38.678
16_8x4^4  &  56.130 & 56.130 & 1.531 & 36.651
64_8x4^4  &  188.235 & 188.235 & 5.952 & 31.623
256_8x4^4  &  410.730 & 410.730 & 24.643 & 16.667
1024_8x4^4  &  668.553 & 668.553 & 80.220 & 8.334
1_8x4^5  &  22.857 & 22.857 & 0.692 & 33.022
4_8x4^5  &  91.852 & 91.852 & 2.618 & 35.084
16_8x4^5  &  300.606 & 300.606 & 10.326 & 29.110
64_8x4^5  &  556.912 & 556.912 & 41.144 & 13.536
256_8x4^5  &  748.679 & 748.679 & 167.313 & 4.475
1024_8x4^5  &  848.486 & 848.486 & 222.909 & 3.806
1_8x4^6  &  153.308 & 153.308 & 4.975 & 30.817
4_8x4^6  &  378.592 & 378.592 & 18.749 & 20.193
16_8x4^6  &  633.713 & 633.713 & 73.259 & 8.650
64_8x4^6  &  802.388 & 802.388 & 218.934 & 3.665
256_8x4^6  &  876.224 & 876.224 & 110.865 & 7.904
1024_8x4^6  &  898.770 & 898.770 & 103.214 & 8.708
1_8x4^7  &  439.946 & 439.946 & 35.482 & 12.399
4_8x4^7  &  714.157 & 714.157 & 135.635 & 5.265
16_8x4^7  &  829.388 & 829.388 & 302.304 & 2.744
64_8x4^7  &  873.392 & 873.392 & 224.075 & 3.898
256_8x4^7  &  884.004 & 884.004 & 108.076 & 8.179
1_8x4^8  &  633.479 & 633.479 & 259.175 & 2.444
4_8x4^8  &  857.467 & 857.467 & 306.570 & 2.797
16_8x4^8  &  887.371 & 887.371 & 307.737 & 2.884
64_8x4^8  &  891.052 & 891.052 & 288.913 & 3.084
1_8x4^9  &  696.270 & 696.270 & 385.118 & 1.808
4_8x4^9  &  886.747 & 886.747 & 313.315 & 2.830
1_8x4^10  &  694.951 & 694.951 & 390.829 & 1.778
1_8x8^1  &  0.024 & 0.024 & 0.000 & 47.904
4_8x8^1  &  0.094 & 0.094 & 0.002 & 57.290
16_8x8^1  &  0.385 & 0.377 & 0.006 & 62.976
64_8x8^1  &  1.482 & 1.455 & 0.026 & 56.000
256_8x8^1  &  6.154 & 5.821 & 0.101 & 57.603
1024_8x8^1  &  23.704 & 24.165 & 0.405 & 59.627
1_8x8^2  &  0.220 & 0.400 & 0.004 & 94.585
4_8x8^2  &  0.860 & 1.510 & 0.017 & 91.084
16_8x8^2  &  3.404 & 6.041 & 0.071 & 84.512
64_8x8^2  &  13.768 & 25.083 & 0.284 & 88.417
256_8x8^2  &  55.633 & 98.521 & 1.105 & 89.147
1024_8x8^2  &  222.684 & 359.495 & 4.667 & 77.032
1_8x8^3  &  1.832 & 4.528 & 0.047 & 97.161
4_8x8^3  &  7.328 & 17.455 & 0.176 & 98.896
16_8x8^3  &  27.429 & 73.846 & 0.698 & 105.734
64_8x8^3  &  114.654 & 284.444 & 2.015 & 141.164
256_8x8^3  &  384.075 & 975.238 & 11.322 & 86.134
1024_8x8^3  &  760.940 & 2297.494 & 45.334 & 50.679
1_8x8^4  &  14.800 & 27.826 & 0.420 & 66.214
4_8x8^4  &  59.201 & 115.097 & 1.590 & 72.394
16_8x8^4  &  238.183 & 460.386 & 6.240 & 73.782
64_8x8^4  &  835.918 & 1606.767 & 25.447 & 63.143
256_8x8^4  &  1610.221 & 2392.098 & 102.345 & 23.373
1024_8x8^4  &  1504.009 & 3804.702 & 388.818 & 9.785
1_8x8^5  &  120.205 & 224.623 & 3.611 & 62.203
4_8x8^5  &  478.575 & 682.524 & 12.808 & 53.290
16_8x8^5  &  1264.320 & 1932.360 & 54.820 & 35.249
64_8x8^5  &  1393.197 & 2130.559 & 221.355 & 9.625
256_8x8^5  &  1573.494 & 2826.084 & 497.992 & 5.675
1024_8x8^5  &  1638.400 & 3256.852 & 509.429 & 6.393
1_8x8^6  &  923.910 & 1480.482 & 31.064 & 47.659
4_8x8^6  &  1764.883 & 2451.471 & 116.386 & 21.063
16_8x8^6  &  1522.912 & 2755.543 & 469.817 & 5.865
64_8x8^6  &  1623.685 & 3204.694 & 507.871 & 6.310
256_8x8^6  &  1652.819 & 3406.311 & 376.897 & 9.038
1024_8x8^6  &  1660.464 & 3455.467 & 361.718 & 9.553
1_8x8^7  &  1579.725 & 2404.361 & 252.888 & 9.508
4_8x8^7  &  1588.752 & 2953.022 & 555.248 & 5.318
16_8x8^7  &  1640.744 & 3170.640 & 584.121 & 5.428
64_8x8^7  &  1656.574 & 3239.488 & 531.261 & 6.098
256_8x8^7  &  1660.472 & 3255.942 & 394.826 & 8.247
1_8x8^8  &  1611.458 & 3147.459 & 605.388 & 5.199
4_8x8^8  &  1638.560 & 3342.741 & 585.834 & 5.706
16_8x8^8  &  1646.076 & 3392.318 & 579.113 & 5.858
64_8x8^8  &  1648.190 & 3406.135 & -1.000 & -3406.135
1_8x8^9  &  1654.543 & 3322.192 & 617.728 & 5.378
4_8x8^9  &  1658.163 & 3379.324 & 589.169 & 5.736
1_8x8^10  &  1656.831 & 3335.464 & -1.000 & -3335.464
1_8x16^1  &  0.045 & 0.045 & 0.001 & 44.591
4_8x16^1  &  0.189 & 0.189 & 0.003 & 56.035
16_8x16^1  &  0.690 & 0.690 & 0.013 & 51.935
64_8x16^1  &  2.963 & 2.963 & 0.049 & 60.566
256_8x16^1  &  11.852 & 11.852 & 0.214 & 55.321
1024_8x16^1  &  44.912 & 44.912 & 0.799 & 56.239
1_8x16^2  &  0.625 & 0.625 & 0.015 & 42.604
4_8x16^2  &  2.526 & 2.526 & 0.054 & 46.393
16_8x16^2  &  9.796 & 9.796 & 0.210 & 46.669
64_8x16^2  &  39.600 & 39.600 & 0.866 & 45.728
256_8x16^2  &  153.600 & 153.600 & 3.545 & 43.326
1024_8x16^2  &  445.217 & 445.217 & 13.815 & 32.226
1_8x16^3  &  8.059 & 8.059 & 0.215 & 37.553
4_8x16^3  &  33.684 & 33.684 & 0.832 & 40.480
16_8x16^3  &  135.758 & 135.758 & 2.938 & 46.205
64_8x16^3  &  445.217 & 445.217 & 13.296 & 33.486
256_8x16^3  &  968.751 & 968.751 & 52.369 & 18.499
1024_8x16^3  &  1287.183 & 1287.183 & 209.360 & 6.148
1_8x16^4  &  109.714 & 109.714 & 3.139 & 34.949
4_8x16^4  &  431.461 & 431.461 & 11.959 & 36.079
16_8x16^4  &  1274.689 & 1274.689 & 46.014 & 27.702
64_8x16^4  &  1788.646 & 1788.646 & 187.287 & 9.550
256_8x16^4  &  1946.614 & 1946.614 & 748.192 & 2.602
1024_8x16^4  &  2064.343 & 2064.343 & 832.873 & 2.479
1_8x16^5  &  1154.327 & 1154.327 & 44.160 & 26.140
4_8x16^5  &  1737.018 & 1737.018 & 169.221 & 10.265
16_8x16^5  &  1954.229 & 1954.229 & 669.911 & 2.917
64_8x16^5  &  2059.418 & 2059.418 & 841.293 & 2.448
256_8x16^5  &  2097.856 & 2097.856 & 864.266 & 2.427
1024_8x16^5  &  2107.376 & 2107.376 & -1.000 & -2107.376
1_8x16^6  &  1965.331 & 1965.331 & 628.488 & 3.127
4_8x16^6  &  2042.529 & 2042.529 & 829.164 & 2.463
16_8x16^6  &  2084.868 & 2084.868 & 850.095 & 2.453
64_8x16^6  &  2097.485 & 2097.485 & -1.000 & -2097.485
1_8x16^7  &  2082.642 & 2082.642 & 848.101 & 2.456
4_8x16^7  &  2095.568 & 2095.568 & -1.000 & -2095.568
1_8x32^1  &  0.093 & 0.093 & 0.002 & 45.868
4_8x32^1  &  0.370 & 0.370 & 0.006 & 57.145
16_8x32^1  &  1.538 & 1.538 & 0.027 & 56.766
64_8x32^1  &  6.275 & 6.275 & 0.108 & 57.945
256_8x32^1  &  23.704 & 23.704 & 0.424 & 55.927
1024_8x32^1  &  70.137 & 70.137 & 1.695 & 41.367
1_8x32^2  &  2.083 & 2.083 & 0.048 & 43.485
4_8x32^2  &  8.247 & 8.247 & 0.178 & 46.373
16_8x32^2  &  33.684 & 33.684 & 0.710 & 47.463
64_8x32^2  &  132.001 & 132.001 & 2.884 & 45.765
256_8x32^2  &  400.098 & 400.098 & 11.467 & 34.892
1024_8x32^2  &  761.338 & 761.338 & 46.127 & 16.505
1_8x32^3  &  50.149 & 50.149 & 1.311 & 38.257
4_8x32^3  &  193.425 & 193.425 & 4.656 & 41.544
16_8x32^3  &  693.817 & 693.817 & 19.367 & 35.825
64_8x32^3  &  1498.700 & 1498.700 & 78.008 & 19.212
256_8x32^3  &  2067.692 & 2067.692 & 321.416 & 6.433
1024_8x32^3  &  2249.519 & 2249.519 & 909.815 & 2.473
1_8x32^4  &  1061.463 & 1061.463 & 36.568 & 29.027
4_8x32^4  &  1836.287 & 1836.287 & 134.773 & 13.625
16_8x32^4  &  2240.412 & 2240.412 & 538.084 & 4.164
64_8x32^4  &  2368.435 & 2368.435 & 919.874 & 2.575
256_8x32^4  &  2415.891 & 2415.891 & 952.396 & 2.537
1024_8x32^4  &  2430.595 & 2430.595 & 943.190 & 2.577
1_8x32^5  &  2324.793 & 2324.793 & 727.664 & 3.195
4_8x32^5  &  2409.620 & 2409.620 & 758.943 & 3.175
16_8x32^5  &  2435.035 & 2435.035 & 934.729 & 2.605
1_8x32^6  &  2428.538 & 2428.538 & 852.677 & 2.848
1_8x64^1  &  0.179 & 0.179 & 0.004 & 44.389
4_8x64^1  &  0.755 & 0.755 & 0.013 & 56.778
16_8x64^1  &  3.137 & 3.137 & 0.052 & 60.267
64_8x64^1  &  11.636 & 11.636 & 0.209 & 55.551
256_8x64^1  &  45.714 & 45.714 & 0.861 & 53.106
1024_8x64^1  &  98.462 & 98.462 & 3.452 & 28.522
1_8x64^2  &  7.742 & 7.742 & 0.176 & 44.007
4_8x64^2  &  30.978 & 30.978 & 0.639 & 48.468
16_8x64^2  &  120.039 & 120.039 & 2.575 & 46.609
64_8x64^2  &  380.925 & 380.925 & 10.154 & 37.515
256_8x64^2  &  731.519 & 731.519 & 40.975 & 17.853
1024_8x64^2  &  1003.145 & 1003.145 & 165.721 & 6.053
1_8x64^3  &  351.278 & 351.278 & 9.234 & 38.042
4_8x64^3  &  1132.606 & 1132.606 & 29.681 & 38.159
16_8x64^3  &  1977.566 & 1977.566 & 137.151 & 14.419
64_8x64^3  &  2419.159 & 2419.159 & 527.372 & 4.587
256_8x64^3  &  2571.006 & 2571.006 & 967.505 & 2.657
1024_8x64^3  &  2613.706 & 2613.706 & 1003.285 & 2.605
1_8x64^4  &  2400.000 & 2400.000 & 457.787 & 5.243
4_8x64^4  &  2584.854 & 2584.854 & 524.182 & 4.931
16_8x64^4  &  2641.998 & 2641.998 & 968.329 & 2.728
64_8x64^4  &  2655.282 & 2655.282 & 959.586 & 2.767
1_8x64^5  &  2661.527 & 2661.527 & 716.661 & 3.714
1_8x128^1  &  0.377 & 0.377 & 0.008 & 47.943
4_8x128^1  &  1.509 & 1.509 & 0.027 & 56.565
16_8x128^1  &  6.154 & 6.154 & 0.105 & 58.655
64_8x128^1  &  24.615 & 24.615 & 0.427 & 57.657
256_8x128^1  &  68.267 & 68.267 & 1.709 & 39.935
1024_8x128^1  &  121.183 & 121.183 & 6.783 & 17.867
1_8x128^2  &  28.041 & 28.041 & 0.671 & 41.804
4_8x128^2  &  112.201 & 112.201 & 2.415 & 46.468
16_8x128^2  &  362.761 & 362.761 & 9.647 & 37.603
64_8x128^2  &  719.432 & 719.432 & 39.016 & 18.439
256_8x128^2  &  1040.837 & 1040.837 & 153.148 & 6.796
1024_8x128^2  &  1309.488 & 1309.488 & 408.039 & 3.209
1_8x128^3  &  1560.000 & 1560.000 & 68.770 & 22.684
4_8x128^3  &  2329.600 & 2329.600 & 254.292 & 9.161
16_8x128^3  &  2615.080 & 2615.080 & 899.511 & 2.907
64_8x128^3  &  2730.666 & 2730.666 & 979.821 & 2.787
256_8x128^3  &  2762.457 & 2762.457 & 1005.855 & 2.746
1_8x128^4  &  2762.833 & 2762.833 & 189.027 & 14.616
4_8x128^4  &  2781.816 & 2781.816 & 509.575 & 5.459
1_16x2^1  &  0.012 & 0.012 & 0.000 & 63.038
4_16x2^1  &  0.045 & 0.045 & 0.001 & 53.905
16_16x2^1  &  0.185 & 0.185 & 0.003 & 56.153
64_16x2^1  &  0.741 & 0.741 & 0.013 & 55.737
256_16x2^1  &  2.963 & 2.963 & 0.054 & 55.196
1024_16x2^1  &  11.034 & 11.034 & 0.210 & 52.429
1_16x2^2  &  0.118 & 0.118 & 0.003 & 43.567
4_16x2^2  &  0.484 & 0.484 & 0.010 & 50.066
16_16x2^2  &  1.915 & 1.915 & 0.037 & 51.329
64_16x2^2  &  7.273 & 7.273 & 0.166 & 43.814
256_16x2^2  &  27.970 & 27.970 & 0.648 & 43.151
1024_16x2^2  &  87.960 & 87.960 & 2.548 & 34.527
1_16x2^3  &  1.373 & 1.373 & 0.036 & 38.395
4_16x2^3  &  5.407 & 5.407 & 0.133 & 40.741
16_16x2^3  &  20.563 & 20.563 & 0.522 & 39.409
64_16x2^3  &  71.220 & 71.220 & 2.145 & 33.203
256_16x2^3  &  213.364 & 213.364 & 8.468 & 25.197
1024_16x2^3  &  339.782 & 339.782 & 34.093 & 9.966
1_16x2^4  &  16.619 & 16.619 & 0.476 & 34.905
4_16x2^4  &  61.905 & 61.905 & 1.780 & 34.777
16_16x2^4  &  199.149 & 199.149 & 7.338 & 27.140
64_16x2^4  &  328.421 & 328.421 & 21.630 & 15.183
256_16x2^4  &  477.247 & 477.247 & 71.574 & 6.668
1024_16x2^4  &  558.493 & 558.493 & 67.801 & 8.237
1_16x2^5  &  167.179 & 167.179 & 6.398 & 26.131
4_16x2^5  &  306.448 & 306.448 & 24.144 & 12.693
16_16x2^5  &  462.036 & 462.036 & 97.793 & 4.725
64_16x2^5  &  543.611 & 543.611 & 107.449 & 5.059
256_16x2^5  &  575.293 & 575.293 & 39.126 & 14.704
1024_16x2^5  &  589.384 & 589.384 & 37.160 & 15.861
1_16x2^6  &  454.203 & 454.203 & 90.276 & 5.031
4_16x2^6  &  533.177 & 533.177 & 202.073 & 2.639
16_16x2^6  &  561.323 & 561.323 & 200.369 & 2.801
64_16x2^6  &  569.424 & 569.424 & 175.202 & 3.250
1_16x2^7  &  579.652 & 579.652 & 381.073 & 1.521
4_16x2^7  &  589.627 & 589.627 & 127.814 & 4.613
1_16x4^1  &  0.024 & 0.024 & 0.000 & 49.666
4_16x4^1  &  0.091 & 0.091 & 0.002 & 54.980
16_16x4^1  &  0.339 & 0.339 & 0.007 & 51.590
64_16x4^1  &  1.509 & 1.509 & 0.025 & 60.857
256_16x4^1  &  6.154 & 6.154 & 0.106 & 58.304
1024_16x4^1  &  21.333 & 21.333 & 0.426 & 50.123
1_16x4^2  &  0.245 & 0.245 & 0.006 & 40.120
4_16x4^2  &  1.020 & 1.020 & 0.023 & 44.650
16_16x4^2  &  4.082 & 4.082 & 0.090 & 45.364
64_16x4^2  &  17.391 & 17.391 & 0.363 & 47.856
256_16x4^2  &  61.557 & 61.557 & 1.386 & 44.425
1024_16x4^2  &  191.089 & 191.089 & 5.744 & 33.267
1_16x4^3  &  3.043 & 3.043 & 0.082 & 37.226
4_16x4^3  &  12.926 & 12.926 & 0.315 & 41.095
16_16x4^3  &  46.667 & 46.667 & 1.246 & 37.443
64_16x4^3  &  155.404 & 155.404 & 4.849 & 32.050
256_16x4^3  &  404.258 & 404.258 & 19.940 & 20.274
1024_16x4^3  &  670.952 & 670.952 & 76.300 & 8.794
1_16x4^4  &  40.000 & 40.000 & 1.102 & 36.307
4_16x4^4  &  141.667 & 141.667 & 4.207 & 33.672
16_16x4^4  &  385.816 & 385.816 & 13.059 & 29.545
64_16x4^4  &  633.479 & 633.479 & 46.633 & 13.584
256_16x4^4  &  814.981 & 814.981 & 157.683 & 5.168
1024_16x4^4  &  925.342 & 925.342 & 149.275 & 6.199
1_16x4^5  &  310.883 & 310.883 & 14.953 & 20.791
4_16x4^5  &  577.354 & 577.354 & 57.292 & 10.077
16_16x4^5  &  788.582 & 788.582 & 227.250 & 3.470
64_16x4^5  &  899.843 & 899.843 & 229.361 & 3.923
256_16x4^5  &  948.805 & 948.805 & 88.227 & 10.754
1024_16x4^5  &  965.639 & 965.639 & 84.070 & 11.486
1_16x4^6  &  667.890 & 667.890 & 208.094 & 3.210
4_16x4^6  &  905.520 & 905.520 & 402.306 & 2.251
16_16x4^6  &  960.891 & 960.891 & 398.854 & 2.409
64_16x4^6  &  977.215 & 977.215 & 355.806 & 2.746
1_16x4^7  &  775.620 & 775.620 & 672.581 & 1.153
4_16x4^7  &  978.121 & 978.121 & 269.363 & 3.631
1_16x8^1  &  0.045 & 0.045 & 0.001 & 48.163
4_16x8^1  &  0.192 & 0.192 & 0.003 & 58.720
16_16x8^1  &  0.741 & 0.741 & 0.013 & 57.453
64_16x8^1  &  3.077 & 3.077 & 0.053 & 57.952
256_16x8^1  &  11.852 & 11.852 & 0.206 & 57.656
1024_16x8^1  &  44.138 & 44.138 & 0.831 & 53.101
1_16x8^2  &  0.606 & 0.606 & 0.015 & 41.410
4_16x8^2  &  2.265 & 2.265 & 0.048 & 46.979
16_16x8^2  &  10.326 & 10.326 & 0.215 & 47.979
64_16x8^2  &  38.412 & 38.412 & 0.831 & 46.239
256_16x8^2  &  139.636 & 139.636 & 3.541 & 39.437
1024_16x8^2  &  455.216 & 455.216 & 13.472 & 33.790
1_16x8^3  &  8.002 & 8.002 & 0.218 & 36.686
4_16x8^3  &  33.939 & 33.939 & 0.823 & 41.240
16_16x8^3  &  114.140 & 114.140 & 3.269 & 34.920
64_16x8^3  &  411.954 & 411.954 & 12.635 & 32.603
256_16x8^3  &  893.296 & 893.296 & 50.396 & 17.726
1024_16x8^3  &  1303.273 & 1303.273 & 212.590 & 6.130
1_16x8^4  &  107.865 & 107.865 & 3.196 & 33.749
4_16x8^4  &  385.930 & 385.930 & 11.724 & 32.918
16_16x8^4  &  994.175 & 994.175 & 46.400 & 21.426
64_16x8^4  &  1372.961 & 1372.961 & 187.327 & 7.329
256_16x8^4  &  1518.912 & 1518.912 & 377.087 & 4.028
1024_16x8^4  &  1610.485 & 1610.485 & 361.967 & 4.449
1_16x8^5  &  909.570 & 909.570 & 34.126 & 26.653
4_16x8^5  &  1278.711 & 1278.711 & 170.066 & 7.519
16_16x8^5  &  1478.184 & 1478.184 & 534.363 & 2.766
64_16x8^5  &  1561.700 & 1561.700 & 522.170 & 2.991
256_16x8^5  &  1585.590 & 1585.590 & 232.257 & 6.827
1024_16x8^5  &  1592.238 & 1592.238 & 221.377 & 7.192
1_16x8^6  &  1401.673 & 1401.673 & 629.899 & 2.225
4_16x8^6  &  1569.396 & 1569.396 & 789.804 & 1.987
16_16x8^6  &  1609.750 & 1609.750 & 784.384 & 2.052
64_16x8^6  &  1615.506 & 1615.506 & 724.176 & 2.231
1_16x8^7  &  1446.183 & 1446.183 & 1072.979 & 1.348
4_16x8^7  &  1606.290 & 1606.290 & 594.401 & 2.702
1_16x16^1  &  0.089 & 0.089 & 0.002 & 58.283
4_16x16^1  &  0.364 & 0.370 & 0.006 & 62.642
16_16x16^1  &  1.481 & 1.481 & 0.027 & 55.355
64_16x16^1  &  6.154 & 5.926 & 0.101 & 58.389
256_16x16^1  &  24.165 & 23.717 & 0.412 & 57.517
1024_16x16^1  &  78.769 & 78.807 & 1.227 & 64.213
1_16x16^2  &  1.739 & 3.075 & 0.039 & 78.180
4_16x16^2  &  6.667 & 12.315 & 0.142 & 86.425
16_16x16^2  &  25.867 & 50.196 & 0.574 & 87.483
64_16x16^2  &  103.467 & 193.208 & 2.288 & 84.454
256_16x16^2  &  390.211 & 744.727 & 9.186 & 81.072
1024_16x16^2  &  784.041 & 2101.355 & 37.146 & 56.571
1_16x16^3  &  29.320 & 40.837 & 0.756 & 53.991
4_16x16^3  &  114.627 & 174.607 & 2.809 & 62.169
16_16x16^3  &  426.667 & 698.182 & 11.448 & 60.990
64_16x16^3  &  1420.578 & 2560.834 & 45.427 & 56.373
256_16x16^3  &  2917.032 & 3765.986 & 184.558 & 20.405
1024_16x16^3  &  2983.429 & 5931.139 & 741.520 & 7.999
1_16x16^4  &  438.075 & 557.397 & 13.621 & 40.921
4_16x16^4  &  1431.112 & 2074.334 & 53.160 & 39.021
16_16x16^4  &  3006.454 & 3386.600 & 204.163 & 16.588
64_16x16^4  &  2968.788 & 4877.239 & 824.578 & 5.915
256_16x16^4  &  3228.374 & 5981.609 & 944.359 & 6.334
1024_16x16^4  &  3293.396 & 6273.263 & 908.823 & 6.903
1_16x16^5  &  3283.367 & 3608.811 & 234.985 & 15.358
4_16x16^5  &  3022.878 & 4386.613 & 873.712 & 5.021
16_16x16^5  &  3228.772 & 5081.295 & 1340.956 & 3.789
64_16x16^5  &  3288.618 & 5267.902 & 1155.611 & 4.559
256_16x16^5  &  3306.898 & 5326.371 & 756.463 & 7.041
1024_16x16^5  &  3311.643 & 5339.050 & -1.000 & -5339.050
1_16x16^6  &  3227.381 & 5876.570 & 1441.821 & 4.076
4_16x16^6  &  3274.924 & 6136.808 & 1354.814 & 4.530
16_16x16^6  &  3299.311 & 6220.004 & 1348.048 & 4.614
64_16x16^6  &  3307.479 & 6233.426 & -1.000 & -6233.426
1_16x16^7  &  3300.949 & 5501.964 & 1424.976 & 3.861
4_16x16^7  &  3310.916 & 5517.279 & -1.000 & -5517.279
1_16x32^1  &  0.185 & 0.185 & 0.004 & 48.118
4_16x32^1  &  0.755 & 0.755 & 0.014 & 55.062
16_16x32^1  &  3.077 & 3.077 & 0.053 & 58.085
64_16x32^1  &  12.075 & 12.075 & 0.208 & 58.087
256_16x32^1  &  48.302 & 48.302 & 0.849 & 56.905
1024_16x32^1  &  120.471 & 120.471 & 3.403 & 35.398
1_16x32^2  &  4.615 & 4.615 & 0.113 & 41.017
4_16x32^2  &  20.870 & 20.870 & 0.427 & 48.853
16_16x32^2  &  80.869 & 80.869 & 1.768 & 45.753
64_16x32^2  &  298.343 & 298.343 & 6.927 & 43.071
256_16x32^2  &  727.235 & 727.235 & 27.790 & 26.169
1024_16x32^2  &  1250.787 & 1250.787 & 107.245 & 11.663
1_16x32^3  &  138.915 & 138.915 & 3.573 & 38.877
4_16x32^3  &  487.619 & 487.619 & 13.367 & 36.480
16_16x32^3  &  1610.786 & 1610.786 & 52.820 & 30.496
64_16x32^3  &  3018.105 & 3018.105 & 210.748 & 14.321
256_16x32^3  &  3300.374 & 3300.374 & 820.849 & 4.021
1024_16x32^3  &  3577.014 & 3577.014 & 1629.529 & 2.195
1_16x32^4  &  2318.490 & 2318.490 & 100.923 & 22.973
4_16x32^4  &  3289.960 & 3289.960 & 374.455 & 8.786
16_16x32^4  &  3479.788 & 3479.788 & 848.660 & 4.100
64_16x32^4  &  3639.204 & 3639.204 & 1718.223 & 2.118
256_16x32^4  &  3707.312 & 3707.312 & 1652.598 & 2.243
1024_16x32^4  &  3726.945 & 3726.945 & 1626.242 & 2.292
1_16x32^5  &  3567.994 & 3567.994 & 1404.668 & 2.540
4_16x32^5  &  3681.298 & 3681.298 & 1415.810 & 2.600
16_16x32^5  &  3717.887 & 3717.887 & 1752.571 & 2.121
1_16x32^6  &  3722.608 & 3722.608 & -1.000 & -3722.608
1_16x64^1  &  0.385 & 0.385 & 0.008 & 49.087
4_16x64^1  &  1.455 & 1.455 & 0.027 & 54.745
16_16x64^1  &  5.818 & 5.818 & 0.105 & 55.285
64_16x64^1  &  22.456 & 22.456 & 0.419 & 53.592
256_16x64^1  &  86.780 & 86.780 & 1.534 & 56.566
1024_16x64^1  &  187.890 & 187.890 & 6.903 & 27.219
1_16x64^2  &  16.495 & 16.495 & 0.393 & 41.989
4_16x64^2  &  68.817 & 68.817 & 1.455 & 47.295
16_16x64^2  &  266.667 & 266.667 & 5.820 & 45.822
64_16x64^2  &  664.935 & 664.935 & 22.932 & 28.996
256_16x64^2  &  1379.270 & 1379.270 & 88.131 & 15.650
1024_16x64^2  &  1981.137 & 1981.137 & 367.634 & 5.389
1_16x64^3  &  751.888 & 751.888 & 21.248 & 35.386
4_16x64^3  &  2263.579 & 2263.579 & 76.740 & 29.497
16_16x64^3  &  3614.118 & 3614.118 & 313.161 & 11.541
64_16x64^3  &  4014.749 & 4014.749 & 1257.895 & 3.192
256_16x64^3  &  4147.223 & 4147.223 & 1860.479 & 2.229
1024_16x64^3  &  4217.278 & 4217.278 & 1901.900 & 2.217
1_16x64^4  &  3960.865 & 3960.865 & 950.415 & 4.168
4_16x64^4  &  4125.729 & 4125.729 & 1018.604 & 4.050
16_16x64^4  &  4185.401 & 4185.401 & 1850.134 & 2.262
64_16x64^4  &  4204.395 & 4204.395 & 1855.162 & 2.266
1_16x64^5  &  4161.946 & 4161.946 & 1279.073 & 3.254
1_16x128^1  &  0.755 & 0.755 & 0.016 & 46.674
4_16x128^1  &  3.019 & 3.019 & 0.052 & 58.140
16_16x128^1  &  12.075 & 12.075 & 0.206 & 58.713
64_16x128^1  &  48.302 & 48.302 & 0.821 & 58.840
256_16x128^1  &  121.905 & 121.905 & 3.296 & 36.990
1024_16x128^1  &  236.763 & 236.763 & 13.152 & 18.002
1_16x128^2  &  60.000 & 60.000 & 1.386 & 43.290
4_16x128^2  &  217.423 & 217.423 & 5.205 & 41.768
16_16x128^2  &  877.714 & 877.714 & 20.734 & 42.332
64_16x128^2  &  2070.648 & 2070.648 & 82.852 & 24.992
256_16x128^2  &  3299.023 & 3299.023 & 332.774 & 9.914
1024_16x128^2  &  3815.162 & 3815.162 & 1280.305 & 2.980
1_16x128^3  &  2954.625 & 2954.625 & 144.040 & 20.513
4_16x128^3  &  3939.499 & 3939.499 & 535.754 & 7.353
16_16x128^3  &  4361.897 & 4361.897 & 1809.252 & 2.411
64_16x128^3  &  4435.087 & 4435.087 & 1955.543 & 2.268
256_16x128^3  &  4476.273 & 4476.273 & 1994.140 & 2.245
1_16x128^4  &  4457.972 & 4457.972 & 378.692 & 11.772
4_16x128^4  &  4470.656 & 4470.656 & 1022.155 & 4.374
1_32x2^1  &  0.023 & 0.023 & 0.001 & 45.190
4_32x2^1  &  0.091 & 0.091 & 0.002 & 54.671
16_32x2^1  &  0.370 & 0.370 & 0.006 & 57.138
64_32x2^1  &  1.429 & 1.429 & 0.026 & 54.509
256_32x2^1  &  5.517 & 5.517 & 0.110 & 50.273
1024_32x2^1  &  17.297 & 17.297 & 0.435 & 39.803
1_32x2^2  &  0.421 & 0.421 & 0.011 & 39.413
4_32x2^2  &  1.518 & 1.518 & 0.039 & 38.741
16_32x2^2  &  5.911 & 5.911 & 0.157 & 37.545
64_32x2^2  &  21.755 & 21.755 & 0.605 & 35.948
256_32x2^2  &  74.029 & 74.029 & 2.517 & 29.409
1024_32x2^2  &  211.294 & 211.294 & 10.039 & 21.048
1_32x2^3  &  7.936 & 7.936 & 0.270 & 29.393
4_32x2^3  &  27.716 & 27.716 & 1.022 & 27.131
16_32x2^3  &  104.015 & 104.015 & 3.897 & 26.691
64_32x2^3  &  225.173 & 225.173 & 16.307 & 13.808
256_32x2^3  &  376.552 & 376.552 & 65.885 & 5.715
1024_32x2^3  &  492.863 & 492.863 & 112.824 & 4.368
1_32x2^4  &  146.611 & 146.611 & 7.264 & 20.182
4_32x2^4  &  301.831 & 301.831 & 27.429 & 11.004
16_32x2^4  &  444.682 & 444.682 & 108.410 & 4.102
64_32x2^4  &  508.855 & 508.855 & 110.449 & 4.607
256_32x2^4  &  532.323 & 532.323 & 37.659 & 14.136
1024_32x2^4  &  539.214 & 539.214 & 35.742 & 15.086
1_32x2^5  &  438.138 & 438.138 & 190.653 & 2.298
4_32x2^5  &  519.885 & 519.885 & 232.130 & 2.240
16_32x2^5  &  533.295 & 533.295 & 224.746 & 2.373
1_32x2^6  &  484.527 & 484.527 & 552.075 & 0.878
1_32x4^1  &  0.045 & 0.045 & 0.001 & 52.042
4_32x4^1  &  0.182 & 0.182 & 0.003 & 53.972
16_32x4^1  &  0.702 & 0.702 & 0.012 & 58.686
64_32x4^1  &  2.909 & 2.909 & 0.052 & 55.448
256_32x4^1  &  11.034 & 11.034 & 0.218 & 50.546
1024_32x4^1  &  34.595 & 34.595 & 0.845 & 40.947
1_32x4^2  &  0.891 & 0.891 & 0.022 & 39.778
4_32x4^2  &  3.104 & 3.104 & 0.078 & 39.588
16_32x4^2  &  12.200 & 12.200 & 0.328 & 37.201
64_32x4^2  &  46.463 & 46.463 & 1.325 & 35.076
256_32x4^2  &  156.735 & 156.735 & 5.283 & 29.666
1024_32x4^2  &  409.657 & 409.657 & 20.575 & 19.910
1_32x4^3  &  16.785 & 16.785 & 0.509 & 32.990
4_32x4^3  &  60.518 & 60.518 & 2.133 & 28.377
16_32x4^3  &  215.331 & 215.331 & 8.432 & 25.536
64_32x4^3  &  460.296 & 460.296 & 32.391 & 14.210
256_32x4^3  &  819.649 & 819.649 & 132.234 & 6.198
1024_32x4^3  &  979.070 & 979.070 & 234.018 & 4.184
1_32x4^4  &  286.239 & 286.239 & 15.300 & 18.709
4_32x4^4  &  595.231 & 595.231 & 57.292 & 10.389
16_32x4^4  &  883.540 & 883.540 & 229.691 & 3.847
64_32x4^4  &  1002.577 & 1002.577 & 228.712 & 4.384
256_32x4^4  &  1044.670 & 1044.670 & 79.995 & 13.059
1024_32x4^4  &  1055.821 & 1055.821 & 75.921 & 13.907
1_32x4^5  &  713.635 & 713.635 & 411.752 & 1.733
4_32x4^5  &  1021.338 & 1021.338 & 462.049 & 2.210
16_32x4^5  &  1046.125 & 1046.125 & 445.131 & 2.350
1_32x4^6  &  793.792 & 793.792 & 978.787 & 0.811
1_32x8^1  &  0.094 & 0.094 & 0.002 & 48.132
4_32x8^1  &  0.370 & 0.370 & 0.006 & 57.082
16_32x8^1  &  1.429 & 1.429 & 0.027 & 53.871
64_32x8^1  &  5.714 & 5.714 & 0.105 & 54.279
256_32x8^1  &  21.695 & 21.695 & 0.411 & 52.817
1024_32x8^1  &  68.267 & 68.267 & 1.656 & 41.216
1_32x8^2  &  2.020 & 2.020 & 0.040 & 50.995
4_32x8^2  &  6.957 & 6.957 & 0.176 & 39.443
16_32x8^2  &  26.884 & 26.884 & 0.701 & 38.345
64_32x8^2  &  97.710 & 97.710 & 2.802 & 34.875
256_32x8^2  &  339.073 & 339.073 & 11.454 & 29.602
1024_32x8^2  &  890.435 & 890.435 & 46.216 & 19.267
1_32x8^3  &  39.763 & 39.763 & 1.310 & 30.351
4_32x8^3  &  135.075 & 135.075 & 4.981 & 27.117
16_32x8^3  &  469.584 & 469.584 & 19.337 & 24.285
64_32x8^3  &  957.862 & 957.862 & 79.375 & 12.068
256_32x8^3  &  1563.927 & 1563.927 & 322.468 & 4.850
1024_32x8^3  &  1826.727 & 1826.727 & 493.066 & 3.705
1_32x8^4  &  774.377 & 774.377 & 35.890 & 21.576
4_32x8^4  &  1219.048 & 1219.048 & 131.149 & 9.295
16_32x8^4  &  1650.047 & 1650.047 & 456.842 & 3.612
64_32x8^4  &  1823.066 & 1823.066 & 486.662 & 3.746
256_32x8^4  &  1876.052 & 1876.052 & 180.082 & 10.418
1024_32x8^4  &  1886.519 & 1886.519 & 170.183 & 11.085
1_32x8^5  &  1404.319 & 1404.319 & 951.713 & 1.476
4_32x8^5  &  1889.779 & 1889.779 & 905.517 & 2.087
16_32x8^5  &  1892.404 & 1892.404 & 875.606 & 2.161
1_32x8^6  &  1489.336 & 1489.336 & 1581.081 & 0.942
1_32x16^1  &  0.185 & 0.185 & 0.004 & 46.650
4_32x16^1  &  0.755 & 0.755 & 0.013 & 56.336
16_32x16^1  &  2.909 & 2.909 & 0.053 & 54.787
64_32x16^1  &  11.636 & 11.636 & 0.211 & 55.139
256_32x16^1  &  41.967 & 41.967 & 0.814 & 51.578
1024_32x16^1  &  136.533 & 136.533 & 3.221 & 42.388
1_32x16^2  &  5.217 & 5.217 & 0.115 & 45.246
4_32x16^2  &  19.800 & 19.800 & 0.427 & 46.322
16_32x16^2  &  76.824 & 76.824 & 1.537 & 49.983
64_32x16^2  &  243.810 & 243.810 & 6.587 & 37.013
256_32x16^2  &  718.728 & 718.728 & 27.032 & 26.588
1024_32x16^2  &  1260.409 & 1260.409 & 110.100 & 11.448
1_32x16^3  &  117.895 & 117.895 & 3.345 & 35.249
4_32x16^3  &  396.090 & 396.090 & 13.061 & 30.327
16_32x16^3  &  1303.458 & 1303.458 & 52.961 & 24.612
64_32x16^3  &  2737.388 & 2737.388 & 207.643 & 13.183
256_32x16^3  &  3667.083 & 3667.083 & 841.785 & 4.356
1024_32x16^3  &  4239.852 & 4239.852 & 1073.900 & 3.948
1_32x16^4  &  1861.818 & 1861.818 & 103.374 & 18.011
4_32x16^4  &  3281.175 & 3281.175 & 362.514 & 9.051
16_32x16^4  &  3949.538 & 3949.538 & 1063.507 & 3.714
64_32x16^4  &  4338.696 & 4338.696 & 1092.368 & 3.972
256_32x16^4  &  4447.893 & 4447.893 & 462.676 & 9.613
1024_32x16^4  &  4484.256 & 4484.256 & 442.890 & 10.125
1_32x16^5  &  4155.484 & 4155.484 & 2241.043 & 1.854
4_32x16^5  &  4389.362 & 4389.362 & 1731.694 & 2.535
16_32x16^5  &  4463.803 & 4463.803 & 1699.329 & 2.627
1_32x16^6  &  4396.311 & 4396.311 & 2393.957 & 1.836
1_32x32^1  &  0.278 & 0.274 & 0.007 & 37.253
4_32x32^1  &  1.096 & 1.096 & 0.027 & 40.197
16_32x32^1  &  4.507 & 4.444 & 0.098 & 45.248
64_32x32^1  &  14.551 & 16.000 & 0.410 & 39.057
256_32x32^1  &  52.245 & 52.801 & 1.646 & 32.074
1024_32x32^1  &  117.029 & 116.384 & 6.805 & 17.104
1_32x32^2  &  13.196 & 18.294 & 0.286 & 64.044
4_32x32^2  &  55.054 & 73.143 & 1.110 & 65.888
16_32x32^2  &  204.800 & 288.578 & 4.637 & 62.229
64_32x32^2  &  811.340 & 1050.677 & 18.628 & 56.402
256_32x32^2  &  2392.370 & 3213.534 & 73.804 & 43.541
1024_32x32^2  &  4283.836 & 4873.131 & 294.122 & 16.568
1_32x32^3  &  415.135 & 461.846 & 12.378 & 37.311
4_32x32^3  &  1471.617 & 1480.761 & 44.719 & 33.112
16_32x32^3  &  3766.888 & 4012.920 & 179.852 & 22.312
64_32x32^3  &  4824.921 & 4890.937 & 715.236 & 6.838
256_32x32^3  &  5890.876 & 6664.766 & 2166.129 & 3.077
1024_32x32^3  &  6416.579 & 7694.088 & 2110.502 & 3.646
1_32x32^4  &  5070.484 & 5449.979 & 438.000 & 12.443
4_32x32^4  &  5461.334 & 6708.740 & 1672.584 & 4.011
16_32x32^4  &  6229.473 & 8250.007 & 2646.083 & 3.118
64_32x32^4  &  6453.768 & 8759.576 & 2272.164 & 3.855
256_32x32^4  &  6511.820 & 8897.193 & 1372.760 & 6.481
1024_32x32^4  &  6529.608 & 8846.643 & -1.000 & -8846.643
1_32x32^5  &  6416.449 & 7946.165 & 2562.678 & 3.101
4_32x32^5  &  6529.929 & 8244.818 & 2377.632 & 3.468
16_32x32^5  &  6564.883 & 8293.236 & 2792.536 & 2.970
1_32x32^6  &  6540.304 & 7578.527 & -1.000 & -7578.527
1_32x64^1  &  0.741 & 0.741 & 0.016 & 46.383
4_32x64^1  &  2.909 & 2.909 & 0.054 & 53.968
16_32x64^1  &  12.308 & 12.308 & 0.203 & 60.518
64_32x64^1  &  44.912 & 44.912 & 0.833 & 53.908
256_32x64^1  &  140.274 & 140.274 & 3.292 & 42.611
1024_32x64^1  &  288.451 & 288.451 & 13.636 & 21.153
1_32x64^2  &  39.588 & 39.588 & 0.916 & 43.220
4_32x64^2  &  155.152 & 155.152 & 3.177 & 48.829
16_32x64^2  &  596.505 & 596.505 & 13.821 & 43.161
64_32x64^2  &  1920.000 & 1920.000 & 54.525 & 35.213
256_32x64^2  &  3654.848 & 3654.848 & 200.667 & 18.214
1024_32x64^2  &  5292.275 & 5292.275 & 897.704 & 5.895
1_32x64^3  &  1716.886 & 1716.886 & 55.998 & 30.660
4_32x64^3  &  3797.616 & 3797.616 & 207.328 & 18.317
16_32x64^3  &  5219.022 & 5219.022 & 847.142 & 6.161
64_32x64^3  &  6369.344 & 6369.344 & 3097.873 & 2.056
256_32x64^3  &  6992.505 & 6992.505 & 3298.426 & 2.120
1024_32x64^3  &  7167.825 & 7167.825 & 3260.778 & 2.198
1_32x64^4  &  6196.281 & 6196.281 & 1906.904 & 3.249
4_32x64^4  &  6982.438 & 6982.438 & 1970.866 & 3.543
16_32x64^4  &  7160.937 & 7160.937 & 3430.360 & 2.088
64_32x64^4  &  7223.381 & 7223.381 & 3335.512 & 2.166
1_32x64^5  &  6781.626 & 6781.626 & -1.000 & -6781.626
1_32x128^1  &  1.455 & 1.455 & 0.030 & 47.745
4_32x128^1  &  5.926 & 5.926 & 0.106 & 55.800
16_32x128^1  &  23.273 & 23.273 & 0.419 & 55.524
64_32x128^1  &  81.270 & 81.270 & 1.658 & 49.003
256_32x128^1  &  186.182 & 186.182 & 6.608 & 28.177
1024_32x128^1  &  356.174 & 356.174 & 24.378 & 14.611
1_32x128^2  &  130.612 & 130.612 & 3.078 & 42.430
4_32x128^2  &  533.333 & 533.333 & 11.505 & 46.355
16_32x128^2  &  1878.899 & 1878.899 & 45.875 & 40.957
64_32x128^2  &  4380.749 & 4380.749 & 183.630 & 23.856
256_32x128^2  &  5882.945 & 5882.945 & 739.634 & 7.954
1024_32x128^2  &  7135.112 & 7135.112 & 2898.741 & 2.461
1_32x128^3  &  4765.429 & 4765.429 & 330.097 & 14.436
4_32x128^3  &  6342.194 & 6342.194 & 1243.944 & 5.098
16_32x128^3  &  7287.562 & 7287.562 & 3437.562 & 2.120
64_32x128^3  &  7778.204 & 7778.204 & 3759.415 & 2.069
256_32x128^3  &  7921.041 & 7921.041 & 3800.739 & 2.084
1_32x128^4  &  7667.930 & 7667.930 & 749.569 & 10.230
4_32x128^4  &  7788.134 & 7788.134 & 1964.403 & 3.965
1_64x2^1  &  0.032 & 0.032 & 0.001 & 32.677
4_64x2^1  &  0.120 & 0.120 & 0.002 & 66.186
16_64x2^1  &  0.476 & 0.476 & 0.013 & 36.064
64_64x2^1  &  2.000 & 2.000 & 0.052 & 38.107
256_64x2^1  &  7.529 & 7.529 & 0.213 & 35.396
1024_64x2^1  &  25.098 & 25.098 & 0.845 & 29.685
1_64x2^2  &  1.078 & 1.078 & 0.040 & 26.658
4_64x2^2  &  3.837 & 3.837 & 0.151 & 25.487
16_64x2^2  &  14.588 & 14.588 & 0.596 & 24.467
64_64x2^2  &  52.800 & 52.800 & 2.346 & 22.505
256_64x2^2  &  171.034 & 171.034 & 9.343 & 18.306
1024_64x2^2  &  287.347 & 287.347 & 38.006 & 7.561
1_64x2^3  &  41.451 & 41.451 & 2.120 & 19.554
4_64x2^3  &  140.933 & 140.933 & 7.653 & 18.416
16_64x2^3  &  293.102 & 293.102 & 31.209 & 9.392
64_64x2^3  &  457.390 & 457.390 & 111.318 & 4.109
256_64x2^3  &  544.560 & 544.560 & 38.011 & 14.326
1024_64x2^3  &  571.170 & 571.170 & 34.807 & 16.410
1_64x2^4  &  391.493 & 391.493 & 107.505 & 3.642
4_64x2^4  &  537.118 & 537.118 & 244.228 & 2.199
16_64x2^4  &  569.714 & 569.714 & 240.056 & 2.373
64_64x2^4  &  578.831 & 578.831 & 202.458 & 2.859
1_64x2^5  &  493.661 & 493.661 & 672.870 & 0.734
1_64x4^1  &  0.063 & 0.063 & 0.002 & 33.752
4_64x4^1  &  0.250 & 0.250 & 0.007 & 37.794
16_64x4^1  &  1.013 & 1.013 & 0.027 & 37.807
64_64x4^1  &  4.000 & 4.000 & 0.106 & 37.653
256_64x4^1  &  15.422 & 15.422 & 0.408 & 37.784
1024_64x4^1  &  50.196 & 50.196 & 1.705 & 29.442
1_64x4^2  &  2.194 & 2.194 & 0.083 & 26.543
4_64x4^2  &  7.907 & 7.907 & 0.306 & 25.806
16_64x4^2  &  30.055 & 30.055 & 1.198 & 25.088
64_64x4^2  &  107.739 & 107.739 & 4.842 & 22.253
256_64x4^2  &  358.143 & 358.143 & 18.360 & 19.506
1024_64x4^2  &  619.502 & 619.502 & 78.580 & 7.884
1_64x4^3  &  75.825 & 75.825 & 4.393 & 17.262
4_64x4^3  &  261.557 & 261.557 & 15.820 & 16.534
16_64x4^3  &  521.577 & 521.577 & 61.462 & 8.486
64_64x4^3  &  911.781 & 911.781 & 226.148 & 4.032
256_64x4^3  &  1076.442 & 1076.442 & 78.481 & 13.716
1024_64x4^3  &  1133.913 & 1133.913 & 71.493 & 15.860
1_64x4^4  &  629.482 & 629.482 & 219.270 & 2.871
4_64x4^4  &  1057.349 & 1057.349 & 484.153 & 2.184
16_64x4^4  &  1129.020 & 1129.020 & 476.324 & 2.370
64_64x4^4  &  1146.819 & 1146.819 & 401.489 & 2.856
1_64x4^5  &  820.820 & 820.820 & 1238.287 & 0.663
1_64x8^1  &  0.127 & 0.127 & 0.004 & 32.191
4_64x8^1  &  0.500 & 0.500 & 0.013 & 37.851
16_64x8^1  &  2.051 & 2.051 & 0.052 & 39.699
64_64x8^1  &  7.901 & 7.901 & 0.214 & 36.930
256_64x8^1  &  29.767 & 29.767 & 0.872 & 34.145
1024_64x8^1  &  98.462 & 98.462 & 3.315 & 29.702
1_64x8^2  &  4.260 & 4.260 & 0.177 & 24.042
4_64x8^2  &  16.744 & 16.744 & 0.647 & 25.887
16_64x8^2  &  64.000 & 64.000 & 2.606 & 24.559
64_64x8^2  &  223.723 & 223.723 & 10.052 & 22.256
256_64x8^2  &  709.008 & 709.008 & 40.341 & 17.575
1024_64x8^2  &  1206.616 & 1206.616 & 162.165 & 7.441
1_64x8^3  &  171.136 & 171.136 & 8.925 & 19.176
4_64x8^3  &  559.573 & 559.573 & 34.157 & 16.382
16_64x8^3  &  1025.449 & 1025.449 & 135.567 & 7.564
64_64x8^3  &  1740.442 & 1740.442 & 431.739 & 4.031
256_64x8^3  &  1936.894 & 1936.894 & 165.769 & 11.684
1024_64x8^3  &  2000.137 & 2000.137 & 151.136 & 13.234
1_64x8^4  &  1437.926 & 1437.926 & 460.508 & 3.122
4_64x8^4  &  1902.318 & 1902.318 & 947.715 & 2.007
16_64x8^4  &  1984.809 & 1984.809 & 942.081 & 2.107
64_64x8^4  &  2011.024 & 2011.024 & 805.741 & 2.496
1_64x8^5  &  1581.740 & 1581.740 & 2175.803 & 0.727
1_64x16^1  &  0.260 & 0.260 & 0.008 & 33.850
4_64x16^1  &  1.039 & 1.039 & 0.025 & 41.022
16_64x16^1  &  4.267 & 4.267 & 0.104 & 41.157
64_64x16^1  &  16.000 & 16.000 & 0.417 & 38.350
256_64x16^1  &  58.182 & 58.182 & 1.672 & 34.800
1024_64x16^1  &  195.048 & 195.048 & 6.432 & 30.325
1_64x16^2  &  15.842 & 15.842 & 0.391 & 40.530
4_64x16^2  &  62.117 & 62.117 & 1.446 & 42.948
16_64x16^2  &  230.631 & 230.631 & 5.713 & 40.368
64_64x16^2  &  731.592 & 731.592 & 22.888 & 31.964
256_64x16^2  &  1735.823 & 1735.823 & 90.745 & 19.129
1024_64x16^2  &  2423.668 & 2423.668 & 360.274 & 6.727
1_64x16^3  &  636.213 & 636.213 & 21.063 & 30.205
4_64x16^3  &  1853.793 & 1853.793 & 77.139 & 24.032
16_64x16^3  &  3215.551 & 3215.551 & 219.198 & 14.670
64_64x16^3  &  4366.294 & 4366.294 & 979.043 & 4.460
256_64x16^3  &  4938.127 & 4938.127 & 369.876 & 13.351
1024_64x16^3  &  5166.368 & 5166.368 & 338.087 & 15.281
1_64x16^4  &  4238.101 & 4238.101 & 1120.165 & 3.783
4_64x16^4  &  4900.211 & 4900.211 & 1856.998 & 2.639
16_64x16^4  &  5131.083 & 5131.083 & 1828.807 & 2.806
64_64x16^4  &  5199.085 & 5199.085 & 1607.048 & 3.235
1_64x16^5  &  5216.811 & 5216.811 & 3426.537 & 1.522
1_64x32^1  &  0.526 & 0.526 & 0.016 & 33.077
4_64x32^1  &  2.051 & 2.051 & 0.053 & 38.383
16_64x32^1  &  8.312 & 8.312 & 0.211 & 39.362
64_64x32^1  &  31.605 & 31.605 & 0.832 & 37.987
256_64x32^1  &  119.070 & 119.070 & 3.349 & 35.550
1024_64x32^1  &  264.258 & 264.258 & 13.673 & 19.327
1_64x32^2  &  40.000 & 40.000 & 0.952 & 42.007
4_64x32^2  &  152.079 & 152.079 & 3.331 & 45.652
16_64x32^2  &  574.206 & 574.206 & 13.607 & 42.200
64_64x32^2  &  1585.868 & 1585.868 & 53.749 & 29.505
256_64x32^2  &  3548.881 & 3548.881 & 222.071 & 15.981
1024_64x32^2  &  5249.880 & 5249.880 & 903.321 & 5.812
1_64x32^3  &  1509.053 & 1509.053 & 57.654 & 26.174
4_64x32^3  &  3333.953 & 3333.953 & 212.406 & 15.696
16_64x32^3  &  5207.174 & 5207.174 & 847.670 & 6.143
64_64x32^3  &  7049.589 & 7049.589 & 2098.965 & 3.359
256_64x32^3  &  7923.178 & 7923.178 & 905.939 & 8.746
1024_64x32^3  &  8186.746 & 8186.746 & 831.972 & 9.840
1_64x32^4  &  7024.223 & 7024.223 & 3251.978 & 2.160
4_64x32^4  &  7852.542 & 7852.542 & 3175.768 & 2.473
16_64x32^4  &  8163.089 & 8163.089 & 3470.407 & 2.352
64_64x32^4  &  8243.900 & 8243.900 & 3196.102 & 2.579
1_64x32^5  &  8251.735 & 8251.735 & 4384.094 & 1.882
1_64x64^1  &  0.909 & 0.909 & 0.031 & 29.048
4_64x64^1  &  3.678 & 3.678 & 0.111 & 32.998
16_64x64^1  &  14.713 & 14.713 & 0.445 & 33.033
64_64x64^1  &  56.889 & 56.889 & 1.633 & 34.835
256_64x64^1  &  121.905 & 121.905 & 6.865 & 17.757
1024_64x64^1  &  257.610 & 257.610 & 26.535 & 9.708
1_64x64^2  &  111.304 & 111.304 & 2.575 & 43.228
4_64x64^2  &  449.955 & 449.955 & 9.381 & 47.963
16_64x64^2  &  1463.265 & 1463.265 & 36.844 & 39.715
64_64x64^2  &  3467.513 & 3467.513 & 149.828 & 23.143
256_64x64^2  &  5554.266 & 5554.266 & 393.039 & 14.132
1024_64x64^2  &  7837.074 & 7837.074 & 2416.353 & 3.243
1_64x64^3  &  3511.249 & 3511.249 & 191.341 & 18.351
4_64x64^3  &  5593.648 & 5593.648 & 738.505 & 7.574
16_64x64^3  &  8024.816 & 8024.816 & 2892.857 & 2.774
64_64x64^3  &  9190.015 & 9190.015 & 4072.677 & 2.257
256_64x64^3  &  10637.342 & 10637.342 & 2346.223 & 4.534
1024_64x64^3  &  10760.604 & 10760.604 & 2197.388 & 4.897
1_64x64^4  &  10064.317 & 10064.317 & 3175.392 & 3.169
4_64x64^4  &  10630.939 & 10630.939 & 3352.155 & 3.171
16_64x64^4  &  10777.597 & 10777.597 & 5367.339 & 2.008
64_64x64^4  &  10780.540 & 10780.540 & -1.000 & -10780.540
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 64 -q 64 -r 10 -w 20 -t float --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1073741824] with F_0 [64, 64] x F_1 [64, 64] x F_2 [64, 64] x F_3 [64, 64] x F_4 [64, 64] x to produce Y[1, 1073741824]
Matmul: 1 x 1073741824 x 1073741824, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
Aborted (core dumped)
1_64x64^5  &  1.000 & 1.000 & -1.000 & -1.000
1_64x128^1  &  2.025 & 2.025 & 0.063 & 32.073
4_64x128^1  &  8.312 & 8.312 & 0.218 & 38.053
16_64x128^1  &  32.000 & 32.000 & 0.763 & 41.936
64_64x128^1  &  119.070 & 119.070 & 3.442 & 34.594
256_64x128^1  &  257.610 & 257.610 & 13.249 & 19.444
1024_64x128^1  &  455.111 & 455.111 & 53.545 & 8.500
1_64x128^2  &  323.368 & 323.368 & 7.470 & 43.292
4_64x128^2  &  1050.256 & 1050.256 & 26.705 & 39.327
16_64x128^2  &  3171.097 & 3171.097 & 107.757 & 29.428
64_64x128^2  &  5585.455 & 5585.455 & 434.889 & 12.843
256_64x128^2  &  7825.194 & 7825.194 & 1758.440 & 4.450
1024_64x128^2  &  9102.222 & 9102.222 & 5169.718 & 1.761
1_64x128^3  &  6407.151 & 6407.151 & 875.778 & 7.316
4_64x128^3  &  8340.945 & 8340.945 & 3133.842 & 2.662
16_64x128^3  &  9291.180 & 9291.180 & 5764.713 & 1.612
64_64x128^3  &  9645.245 & 9645.245 & 5782.053 & 1.668
256_64x128^3  &  9754.114 & 9754.114 & 4759.989 & 2.049
1_64x128^4  &  9392.476 & 9392.476 & 1410.484 & 6.659
4_64x128^4  &  9750.039 & 9750.039 & 3502.814 & 2.783
1_128x2^1  &  0.054 & 0.054 & 0.002 & 27.017
4_128x2^1  &  0.211 & 0.211 & 0.007 & 32.279
16_128x2^1  &  0.833 & 0.833 & 0.027 & 30.513
64_128x2^1  &  3.333 & 3.333 & 0.107 & 31.112
256_128x2^1  &  8.767 & 8.767 & 0.425 & 20.645
1024_128x2^1  &  19.248 & 19.248 & 1.754 & 10.972
1_128x2^2  &  3.385 & 3.385 & 0.162 & 20.917
4_128x2^2  &  11.872 & 11.872 & 0.583 & 20.352
16_128x2^2  &  36.491 & 36.491 & 2.330 & 15.662
64_128x2^2  &  105.316 & 105.316 & 8.808 & 11.957
256_128x2^2  &  183.058 & 183.058 & 36.532 & 5.011
1024_128x2^2  &  290.022 & 290.022 & 141.989 & 2.043
1_128x2^3  &  139.866 & 139.866 & 16.649 & 8.401
4_128x2^3  &  273.975 & 273.975 & 58.260 & 4.703
16_128x2^3  &  340.978 & 340.978 & 232.942 & 1.464
64_128x2^3  &  402.880 & 402.880 & 114.053 & 3.532
256_128x2^3  &  413.829 & 413.829 & 36.436 & 11.358
1_128x2^4  &  407.896 & 407.896 & 716.430 & 0.569
4_128x2^4  &  414.821 & 414.821 & 138.120 & 3.003
1_128x4^1  &  0.106 & 0.106 & 0.004 & 27.634
4_128x4^1  &  0.426 & 0.426 & 0.013 & 31.589
16_128x4^1  &  1.702 & 1.702 & 0.052 & 32.479
64_128x4^1  &  6.737 & 6.737 & 0.201 & 33.551
256_128x4^1  &  17.534 & 17.534 & 0.860 & 20.379
1024_128x4^1  &  38.209 & 38.209 & 3.369 & 11.341
1_128x4^2  &  6.735 & 6.735 & 0.326 & 20.643
4_128x4^2  &  26.269 & 26.269 & 1.210 & 21.715
16_128x4^2  &  81.231 & 81.231 & 4.217 & 19.262
64_128x4^2  &  236.639 & 236.639 & 19.189 & 12.332
256_128x4^2  &  478.640 & 478.640 & 72.923 & 6.564
1024_128x4^2  &  615.519 & 615.519 & 285.742 & 2.154
1_128x4^3  &  275.889 & 275.889 & 33.160 & 8.320
4_128x4^3  &  523.186 & 523.186 & 124.014 & 4.219
16_128x4^3  &  662.728 & 662.728 & 473.868 & 1.399
64_128x4^3  &  796.269 & 796.269 & 229.365 & 3.472
256_128x4^3  &  812.650 & 812.650 & 73.792 & 11.013
1_128x4^4  &  801.214 & 801.214 & 1364.426 & 0.587
4_128x4^4  &  813.789 & 813.789 & 277.470 & 2.933
1_128x8^1  &  0.215 & 0.215 & 0.008 & 27.731
4_128x8^1  &  0.842 & 0.842 & 0.026 & 31.822
16_128x8^1  &  3.404 & 3.404 & 0.098 & 34.747
64_128x8^1  &  13.196 & 13.196 & 0.399 & 33.036
256_128x8^1  &  33.907 & 33.907 & 1.655 & 20.488
1024_128x8^1  &  73.669 & 73.669 & 6.782 & 10.863
1_128x8^2  &  13.465 & 13.465 & 0.662 & 20.325
4_128x8^2  &  47.504 & 47.504 & 2.433 & 19.521
16_128x8^2  &  146.547 & 146.547 & 9.836 & 14.899
64_128x8^2  &  426.699 & 426.699 & 39.754 & 10.734
256_128x8^2  &  697.737 & 697.737 & 136.144 & 5.125
1024_128x8^2  &  1213.630 & 1213.630 & 581.306 & 2.088
1_128x8^3  &  626.237 & 626.237 & 67.353 & 9.298
4_128x8^3  &  1226.105 & 1226.105 & 248.457 & 4.935
16_128x8^3  &  1554.794 & 1554.794 & 935.317 & 1.662
64_128x8^3  &  1621.061 & 1621.061 & 464.338 & 3.491
256_128x8^3  &  1629.210 & 1629.210 & 151.722 & 10.738
1_128x8^4  &  1609.301 & 1609.301 & 2508.036 & 0.642
4_128x8^4  &  1626.960 & 1626.960 & 558.779 & 2.912
1_128x16^1  &  0.449 & 0.449 & 0.015 & 29.834
4_128x16^1  &  1.778 & 1.778 & 0.053 & 33.420
16_128x16^1  &  7.111 & 7.111 & 0.211 & 33.763
64_128x16^1  &  27.234 & 27.234 & 0.785 & 34.698
256_128x16^1  &  74.203 & 74.203 & 3.370 & 22.018
1024_128x16^1  &  151.704 & 151.704 & 13.280 & 11.423
1_128x16^2  &  52.364 & 52.364 & 1.402 & 37.360
4_128x16^2  &  198.621 & 198.621 & 5.076 & 39.132
16_128x16^2  &  725.669 & 725.669 & 19.479 & 37.254
64_128x16^2  &  1843.200 & 1843.200 & 82.045 & 22.466
256_128x16^2  &  3536.115 & 3536.115 & 325.873 & 10.851
1024_128x16^2  &  4551.111 & 4551.111 & 1165.287 & 3.906
1_128x16^3  &  2099.775 & 2099.775 & 105.488 & 19.905
4_128x16^3  &  3714.385 & 3714.385 & 368.565 & 10.078
16_128x16^3  &  5128.782 & 5128.782 & 1844.230 & 2.781
64_128x16^3  &  5546.172 & 5546.172 & 954.682 & 5.809
256_128x16^3  &  5687.604 & 5687.604 & 319.805 & 17.785
1_128x16^4  &  5638.023 & 5638.023 & 4323.222 & 1.304
4_128x16^4  &  5702.767 & 5702.767 & 1137.204 & 5.015
1_128x32^1  &  0.860 & 0.860 & 0.030 & 28.857
4_128x32^1  &  3.368 & 3.368 & 0.106 & 31.765
16_128x32^1  &  13.617 & 13.617 & 0.383 & 35.590
64_128x32^1  &  53.895 & 53.895 & 1.664 & 32.386
256_128x32^1  &  131.282 & 131.282 & 6.657 & 19.722
1024_128x32^1  &  212.228 & 212.228 & 27.275 & 7.781
1_128x32^2  &  100.000 & 100.000 & 3.223 & 31.030
4_128x32^2  &  385.053 & 385.053 & 11.212 & 34.344
16_128x32^2  &  1211.834 & 1211.834 & 44.184 & 27.427
64_128x32^2  &  3456.540 & 3456.540 & 183.778 & 18.808
256_128x32^2  &  5389.474 & 5389.474 & 766.189 & 7.034
1024_128x32^2  &  7515.596 & 7515.596 & 2361.962 & 3.182
1_128x32^3  &  3797.616 & 3797.616 & 330.658 & 11.485
4_128x32^3  &  6188.202 & 6188.202 & 1264.217 & 4.895
16_128x32^3  &  8095.624 & 8095.624 & 3586.267 & 2.257
64_128x32^3  &  8926.583 & 8926.583 & 2005.666 & 4.451
256_128x32^3  &  9225.009 & 9225.009 & 713.849 & 12.923
1_128x32^4  &  9151.193 & 9151.193 & 6314.659 & 1.449
4_128x32^4  &  9294.336 & 9294.336 & 2292.763 & 4.054
1_128x64^1  &  1.684 & 1.684 & 0.064 & 26.512
4_128x64^1  &  6.882 & 6.882 & 0.195 & 35.381
16_128x64^1  &  27.234 & 27.234 & 0.733 & 37.170
64_128x64^1  &  69.660 & 69.660 & 3.393 & 20.530
256_128x64^1  &  178.087 & 178.087 & 13.217 & 13.474
1024_128x64^1  &  305.102 & 305.102 & 53.918 & 5.659
1_128x64^2  &  281.835 & 281.835 & 7.463 & 37.766
4_128x64^2  &  975.238 & 975.238 & 27.483 & 35.485
16_128x64^2  &  2482.424 & 2482.424 & 110.820 & 22.401
64_128x64^2  &  5270.992 & 5270.992 & 433.817 & 12.150
256_128x64^2  &  7695.029 & 7695.029 & 1813.384 & 4.243
1024_128x64^2  &  9230.422 & 9230.422 & 4241.277 & 2.176
1_128x64^3  &  6157.745 & 6157.745 & 875.680 & 7.032
4_128x64^3  &  8269.527 & 8269.527 & 3333.742 & 2.481
16_128x64^3  &  9471.009 & 9471.009 & 5820.001 & 1.627
64_128x64^3  &  9865.966 & 9865.966 & 4013.648 & 2.458
256_128x64^3  &  9938.015 & 9938.015 & 1698.276 & 5.852
1_128x64^4  &  9886.319 & 9886.319 & 5181.917 & 1.908
4_128x64^4  &  9932.794 & 9932.794 & 3674.627 & 2.703
1_128x128^1  &  3.333 & 3.333 & 0.124 & 26.974
4_128x128^1  &  13.617 & 13.617 & 0.423 & 32.175
16_128x128^1  &  52.784 & 52.784 & 1.688 & 31.262
64_128x128^1  &  92.252 & 92.252 & 6.298 & 14.648
256_128x128^1  &  206.348 & 206.348 & 25.985 & 7.941
1024_128x128^1  &  251.868 & 251.868 & 106.996 & 2.354
1_128x128^2  &  827.475 & 827.475 & 19.525 & 42.380
4_128x128^2  &  1476.244 & 1476.244 & 74.928 & 19.702
16_128x128^2  &  4582.937 & 4582.937 & 277.493 & 16.516
64_128x128^2  &  7415.672 & 7415.672 & 1193.323 & 6.214
256_128x128^2  &  9312.527 & 9312.527 & 4688.319 & 1.986
1024_128x128^2  &  11551.374 & 11551.374 & 6331.950 & 1.824
1_128x128^3  &  9379.034 & 9379.034 & 3055.774 & 3.069
4_128x128^3  &  11049.272 & 11049.272 & 7128.487 & 1.550
16_128x128^3  &  11975.742 & 11975.742 & 7637.778 & 1.568
64_128x128^3  &  12301.065 & 12301.065 & 6571.385 & 1.872
256_128x128^3  &  12345.719 & 12345.719 & 3976.481 & 3.105
1_128x128^4  &  11201.893 & 11201.893 & 2574.194 & 4.352
4_128x128^4  &  12356.105 & 12356.105 & -1.000 & -12356.105
------- Single CUDA DOUBLE FastTune NN -------
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2] with F_0 [2, 2] x to produce Y[1, 2]
Matmul: 1 x 2 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^1  &  1.000 & 1.000 & 0.000 & 34266.710
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2] with F_0 [2, 2] x to produce Y[4, 2]
Matmul: 4 x 2 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^1  &  1.000 & 1.000 & 0.000 & 11143.833
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2] with F_0 [2, 2] x to produce Y[16, 2]
Matmul: 16 x 2 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^1  &  1.000 & 1.000 & 0.000 & 2786.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2] with F_0 [2, 2] x to produce Y[64, 2]
Matmul: 64 x 2 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^1  &  1.000 & 1.000 & 0.002 & 616.652
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2] with F_0 [2, 2] x to produce Y[256, 2]
Matmul: 256 x 2 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^1  &  1.000 & 1.000 & 0.007 & 152.749
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 2] with F_0 [2, 2] x to produce Y[1024, 2]
Matmul: 1024 x 2 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^1  &  1.000 & 1.000 & 0.027 & 37.273
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [2, 2] x F_1 [2, 2] x to produce Y[1, 4]
Matmul: 1 x 4 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4*(2x2^1)_NN_d
Tuning for shape 1x4*(2x2^2)_NN_d
Tuning for shape 1x4*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^2  &  1.000 & 1.000 & 0.000 & 13972.074
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [2, 2] x F_1 [2, 2] x to produce Y[4, 4]
Matmul: 4 x 4 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4*(2x2^1)_NN_d
Tuning for shape 4x4*(2x2^2)_NN_d
Tuning for shape 4x4*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^2  &  1.000 & 1.000 & 0.000 & 3605.522
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [2, 2] x F_1 [2, 2] x to produce Y[16, 4]
Matmul: 16 x 4 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4*(2x2^1)_NN_d
Tuning for shape 16x4*(2x2^2)_NN_d
Tuning for shape 16x4*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^2  &  1.000 & 1.000 & 0.001 & 873.185
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [2, 2] x F_1 [2, 2] x to produce Y[64, 4]
Matmul: 64 x 4 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4*(2x2^1)_NN_d
Tuning for shape 64x4*(2x2^2)_NN_d
Tuning for shape 64x4*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^2  &  1.000 & 1.000 & 0.005 & 220.077
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [2, 2] x F_1 [2, 2] x to produce Y[256, 4]
Matmul: 256 x 4 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4*(2x2^1)_NN_d
Tuning for shape 256x4*(2x2^2)_NN_d
Tuning for shape 256x4*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^2  &  1.000 & 1.000 & 0.018 & 55.127
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [2, 2] x F_1 [2, 2] x to produce Y[1024, 4]
Matmul: 1024 x 4 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4*(2x2^1)_NN_d
Tuning for shape 1024x4*(2x2^2)_NN_d
Tuning for shape 1024x4*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^2  &  1.000 & 1.000 & 0.069 & 14.467
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x to produce Y[1, 8]
Matmul: 1 x 8 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(2x2^1)_NN_d
Tuning for shape 1x8*(2x2^2)_NN_d
Tuning for shape 1x8*(2x2^3)_NN_d
Tuning for shape 1x8*(2x2^1)_NN_d
Tuning for shape 1x8*(2x2^2)_NN_d
Tuning for shape 1x8*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^3  &  1.000 & 1.000 & 0.000 & 5400.926
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x to produce Y[4, 8]
Matmul: 4 x 8 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(2x2^1)_NN_d
Tuning for shape 4x8*(2x2^2)_NN_d
Tuning for shape 4x8*(2x2^3)_NN_d
Tuning for shape 4x8*(2x2^1)_NN_d
Tuning for shape 4x8*(2x2^2)_NN_d
Tuning for shape 4x8*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^3  &  1.000 & 1.000 & 0.001 & 1433.895
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x to produce Y[16, 8]
Matmul: 16 x 8 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(2x2^1)_NN_d
Tuning for shape 16x8*(2x2^2)_NN_d
Tuning for shape 16x8*(2x2^3)_NN_d
Tuning for shape 16x8*(2x2^1)_NN_d
Tuning for shape 16x8*(2x2^2)_NN_d
Tuning for shape 16x8*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^3  &  1.000 & 1.000 & 0.003 & 351.512
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x to produce Y[64, 8]
Matmul: 64 x 8 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(2x2^1)_NN_d
Tuning for shape 64x8*(2x2^2)_NN_d
Tuning for shape 64x8*(2x2^3)_NN_d
Tuning for shape 64x8*(2x2^1)_NN_d
Tuning for shape 64x8*(2x2^2)_NN_d
Tuning for shape 64x8*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^3  &  1.000 & 1.000 & 0.011 & 89.073
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x to produce Y[256, 8]
Matmul: 256 x 8 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(2x2^1)_NN_d
Tuning for shape 256x8*(2x2^2)_NN_d
Tuning for shape 256x8*(2x2^3)_NN_d
Tuning for shape 256x8*(2x2^1)_NN_d
Tuning for shape 256x8*(2x2^2)_NN_d
Tuning for shape 256x8*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^3  &  1.000 & 1.000 & 0.045 & 22.262
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x to produce Y[1024, 8]
Matmul: 1024 x 8 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(2x2^1)_NN_d
Tuning for shape 1024x8*(2x2^2)_NN_d
Tuning for shape 1024x8*(2x2^3)_NN_d
Tuning for shape 1024x8*(2x2^1)_NN_d
Tuning for shape 1024x8*(2x2^2)_NN_d
Tuning for shape 1024x8*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^3  &  1.000 & 1.000 & 0.182 & 5.480
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x to produce Y[1, 16]
Matmul: 1 x 16 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16*(2x2^1)_NN_d
Tuning for shape 1x16*(2x2^2)_NN_d
Tuning for shape 1x16*(2x2^3)_NN_d
Tuning for shape 1x16*(2x2^4)_NN_d
Tuning for shape 1x16*(2x2^1)_NN_d
Tuning for shape 1x16*(2x2^2)_NN_d
Tuning for shape 1x16*(2x2^3)_NN_d
Tuning for shape 1x16*(2x2^1)_NN_d
Tuning for shape 1x16*(2x2^2)_NN_d
Tuning for shape 1x16*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^4  &  1.000 & 1.000 & 0.000 & 2382.090
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x to produce Y[4, 16]
Matmul: 4 x 16 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16*(2x2^1)_NN_d
Tuning for shape 4x16*(2x2^2)_NN_d
Tuning for shape 4x16*(2x2^3)_NN_d
Tuning for shape 4x16*(2x2^4)_NN_d
Tuning for shape 4x16*(2x2^1)_NN_d
Tuning for shape 4x16*(2x2^2)_NN_d
Tuning for shape 4x16*(2x2^3)_NN_d
Tuning for shape 4x16*(2x2^1)_NN_d
Tuning for shape 4x16*(2x2^2)_NN_d
Tuning for shape 4x16*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^4  &  1.000 & 1.000 & 0.002 & 636.117
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x to produce Y[16, 16]
Matmul: 16 x 16 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16*(2x2^1)_NN_d
Tuning for shape 16x16*(2x2^2)_NN_d
Tuning for shape 16x16*(2x2^3)_NN_d
Tuning for shape 16x16*(2x2^4)_NN_d
Tuning for shape 16x16*(2x2^1)_NN_d
Tuning for shape 16x16*(2x2^2)_NN_d
Tuning for shape 16x16*(2x2^3)_NN_d
Tuning for shape 16x16*(2x2^1)_NN_d
Tuning for shape 16x16*(2x2^2)_NN_d
Tuning for shape 16x16*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^4  &  1.000 & 1.000 & 0.006 & 159.530
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x to produce Y[64, 16]
Matmul: 64 x 16 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16*(2x2^1)_NN_d
Tuning for shape 64x16*(2x2^2)_NN_d
Tuning for shape 64x16*(2x2^3)_NN_d
Tuning for shape 64x16*(2x2^4)_NN_d
Tuning for shape 64x16*(2x2^1)_NN_d
Tuning for shape 64x16*(2x2^2)_NN_d
Tuning for shape 64x16*(2x2^3)_NN_d
Tuning for shape 64x16*(2x2^1)_NN_d
Tuning for shape 64x16*(2x2^2)_NN_d
Tuning for shape 64x16*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^4  &  1.000 & 1.000 & 0.026 & 38.884
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x to produce Y[256, 16]
Matmul: 256 x 16 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16*(2x2^1)_NN_d
Tuning for shape 256x16*(2x2^2)_NN_d
Tuning for shape 256x16*(2x2^3)_NN_d
Tuning for shape 256x16*(2x2^4)_NN_d
Tuning for shape 256x16*(2x2^1)_NN_d
Tuning for shape 256x16*(2x2^2)_NN_d
Tuning for shape 256x16*(2x2^3)_NN_d
Tuning for shape 256x16*(2x2^1)_NN_d
Tuning for shape 256x16*(2x2^2)_NN_d
Tuning for shape 256x16*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^4  &  1.000 & 1.000 & 0.102 & 9.819
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16*(2x2^1)_NN_d
Tuning for shape 1024x16*(2x2^2)_NN_d
Tuning for shape 1024x16*(2x2^3)_NN_d
Tuning for shape 1024x16*(2x2^4)_NN_d
Tuning for shape 1024x16*(2x2^1)_NN_d
Tuning for shape 1024x16*(2x2^2)_NN_d
Tuning for shape 1024x16*(2x2^3)_NN_d
Tuning for shape 1024x16*(2x2^1)_NN_d
Tuning for shape 1024x16*(2x2^2)_NN_d
Tuning for shape 1024x16*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^4  &  1.000 & 1.000 & 0.400 & 2.500
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x to produce Y[1, 32]
Matmul: 1 x 32 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32*(2x2^1)_NN_d
Tuning for shape 1x32*(2x2^2)_NN_d
Tuning for shape 1x32*(2x2^3)_NN_d
Tuning for shape 1x32*(2x2^4)_NN_d
Tuning for shape 1x32*(2x2^5)_NN_d
Tuning for shape 1x32*(2x2^1)_NN_d
Tuning for shape 1x32*(2x2^2)_NN_d
Tuning for shape 1x32*(2x2^3)_NN_d
Tuning for shape 1x32*(2x2^4)_NN_d
Tuning for shape 1x32*(2x2^1)_NN_d
Tuning for shape 1x32*(2x2^2)_NN_d
Tuning for shape 1x32*(2x2^3)_NN_d
Tuning for shape 1x32*(2x2^1)_NN_d
Tuning for shape 1x32*(2x2^2)_NN_d
Tuning for shape 1x32*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^5  &  1.000 & 1.000 & 0.001 & 1088.884
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x to produce Y[4, 32]
Matmul: 4 x 32 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32*(2x2^1)_NN_d
Tuning for shape 4x32*(2x2^2)_NN_d
Tuning for shape 4x32*(2x2^3)_NN_d
Tuning for shape 4x32*(2x2^4)_NN_d
Tuning for shape 4x32*(2x2^5)_NN_d
Tuning for shape 4x32*(2x2^1)_NN_d
Tuning for shape 4x32*(2x2^2)_NN_d
Tuning for shape 4x32*(2x2^3)_NN_d
Tuning for shape 4x32*(2x2^4)_NN_d
Tuning for shape 4x32*(2x2^1)_NN_d
Tuning for shape 4x32*(2x2^2)_NN_d
Tuning for shape 4x32*(2x2^3)_NN_d
Tuning for shape 4x32*(2x2^1)_NN_d
Tuning for shape 4x32*(2x2^2)_NN_d
Tuning for shape 4x32*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^5  &  1.000 & 1.000 & 0.003 & 291.103
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x to produce Y[16, 32]
Matmul: 16 x 32 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32*(2x2^1)_NN_d
Tuning for shape 16x32*(2x2^2)_NN_d
Tuning for shape 16x32*(2x2^3)_NN_d
Tuning for shape 16x32*(2x2^4)_NN_d
Tuning for shape 16x32*(2x2^5)_NN_d
Tuning for shape 16x32*(2x2^1)_NN_d
Tuning for shape 16x32*(2x2^2)_NN_d
Tuning for shape 16x32*(2x2^3)_NN_d
Tuning for shape 16x32*(2x2^4)_NN_d
Tuning for shape 16x32*(2x2^1)_NN_d
Tuning for shape 16x32*(2x2^2)_NN_d
Tuning for shape 16x32*(2x2^3)_NN_d
Tuning for shape 16x32*(2x2^1)_NN_d
Tuning for shape 16x32*(2x2^2)_NN_d
Tuning for shape 16x32*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^5  &  1.000 & 1.000 & 0.014 & 71.820
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 32] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x to produce Y[64, 32]
Matmul: 64 x 32 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32*(2x2^1)_NN_d
Tuning for shape 64x32*(2x2^2)_NN_d
Tuning for shape 64x32*(2x2^3)_NN_d
Tuning for shape 64x32*(2x2^4)_NN_d
Tuning for shape 64x32*(2x2^5)_NN_d
Tuning for shape 64x32*(2x2^1)_NN_d
Tuning for shape 64x32*(2x2^2)_NN_d
Tuning for shape 64x32*(2x2^3)_NN_d
Tuning for shape 64x32*(2x2^4)_NN_d
Tuning for shape 64x32*(2x2^1)_NN_d
Tuning for shape 64x32*(2x2^2)_NN_d
Tuning for shape 64x32*(2x2^3)_NN_d
Tuning for shape 64x32*(2x2^1)_NN_d
Tuning for shape 64x32*(2x2^2)_NN_d
Tuning for shape 64x32*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^5  &  1.000 & 1.000 & 0.055 & 18.046
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 32] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x to produce Y[256, 32]
Matmul: 256 x 32 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32*(2x2^1)_NN_d
Tuning for shape 256x32*(2x2^2)_NN_d
Tuning for shape 256x32*(2x2^3)_NN_d
Tuning for shape 256x32*(2x2^4)_NN_d
Tuning for shape 256x32*(2x2^5)_NN_d
Tuning for shape 256x32*(2x2^1)_NN_d
Tuning for shape 256x32*(2x2^2)_NN_d
Tuning for shape 256x32*(2x2^3)_NN_d
Tuning for shape 256x32*(2x2^4)_NN_d
Tuning for shape 256x32*(2x2^1)_NN_d
Tuning for shape 256x32*(2x2^2)_NN_d
Tuning for shape 256x32*(2x2^3)_NN_d
Tuning for shape 256x32*(2x2^1)_NN_d
Tuning for shape 256x32*(2x2^2)_NN_d
Tuning for shape 256x32*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^5  &  1.000 & 1.000 & 0.225 & 4.439
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 5 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 32] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x to produce Y[1024, 32]
Matmul: 1024 x 32 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32*(2x2^1)_NN_d
Tuning for shape 1024x32*(2x2^2)_NN_d
Tuning for shape 1024x32*(2x2^3)_NN_d
Tuning for shape 1024x32*(2x2^4)_NN_d
Tuning for shape 1024x32*(2x2^5)_NN_d
Tuning for shape 1024x32*(2x2^1)_NN_d
Tuning for shape 1024x32*(2x2^2)_NN_d
Tuning for shape 1024x32*(2x2^3)_NN_d
Tuning for shape 1024x32*(2x2^4)_NN_d
Tuning for shape 1024x32*(2x2^1)_NN_d
Tuning for shape 1024x32*(2x2^2)_NN_d
Tuning for shape 1024x32*(2x2^3)_NN_d
Tuning for shape 1024x32*(2x2^1)_NN_d
Tuning for shape 1024x32*(2x2^2)_NN_d
Tuning for shape 1024x32*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^5  &  1.000 & 1.000 & 0.904 & 1.106
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x to produce Y[1, 64]
Matmul: 1 x 64 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x64*(2x2^1)_NN_d
Tuning for shape 1x64*(2x2^2)_NN_d
Tuning for shape 1x64*(2x2^3)_NN_d
Tuning for shape 1x64*(2x2^4)_NN_d
Tuning for shape 1x64*(2x2^5)_NN_d
Tuning for shape 1x64*(2x2^6)_NN_d
Tuning for shape 1x64*(2x2^1)_NN_d
Tuning for shape 1x64*(2x2^2)_NN_d
Tuning for shape 1x64*(2x2^3)_NN_d
Tuning for shape 1x64*(2x2^4)_NN_d
Tuning for shape 1x64*(2x2^5)_NN_d
Tuning for shape 1x64*(2x2^1)_NN_d
Tuning for shape 1x64*(2x2^2)_NN_d
Tuning for shape 1x64*(2x2^3)_NN_d
Tuning for shape 1x64*(2x2^4)_NN_d
Tuning for shape 1x64*(2x2^1)_NN_d
Tuning for shape 1x64*(2x2^2)_NN_d
Tuning for shape 1x64*(2x2^3)_NN_d
Tuning for shape 1x64*(2x2^1)_NN_d
Tuning for shape 1x64*(2x2^2)_NN_d
Tuning for shape 1x64*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^6  &  1.000 & 1.000 & 0.002 & 516.426
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x to produce Y[4, 64]
Matmul: 4 x 64 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x64*(2x2^1)_NN_d
Tuning for shape 4x64*(2x2^2)_NN_d
Tuning for shape 4x64*(2x2^3)_NN_d
Tuning for shape 4x64*(2x2^4)_NN_d
Tuning for shape 4x64*(2x2^5)_NN_d
Tuning for shape 4x64*(2x2^6)_NN_d
Tuning for shape 4x64*(2x2^1)_NN_d
Tuning for shape 4x64*(2x2^2)_NN_d
Tuning for shape 4x64*(2x2^3)_NN_d
Tuning for shape 4x64*(2x2^4)_NN_d
Tuning for shape 4x64*(2x2^5)_NN_d
Tuning for shape 4x64*(2x2^1)_NN_d
Tuning for shape 4x64*(2x2^2)_NN_d
Tuning for shape 4x64*(2x2^3)_NN_d
Tuning for shape 4x64*(2x2^4)_NN_d
Tuning for shape 4x64*(2x2^1)_NN_d
Tuning for shape 4x64*(2x2^2)_NN_d
Tuning for shape 4x64*(2x2^3)_NN_d
Tuning for shape 4x64*(2x2^1)_NN_d
Tuning for shape 4x64*(2x2^2)_NN_d
Tuning for shape 4x64*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^6  &  1.000 & 1.000 & 0.007 & 137.626
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x to produce Y[16, 64]
Matmul: 16 x 64 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x64*(2x2^1)_NN_d
Tuning for shape 16x64*(2x2^2)_NN_d
Tuning for shape 16x64*(2x2^3)_NN_d
Tuning for shape 16x64*(2x2^4)_NN_d
Tuning for shape 16x64*(2x2^5)_NN_d
Tuning for shape 16x64*(2x2^6)_NN_d
Tuning for shape 16x64*(2x2^1)_NN_d
Tuning for shape 16x64*(2x2^2)_NN_d
Tuning for shape 16x64*(2x2^3)_NN_d
Tuning for shape 16x64*(2x2^4)_NN_d
Tuning for shape 16x64*(2x2^5)_NN_d
Tuning for shape 16x64*(2x2^1)_NN_d
Tuning for shape 16x64*(2x2^2)_NN_d
Tuning for shape 16x64*(2x2^3)_NN_d
Tuning for shape 16x64*(2x2^4)_NN_d
Tuning for shape 16x64*(2x2^1)_NN_d
Tuning for shape 16x64*(2x2^2)_NN_d
Tuning for shape 16x64*(2x2^3)_NN_d
Tuning for shape 16x64*(2x2^1)_NN_d
Tuning for shape 16x64*(2x2^2)_NN_d
Tuning for shape 16x64*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^6  &  1.000 & 1.000 & 0.029 & 34.317
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 6 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x to produce Y[64, 64]
Matmul: 64 x 64 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x64*(2x2^1)_NN_d
Tuning for shape 64x64*(2x2^2)_NN_d
Tuning for shape 64x64*(2x2^3)_NN_d
Tuning for shape 64x64*(2x2^4)_NN_d
Tuning for shape 64x64*(2x2^5)_NN_d
Tuning for shape 64x64*(2x2^6)_NN_d
Tuning for shape 64x64*(2x2^1)_NN_d
Tuning for shape 64x64*(2x2^2)_NN_d
Tuning for shape 64x64*(2x2^3)_NN_d
Tuning for shape 64x64*(2x2^4)_NN_d
Tuning for shape 64x64*(2x2^5)_NN_d
Tuning for shape 64x64*(2x2^1)_NN_d
Tuning for shape 64x64*(2x2^2)_NN_d
Tuning for shape 64x64*(2x2^3)_NN_d
Tuning for shape 64x64*(2x2^4)_NN_d
Tuning for shape 64x64*(2x2^1)_NN_d
Tuning for shape 64x64*(2x2^2)_NN_d
Tuning for shape 64x64*(2x2^3)_NN_d
Tuning for shape 64x64*(2x2^1)_NN_d
Tuning for shape 64x64*(2x2^2)_NN_d
Tuning for shape 64x64*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^6  &  1.000 & 1.000 & 0.119 & 8.429
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 6 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x to produce Y[256, 64]
Matmul: 256 x 64 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x64*(2x2^1)_NN_d
Tuning for shape 256x64*(2x2^2)_NN_d
Tuning for shape 256x64*(2x2^3)_NN_d
Tuning for shape 256x64*(2x2^4)_NN_d
Tuning for shape 256x64*(2x2^5)_NN_d
Tuning for shape 256x64*(2x2^6)_NN_d
Tuning for shape 256x64*(2x2^1)_NN_d
Tuning for shape 256x64*(2x2^2)_NN_d
Tuning for shape 256x64*(2x2^3)_NN_d
Tuning for shape 256x64*(2x2^4)_NN_d
Tuning for shape 256x64*(2x2^5)_NN_d
Tuning for shape 256x64*(2x2^1)_NN_d
Tuning for shape 256x64*(2x2^2)_NN_d
Tuning for shape 256x64*(2x2^3)_NN_d
Tuning for shape 256x64*(2x2^4)_NN_d
Tuning for shape 256x64*(2x2^1)_NN_d
Tuning for shape 256x64*(2x2^2)_NN_d
Tuning for shape 256x64*(2x2^3)_NN_d
Tuning for shape 256x64*(2x2^1)_NN_d
Tuning for shape 256x64*(2x2^2)_NN_d
Tuning for shape 256x64*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^6  &  1.000 & 1.000 & 0.483 & 2.070
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 6 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x64*(2x2^1)_NN_d
Tuning for shape 1024x64*(2x2^2)_NN_d
Tuning for shape 1024x64*(2x2^3)_NN_d
Tuning for shape 1024x64*(2x2^4)_NN_d
Tuning for shape 1024x64*(2x2^5)_NN_d
Tuning for shape 1024x64*(2x2^6)_NN_d
Tuning for shape 1024x64*(2x2^1)_NN_d
Tuning for shape 1024x64*(2x2^2)_NN_d
Tuning for shape 1024x64*(2x2^3)_NN_d
Tuning for shape 1024x64*(2x2^4)_NN_d
Tuning for shape 1024x64*(2x2^5)_NN_d
Tuning for shape 1024x64*(2x2^1)_NN_d
Tuning for shape 1024x64*(2x2^2)_NN_d
Tuning for shape 1024x64*(2x2^3)_NN_d
Tuning for shape 1024x64*(2x2^4)_NN_d
Tuning for shape 1024x64*(2x2^1)_NN_d
Tuning for shape 1024x64*(2x2^2)_NN_d
Tuning for shape 1024x64*(2x2^3)_NN_d
Tuning for shape 1024x64*(2x2^1)_NN_d
Tuning for shape 1024x64*(2x2^2)_NN_d
Tuning for shape 1024x64*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^6  &  1.000 & 1.000 & 1.900 & 0.526
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 128] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x to produce Y[1, 128]
Matmul: 1 x 128 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(2x2^1)_NN_d
Tuning for shape 1x128*(2x2^2)_NN_d
Tuning for shape 1x128*(2x2^3)_NN_d
Tuning for shape 1x128*(2x2^4)_NN_d
Tuning for shape 1x128*(2x2^5)_NN_d
Tuning for shape 1x128*(2x2^6)_NN_d
Tuning for shape 1x128*(2x2^7)_NN_d
Tuning for shape 1x128*(2x2^1)_NN_d
Tuning for shape 1x128*(2x2^2)_NN_d
Tuning for shape 1x128*(2x2^3)_NN_d
Tuning for shape 1x128*(2x2^4)_NN_d
Tuning for shape 1x128*(2x2^5)_NN_d
Tuning for shape 1x128*(2x2^6)_NN_d
Tuning for shape 1x128*(2x2^1)_NN_d
Tuning for shape 1x128*(2x2^2)_NN_d
Tuning for shape 1x128*(2x2^3)_NN_d
Tuning for shape 1x128*(2x2^4)_NN_d
Tuning for shape 1x128*(2x2^5)_NN_d
Tuning for shape 1x128*(2x2^1)_NN_d
Tuning for shape 1x128*(2x2^2)_NN_d
Tuning for shape 1x128*(2x2^3)_NN_d
Tuning for shape 1x128*(2x2^4)_NN_d
Tuning for shape 1x128*(2x2^1)_NN_d
Tuning for shape 1x128*(2x2^2)_NN_d
Tuning for shape 1x128*(2x2^3)_NN_d
Tuning for shape 1x128*(2x2^1)_NN_d
Tuning for shape 1x128*(2x2^2)_NN_d
Tuning for shape 1x128*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^7  &  1.000 & 1.000 & 0.004 & 251.966
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 7 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 128] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x to produce Y[4, 128]
Matmul: 4 x 128 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(2x2^1)_NN_d
Tuning for shape 4x128*(2x2^2)_NN_d
Tuning for shape 4x128*(2x2^3)_NN_d
Tuning for shape 4x128*(2x2^4)_NN_d
Tuning for shape 4x128*(2x2^5)_NN_d
Tuning for shape 4x128*(2x2^6)_NN_d
Tuning for shape 4x128*(2x2^7)_NN_d
Tuning for shape 4x128*(2x2^1)_NN_d
Tuning for shape 4x128*(2x2^2)_NN_d
Tuning for shape 4x128*(2x2^3)_NN_d
Tuning for shape 4x128*(2x2^4)_NN_d
Tuning for shape 4x128*(2x2^5)_NN_d
Tuning for shape 4x128*(2x2^6)_NN_d
Tuning for shape 4x128*(2x2^1)_NN_d
Tuning for shape 4x128*(2x2^2)_NN_d
Tuning for shape 4x128*(2x2^3)_NN_d
Tuning for shape 4x128*(2x2^4)_NN_d
Tuning for shape 4x128*(2x2^5)_NN_d
Tuning for shape 4x128*(2x2^1)_NN_d
Tuning for shape 4x128*(2x2^2)_NN_d
Tuning for shape 4x128*(2x2^3)_NN_d
Tuning for shape 4x128*(2x2^4)_NN_d
Tuning for shape 4x128*(2x2^1)_NN_d
Tuning for shape 4x128*(2x2^2)_NN_d
Tuning for shape 4x128*(2x2^3)_NN_d
Tuning for shape 4x128*(2x2^1)_NN_d
Tuning for shape 4x128*(2x2^2)_NN_d
Tuning for shape 4x128*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^7  &  1.000 & 1.000 & 0.016 & 62.428
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 7 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 128] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x to produce Y[16, 128]
Matmul: 16 x 128 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(2x2^1)_NN_d
Tuning for shape 16x128*(2x2^2)_NN_d
Tuning for shape 16x128*(2x2^3)_NN_d
Tuning for shape 16x128*(2x2^4)_NN_d
Tuning for shape 16x128*(2x2^5)_NN_d
Tuning for shape 16x128*(2x2^6)_NN_d
Tuning for shape 16x128*(2x2^7)_NN_d
Tuning for shape 16x128*(2x2^1)_NN_d
Tuning for shape 16x128*(2x2^2)_NN_d
Tuning for shape 16x128*(2x2^3)_NN_d
Tuning for shape 16x128*(2x2^4)_NN_d
Tuning for shape 16x128*(2x2^5)_NN_d
Tuning for shape 16x128*(2x2^6)_NN_d
Tuning for shape 16x128*(2x2^1)_NN_d
Tuning for shape 16x128*(2x2^2)_NN_d
Tuning for shape 16x128*(2x2^3)_NN_d
Tuning for shape 16x128*(2x2^4)_NN_d
Tuning for shape 16x128*(2x2^5)_NN_d
Tuning for shape 16x128*(2x2^1)_NN_d
Tuning for shape 16x128*(2x2^2)_NN_d
Tuning for shape 16x128*(2x2^3)_NN_d
Tuning for shape 16x128*(2x2^4)_NN_d
Tuning for shape 16x128*(2x2^1)_NN_d
Tuning for shape 16x128*(2x2^2)_NN_d
Tuning for shape 16x128*(2x2^3)_NN_d
Tuning for shape 16x128*(2x2^1)_NN_d
Tuning for shape 16x128*(2x2^2)_NN_d
Tuning for shape 16x128*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^7  &  1.000 & 1.000 & 0.062 & 16.134
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 7 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 128] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x to produce Y[64, 128]
Matmul: 64 x 128 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(2x2^1)_NN_d
Tuning for shape 64x128*(2x2^2)_NN_d
Tuning for shape 64x128*(2x2^3)_NN_d
Tuning for shape 64x128*(2x2^4)_NN_d
Tuning for shape 64x128*(2x2^5)_NN_d
Tuning for shape 64x128*(2x2^6)_NN_d
Tuning for shape 64x128*(2x2^7)_NN_d
Tuning for shape 64x128*(2x2^1)_NN_d
Tuning for shape 64x128*(2x2^2)_NN_d
Tuning for shape 64x128*(2x2^3)_NN_d
Tuning for shape 64x128*(2x2^4)_NN_d
Tuning for shape 64x128*(2x2^5)_NN_d
Tuning for shape 64x128*(2x2^6)_NN_d
Tuning for shape 64x128*(2x2^1)_NN_d
Tuning for shape 64x128*(2x2^2)_NN_d
Tuning for shape 64x128*(2x2^3)_NN_d
Tuning for shape 64x128*(2x2^4)_NN_d
Tuning for shape 64x128*(2x2^5)_NN_d
Tuning for shape 64x128*(2x2^1)_NN_d
Tuning for shape 64x128*(2x2^2)_NN_d
Tuning for shape 64x128*(2x2^3)_NN_d
Tuning for shape 64x128*(2x2^4)_NN_d
Tuning for shape 64x128*(2x2^1)_NN_d
Tuning for shape 64x128*(2x2^2)_NN_d
Tuning for shape 64x128*(2x2^3)_NN_d
Tuning for shape 64x128*(2x2^1)_NN_d
Tuning for shape 64x128*(2x2^2)_NN_d
Tuning for shape 64x128*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^7  &  1.000 & 1.000 & 0.252 & 3.970
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 7 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 128] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x to produce Y[256, 128]
Matmul: 256 x 128 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(2x2^1)_NN_d
Tuning for shape 256x128*(2x2^2)_NN_d
Tuning for shape 256x128*(2x2^3)_NN_d
Tuning for shape 256x128*(2x2^4)_NN_d
Tuning for shape 256x128*(2x2^5)_NN_d
Tuning for shape 256x128*(2x2^6)_NN_d
Tuning for shape 256x128*(2x2^7)_NN_d
Tuning for shape 256x128*(2x2^1)_NN_d
Tuning for shape 256x128*(2x2^2)_NN_d
Tuning for shape 256x128*(2x2^3)_NN_d
Tuning for shape 256x128*(2x2^4)_NN_d
Tuning for shape 256x128*(2x2^5)_NN_d
Tuning for shape 256x128*(2x2^6)_NN_d
Tuning for shape 256x128*(2x2^1)_NN_d
Tuning for shape 256x128*(2x2^2)_NN_d
Tuning for shape 256x128*(2x2^3)_NN_d
Tuning for shape 256x128*(2x2^4)_NN_d
Tuning for shape 256x128*(2x2^5)_NN_d
Tuning for shape 256x128*(2x2^1)_NN_d
Tuning for shape 256x128*(2x2^2)_NN_d
Tuning for shape 256x128*(2x2^3)_NN_d
Tuning for shape 256x128*(2x2^4)_NN_d
Tuning for shape 256x128*(2x2^1)_NN_d
Tuning for shape 256x128*(2x2^2)_NN_d
Tuning for shape 256x128*(2x2^3)_NN_d
Tuning for shape 256x128*(2x2^1)_NN_d
Tuning for shape 256x128*(2x2^2)_NN_d
Tuning for shape 256x128*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^7  &  1.000 & 1.000 & 0.996 & 1.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 7 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 128] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x to produce Y[1024, 128]
Matmul: 1024 x 128 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(2x2^1)_NN_d
Tuning for shape 1024x128*(2x2^2)_NN_d
Tuning for shape 1024x128*(2x2^3)_NN_d
Tuning for shape 1024x128*(2x2^4)_NN_d
Tuning for shape 1024x128*(2x2^5)_NN_d
Tuning for shape 1024x128*(2x2^6)_NN_d
Tuning for shape 1024x128*(2x2^7)_NN_d
Tuning for shape 1024x128*(2x2^1)_NN_d
Tuning for shape 1024x128*(2x2^2)_NN_d
Tuning for shape 1024x128*(2x2^3)_NN_d
Tuning for shape 1024x128*(2x2^4)_NN_d
Tuning for shape 1024x128*(2x2^5)_NN_d
Tuning for shape 1024x128*(2x2^6)_NN_d
Tuning for shape 1024x128*(2x2^1)_NN_d
Tuning for shape 1024x128*(2x2^2)_NN_d
Tuning for shape 1024x128*(2x2^3)_NN_d
Tuning for shape 1024x128*(2x2^4)_NN_d
Tuning for shape 1024x128*(2x2^5)_NN_d
Tuning for shape 1024x128*(2x2^1)_NN_d
Tuning for shape 1024x128*(2x2^2)_NN_d
Tuning for shape 1024x128*(2x2^3)_NN_d
Tuning for shape 1024x128*(2x2^4)_NN_d
Tuning for shape 1024x128*(2x2^1)_NN_d
Tuning for shape 1024x128*(2x2^2)_NN_d
Tuning for shape 1024x128*(2x2^3)_NN_d
Tuning for shape 1024x128*(2x2^1)_NN_d
Tuning for shape 1024x128*(2x2^2)_NN_d
Tuning for shape 1024x128*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^7  &  1.000 & 1.000 & 4.077 & 0.245
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 8 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x to produce Y[1, 256]
Matmul: 1 x 256 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(2x2^1)_NN_d
Tuning for shape 1x256*(2x2^2)_NN_d
Tuning for shape 1x256*(2x2^3)_NN_d
Tuning for shape 1x256*(2x2^4)_NN_d
Tuning for shape 1x256*(2x2^5)_NN_d
Tuning for shape 1x256*(2x2^6)_NN_d
Tuning for shape 1x256*(2x2^7)_NN_d
Tuning for shape 1x256*(2x2^8)_NN_d
Tuning for shape 1x256*(2x2^1)_NN_d
Tuning for shape 1x256*(2x2^2)_NN_d
Tuning for shape 1x256*(2x2^3)_NN_d
Tuning for shape 1x256*(2x2^4)_NN_d
Tuning for shape 1x256*(2x2^5)_NN_d
Tuning for shape 1x256*(2x2^6)_NN_d
Tuning for shape 1x256*(2x2^7)_NN_d
Tuning for shape 1x256*(2x2^1)_NN_d
Tuning for shape 1x256*(2x2^2)_NN_d
Tuning for shape 1x256*(2x2^3)_NN_d
Tuning for shape 1x256*(2x2^4)_NN_d
Tuning for shape 1x256*(2x2^5)_NN_d
Tuning for shape 1x256*(2x2^6)_NN_d
Tuning for shape 1x256*(2x2^1)_NN_d
Tuning for shape 1x256*(2x2^2)_NN_d
Tuning for shape 1x256*(2x2^3)_NN_d
Tuning for shape 1x256*(2x2^4)_NN_d
Tuning for shape 1x256*(2x2^5)_NN_d
Tuning for shape 1x256*(2x2^1)_NN_d
Tuning for shape 1x256*(2x2^2)_NN_d
Tuning for shape 1x256*(2x2^3)_NN_d
Tuning for shape 1x256*(2x2^4)_NN_d
Tuning for shape 1x256*(2x2^1)_NN_d
Tuning for shape 1x256*(2x2^2)_NN_d
Tuning for shape 1x256*(2x2^3)_NN_d
Tuning for shape 1x256*(2x2^1)_NN_d
Tuning for shape 1x256*(2x2^2)_NN_d
Tuning for shape 1x256*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^8  &  1.000 & 1.000 & 0.008 & 120.615
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 8 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x to produce Y[4, 256]
Matmul: 4 x 256 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(2x2^1)_NN_d
Tuning for shape 4x256*(2x2^2)_NN_d
Tuning for shape 4x256*(2x2^3)_NN_d
Tuning for shape 4x256*(2x2^4)_NN_d
Tuning for shape 4x256*(2x2^5)_NN_d
Tuning for shape 4x256*(2x2^6)_NN_d
Tuning for shape 4x256*(2x2^7)_NN_d
Tuning for shape 4x256*(2x2^8)_NN_d
Tuning for shape 4x256*(2x2^1)_NN_d
Tuning for shape 4x256*(2x2^2)_NN_d
Tuning for shape 4x256*(2x2^3)_NN_d
Tuning for shape 4x256*(2x2^4)_NN_d
Tuning for shape 4x256*(2x2^5)_NN_d
Tuning for shape 4x256*(2x2^6)_NN_d
Tuning for shape 4x256*(2x2^7)_NN_d
Tuning for shape 4x256*(2x2^1)_NN_d
Tuning for shape 4x256*(2x2^2)_NN_d
Tuning for shape 4x256*(2x2^3)_NN_d
Tuning for shape 4x256*(2x2^4)_NN_d
Tuning for shape 4x256*(2x2^5)_NN_d
Tuning for shape 4x256*(2x2^6)_NN_d
Tuning for shape 4x256*(2x2^1)_NN_d
Tuning for shape 4x256*(2x2^2)_NN_d
Tuning for shape 4x256*(2x2^3)_NN_d
Tuning for shape 4x256*(2x2^4)_NN_d
Tuning for shape 4x256*(2x2^5)_NN_d
Tuning for shape 4x256*(2x2^1)_NN_d
Tuning for shape 4x256*(2x2^2)_NN_d
Tuning for shape 4x256*(2x2^3)_NN_d
Tuning for shape 4x256*(2x2^4)_NN_d
Tuning for shape 4x256*(2x2^1)_NN_d
Tuning for shape 4x256*(2x2^2)_NN_d
Tuning for shape 4x256*(2x2^3)_NN_d
Tuning for shape 4x256*(2x2^1)_NN_d
Tuning for shape 4x256*(2x2^2)_NN_d
Tuning for shape 4x256*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^8  &  1.000 & 1.000 & 0.032 & 31.200
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 8 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x to produce Y[16, 256]
Matmul: 16 x 256 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(2x2^1)_NN_d
Tuning for shape 16x256*(2x2^2)_NN_d
Tuning for shape 16x256*(2x2^3)_NN_d
Tuning for shape 16x256*(2x2^4)_NN_d
Tuning for shape 16x256*(2x2^5)_NN_d
Tuning for shape 16x256*(2x2^6)_NN_d
Tuning for shape 16x256*(2x2^7)_NN_d
Tuning for shape 16x256*(2x2^8)_NN_d
Tuning for shape 16x256*(2x2^1)_NN_d
Tuning for shape 16x256*(2x2^2)_NN_d
Tuning for shape 16x256*(2x2^3)_NN_d
Tuning for shape 16x256*(2x2^4)_NN_d
Tuning for shape 16x256*(2x2^5)_NN_d
Tuning for shape 16x256*(2x2^6)_NN_d
Tuning for shape 16x256*(2x2^7)_NN_d
Tuning for shape 16x256*(2x2^1)_NN_d
Tuning for shape 16x256*(2x2^2)_NN_d
Tuning for shape 16x256*(2x2^3)_NN_d
Tuning for shape 16x256*(2x2^4)_NN_d
Tuning for shape 16x256*(2x2^5)_NN_d
Tuning for shape 16x256*(2x2^6)_NN_d
Tuning for shape 16x256*(2x2^1)_NN_d
Tuning for shape 16x256*(2x2^2)_NN_d
Tuning for shape 16x256*(2x2^3)_NN_d
Tuning for shape 16x256*(2x2^4)_NN_d
Tuning for shape 16x256*(2x2^5)_NN_d
Tuning for shape 16x256*(2x2^1)_NN_d
Tuning for shape 16x256*(2x2^2)_NN_d
Tuning for shape 16x256*(2x2^3)_NN_d
Tuning for shape 16x256*(2x2^4)_NN_d
Tuning for shape 16x256*(2x2^1)_NN_d
Tuning for shape 16x256*(2x2^2)_NN_d
Tuning for shape 16x256*(2x2^3)_NN_d
Tuning for shape 16x256*(2x2^1)_NN_d
Tuning for shape 16x256*(2x2^2)_NN_d
Tuning for shape 16x256*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^8  &  1.000 & 1.000 & 0.125 & 7.994
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 8 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x to produce Y[64, 256]
Matmul: 64 x 256 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(2x2^1)_NN_d
Tuning for shape 64x256*(2x2^2)_NN_d
Tuning for shape 64x256*(2x2^3)_NN_d
Tuning for shape 64x256*(2x2^4)_NN_d
Tuning for shape 64x256*(2x2^5)_NN_d
Tuning for shape 64x256*(2x2^6)_NN_d
Tuning for shape 64x256*(2x2^7)_NN_d
Tuning for shape 64x256*(2x2^8)_NN_d
Tuning for shape 64x256*(2x2^1)_NN_d
Tuning for shape 64x256*(2x2^2)_NN_d
Tuning for shape 64x256*(2x2^3)_NN_d
Tuning for shape 64x256*(2x2^4)_NN_d
Tuning for shape 64x256*(2x2^5)_NN_d
Tuning for shape 64x256*(2x2^6)_NN_d
Tuning for shape 64x256*(2x2^7)_NN_d
Tuning for shape 64x256*(2x2^1)_NN_d
Tuning for shape 64x256*(2x2^2)_NN_d
Tuning for shape 64x256*(2x2^3)_NN_d
Tuning for shape 64x256*(2x2^4)_NN_d
Tuning for shape 64x256*(2x2^5)_NN_d
Tuning for shape 64x256*(2x2^6)_NN_d
Tuning for shape 64x256*(2x2^1)_NN_d
Tuning for shape 64x256*(2x2^2)_NN_d
Tuning for shape 64x256*(2x2^3)_NN_d
Tuning for shape 64x256*(2x2^4)_NN_d
Tuning for shape 64x256*(2x2^5)_NN_d
Tuning for shape 64x256*(2x2^1)_NN_d
Tuning for shape 64x256*(2x2^2)_NN_d
Tuning for shape 64x256*(2x2^3)_NN_d
Tuning for shape 64x256*(2x2^4)_NN_d
Tuning for shape 64x256*(2x2^1)_NN_d
Tuning for shape 64x256*(2x2^2)_NN_d
Tuning for shape 64x256*(2x2^3)_NN_d
Tuning for shape 64x256*(2x2^1)_NN_d
Tuning for shape 64x256*(2x2^2)_NN_d
Tuning for shape 64x256*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^8  &  1.000 & 1.000 & 0.530 & 1.886
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 8 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x to produce Y[256, 256]
Matmul: 256 x 256 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(2x2^1)_NN_d
Tuning for shape 256x256*(2x2^2)_NN_d
Tuning for shape 256x256*(2x2^3)_NN_d
Tuning for shape 256x256*(2x2^4)_NN_d
Tuning for shape 256x256*(2x2^5)_NN_d
Tuning for shape 256x256*(2x2^6)_NN_d
Tuning for shape 256x256*(2x2^7)_NN_d
Tuning for shape 256x256*(2x2^8)_NN_d
Tuning for shape 256x256*(2x2^1)_NN_d
Tuning for shape 256x256*(2x2^2)_NN_d
Tuning for shape 256x256*(2x2^3)_NN_d
Tuning for shape 256x256*(2x2^4)_NN_d
Tuning for shape 256x256*(2x2^5)_NN_d
Tuning for shape 256x256*(2x2^6)_NN_d
Tuning for shape 256x256*(2x2^7)_NN_d
Tuning for shape 256x256*(2x2^1)_NN_d
Tuning for shape 256x256*(2x2^2)_NN_d
Tuning for shape 256x256*(2x2^3)_NN_d
Tuning for shape 256x256*(2x2^4)_NN_d
Tuning for shape 256x256*(2x2^5)_NN_d
Tuning for shape 256x256*(2x2^6)_NN_d
Tuning for shape 256x256*(2x2^1)_NN_d
Tuning for shape 256x256*(2x2^2)_NN_d
Tuning for shape 256x256*(2x2^3)_NN_d
Tuning for shape 256x256*(2x2^4)_NN_d
Tuning for shape 256x256*(2x2^5)_NN_d
Tuning for shape 256x256*(2x2^1)_NN_d
Tuning for shape 256x256*(2x2^2)_NN_d
Tuning for shape 256x256*(2x2^3)_NN_d
Tuning for shape 256x256*(2x2^4)_NN_d
Tuning for shape 256x256*(2x2^1)_NN_d
Tuning for shape 256x256*(2x2^2)_NN_d
Tuning for shape 256x256*(2x2^3)_NN_d
Tuning for shape 256x256*(2x2^1)_NN_d
Tuning for shape 256x256*(2x2^2)_NN_d
Tuning for shape 256x256*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^8  &  1.000 & 1.000 & 2.085 & 0.480
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 8 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 256] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x to produce Y[1024, 256]
Matmul: 1024 x 256 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x256*(2x2^1)_NN_d
Tuning for shape 1024x256*(2x2^2)_NN_d
Tuning for shape 1024x256*(2x2^3)_NN_d
Tuning for shape 1024x256*(2x2^4)_NN_d
Tuning for shape 1024x256*(2x2^5)_NN_d
Tuning for shape 1024x256*(2x2^6)_NN_d
Tuning for shape 1024x256*(2x2^7)_NN_d
Tuning for shape 1024x256*(2x2^8)_NN_d
Tuning for shape 1024x256*(2x2^1)_NN_d
Tuning for shape 1024x256*(2x2^2)_NN_d
Tuning for shape 1024x256*(2x2^3)_NN_d
Tuning for shape 1024x256*(2x2^4)_NN_d
Tuning for shape 1024x256*(2x2^5)_NN_d
Tuning for shape 1024x256*(2x2^6)_NN_d
Tuning for shape 1024x256*(2x2^7)_NN_d
Tuning for shape 1024x256*(2x2^1)_NN_d
Tuning for shape 1024x256*(2x2^2)_NN_d
Tuning for shape 1024x256*(2x2^3)_NN_d
Tuning for shape 1024x256*(2x2^4)_NN_d
Tuning for shape 1024x256*(2x2^5)_NN_d
Tuning for shape 1024x256*(2x2^6)_NN_d
Tuning for shape 1024x256*(2x2^1)_NN_d
Tuning for shape 1024x256*(2x2^2)_NN_d
Tuning for shape 1024x256*(2x2^3)_NN_d
Tuning for shape 1024x256*(2x2^4)_NN_d
Tuning for shape 1024x256*(2x2^5)_NN_d
Tuning for shape 1024x256*(2x2^1)_NN_d
Tuning for shape 1024x256*(2x2^2)_NN_d
Tuning for shape 1024x256*(2x2^3)_NN_d
Tuning for shape 1024x256*(2x2^4)_NN_d
Tuning for shape 1024x256*(2x2^1)_NN_d
Tuning for shape 1024x256*(2x2^2)_NN_d
Tuning for shape 1024x256*(2x2^3)_NN_d
Tuning for shape 1024x256*(2x2^1)_NN_d
Tuning for shape 1024x256*(2x2^2)_NN_d
Tuning for shape 1024x256*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^8  &  1.000 & 1.000 & 8.240 & 0.121
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 9 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x to produce Y[1, 512]
Matmul: 1 x 512 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(2x2^1)_NN_d
Tuning for shape 1x512*(2x2^2)_NN_d
Tuning for shape 1x512*(2x2^3)_NN_d
Tuning for shape 1x512*(2x2^4)_NN_d
Tuning for shape 1x512*(2x2^5)_NN_d
Tuning for shape 1x512*(2x2^6)_NN_d
Tuning for shape 1x512*(2x2^7)_NN_d
Tuning for shape 1x512*(2x2^8)_NN_d
Tuning for shape 1x512*(2x2^9)_NN_d
Tuning for shape 1x512*(2x2^1)_NN_d
Tuning for shape 1x512*(2x2^2)_NN_d
Tuning for shape 1x512*(2x2^3)_NN_d
Tuning for shape 1x512*(2x2^4)_NN_d
Tuning for shape 1x512*(2x2^5)_NN_d
Tuning for shape 1x512*(2x2^6)_NN_d
Tuning for shape 1x512*(2x2^7)_NN_d
Tuning for shape 1x512*(2x2^8)_NN_d
Tuning for shape 1x512*(2x2^1)_NN_d
Tuning for shape 1x512*(2x2^2)_NN_d
Tuning for shape 1x512*(2x2^3)_NN_d
Tuning for shape 1x512*(2x2^4)_NN_d
Tuning for shape 1x512*(2x2^5)_NN_d
Tuning for shape 1x512*(2x2^6)_NN_d
Tuning for shape 1x512*(2x2^7)_NN_d
Tuning for shape 1x512*(2x2^1)_NN_d
Tuning for shape 1x512*(2x2^2)_NN_d
Tuning for shape 1x512*(2x2^3)_NN_d
Tuning for shape 1x512*(2x2^4)_NN_d
Tuning for shape 1x512*(2x2^5)_NN_d
Tuning for shape 1x512*(2x2^6)_NN_d
Tuning for shape 1x512*(2x2^1)_NN_d
Tuning for shape 1x512*(2x2^2)_NN_d
Tuning for shape 1x512*(2x2^3)_NN_d
Tuning for shape 1x512*(2x2^4)_NN_d
Tuning for shape 1x512*(2x2^5)_NN_d
Tuning for shape 1x512*(2x2^1)_NN_d
Tuning for shape 1x512*(2x2^2)_NN_d
Tuning for shape 1x512*(2x2^3)_NN_d
Tuning for shape 1x512*(2x2^4)_NN_d
Tuning for shape 1x512*(2x2^1)_NN_d
Tuning for shape 1x512*(2x2^2)_NN_d
Tuning for shape 1x512*(2x2^3)_NN_d
Tuning for shape 1x512*(2x2^1)_NN_d
Tuning for shape 1x512*(2x2^2)_NN_d
Tuning for shape 1x512*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^9  &  1.000 & 1.000 & 0.015 & 65.425
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 9 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x to produce Y[4, 512]
Matmul: 4 x 512 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(2x2^1)_NN_d
Tuning for shape 4x512*(2x2^2)_NN_d
Tuning for shape 4x512*(2x2^3)_NN_d
Tuning for shape 4x512*(2x2^4)_NN_d
Tuning for shape 4x512*(2x2^5)_NN_d
Tuning for shape 4x512*(2x2^6)_NN_d
Tuning for shape 4x512*(2x2^7)_NN_d
Tuning for shape 4x512*(2x2^8)_NN_d
Tuning for shape 4x512*(2x2^9)_NN_d
Tuning for shape 4x512*(2x2^1)_NN_d
Tuning for shape 4x512*(2x2^2)_NN_d
Tuning for shape 4x512*(2x2^3)_NN_d
Tuning for shape 4x512*(2x2^4)_NN_d
Tuning for shape 4x512*(2x2^5)_NN_d
Tuning for shape 4x512*(2x2^6)_NN_d
Tuning for shape 4x512*(2x2^7)_NN_d
Tuning for shape 4x512*(2x2^8)_NN_d
Tuning for shape 4x512*(2x2^1)_NN_d
Tuning for shape 4x512*(2x2^2)_NN_d
Tuning for shape 4x512*(2x2^3)_NN_d
Tuning for shape 4x512*(2x2^4)_NN_d
Tuning for shape 4x512*(2x2^5)_NN_d
Tuning for shape 4x512*(2x2^6)_NN_d
Tuning for shape 4x512*(2x2^7)_NN_d
Tuning for shape 4x512*(2x2^1)_NN_d
Tuning for shape 4x512*(2x2^2)_NN_d
Tuning for shape 4x512*(2x2^3)_NN_d
Tuning for shape 4x512*(2x2^4)_NN_d
Tuning for shape 4x512*(2x2^5)_NN_d
Tuning for shape 4x512*(2x2^6)_NN_d
Tuning for shape 4x512*(2x2^1)_NN_d
Tuning for shape 4x512*(2x2^2)_NN_d
Tuning for shape 4x512*(2x2^3)_NN_d
Tuning for shape 4x512*(2x2^4)_NN_d
Tuning for shape 4x512*(2x2^5)_NN_d
Tuning for shape 4x512*(2x2^1)_NN_d
Tuning for shape 4x512*(2x2^2)_NN_d
Tuning for shape 4x512*(2x2^3)_NN_d
Tuning for shape 4x512*(2x2^4)_NN_d
Tuning for shape 4x512*(2x2^1)_NN_d
Tuning for shape 4x512*(2x2^2)_NN_d
Tuning for shape 4x512*(2x2^3)_NN_d
Tuning for shape 4x512*(2x2^1)_NN_d
Tuning for shape 4x512*(2x2^2)_NN_d
Tuning for shape 4x512*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^9  &  1.000 & 1.000 & 0.067 & 14.986
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 9 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 512] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x to produce Y[16, 512]
Matmul: 16 x 512 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(2x2^1)_NN_d
Tuning for shape 16x512*(2x2^2)_NN_d
Tuning for shape 16x512*(2x2^3)_NN_d
Tuning for shape 16x512*(2x2^4)_NN_d
Tuning for shape 16x512*(2x2^5)_NN_d
Tuning for shape 16x512*(2x2^6)_NN_d
Tuning for shape 16x512*(2x2^7)_NN_d
Tuning for shape 16x512*(2x2^8)_NN_d
Tuning for shape 16x512*(2x2^9)_NN_d
Tuning for shape 16x512*(2x2^1)_NN_d
Tuning for shape 16x512*(2x2^2)_NN_d
Tuning for shape 16x512*(2x2^3)_NN_d
Tuning for shape 16x512*(2x2^4)_NN_d
Tuning for shape 16x512*(2x2^5)_NN_d
Tuning for shape 16x512*(2x2^6)_NN_d
Tuning for shape 16x512*(2x2^7)_NN_d
Tuning for shape 16x512*(2x2^8)_NN_d
Tuning for shape 16x512*(2x2^1)_NN_d
Tuning for shape 16x512*(2x2^2)_NN_d
Tuning for shape 16x512*(2x2^3)_NN_d
Tuning for shape 16x512*(2x2^4)_NN_d
Tuning for shape 16x512*(2x2^5)_NN_d
Tuning for shape 16x512*(2x2^6)_NN_d
Tuning for shape 16x512*(2x2^7)_NN_d
Tuning for shape 16x512*(2x2^1)_NN_d
Tuning for shape 16x512*(2x2^2)_NN_d
Tuning for shape 16x512*(2x2^3)_NN_d
Tuning for shape 16x512*(2x2^4)_NN_d
Tuning for shape 16x512*(2x2^5)_NN_d
Tuning for shape 16x512*(2x2^6)_NN_d
Tuning for shape 16x512*(2x2^1)_NN_d
Tuning for shape 16x512*(2x2^2)_NN_d
Tuning for shape 16x512*(2x2^3)_NN_d
Tuning for shape 16x512*(2x2^4)_NN_d
Tuning for shape 16x512*(2x2^5)_NN_d
Tuning for shape 16x512*(2x2^1)_NN_d
Tuning for shape 16x512*(2x2^2)_NN_d
Tuning for shape 16x512*(2x2^3)_NN_d
Tuning for shape 16x512*(2x2^4)_NN_d
Tuning for shape 16x512*(2x2^1)_NN_d
Tuning for shape 16x512*(2x2^2)_NN_d
Tuning for shape 16x512*(2x2^3)_NN_d
Tuning for shape 16x512*(2x2^1)_NN_d
Tuning for shape 16x512*(2x2^2)_NN_d
Tuning for shape 16x512*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^9  &  1.000 & 1.000 & 0.260 & 3.848
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 9 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 512] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x to produce Y[64, 512]
Matmul: 64 x 512 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x512*(2x2^1)_NN_d
Tuning for shape 64x512*(2x2^2)_NN_d
Tuning for shape 64x512*(2x2^3)_NN_d
Tuning for shape 64x512*(2x2^4)_NN_d
Tuning for shape 64x512*(2x2^5)_NN_d
Tuning for shape 64x512*(2x2^6)_NN_d
Tuning for shape 64x512*(2x2^7)_NN_d
Tuning for shape 64x512*(2x2^8)_NN_d
Tuning for shape 64x512*(2x2^9)_NN_d
Tuning for shape 64x512*(2x2^1)_NN_d
Tuning for shape 64x512*(2x2^2)_NN_d
Tuning for shape 64x512*(2x2^3)_NN_d
Tuning for shape 64x512*(2x2^4)_NN_d
Tuning for shape 64x512*(2x2^5)_NN_d
Tuning for shape 64x512*(2x2^6)_NN_d
Tuning for shape 64x512*(2x2^7)_NN_d
Tuning for shape 64x512*(2x2^8)_NN_d
Tuning for shape 64x512*(2x2^1)_NN_d
Tuning for shape 64x512*(2x2^2)_NN_d
Tuning for shape 64x512*(2x2^3)_NN_d
Tuning for shape 64x512*(2x2^4)_NN_d
Tuning for shape 64x512*(2x2^5)_NN_d
Tuning for shape 64x512*(2x2^6)_NN_d
Tuning for shape 64x512*(2x2^7)_NN_d
Tuning for shape 64x512*(2x2^1)_NN_d
Tuning for shape 64x512*(2x2^2)_NN_d
Tuning for shape 64x512*(2x2^3)_NN_d
Tuning for shape 64x512*(2x2^4)_NN_d
Tuning for shape 64x512*(2x2^5)_NN_d
Tuning for shape 64x512*(2x2^6)_NN_d
Tuning for shape 64x512*(2x2^1)_NN_d
Tuning for shape 64x512*(2x2^2)_NN_d
Tuning for shape 64x512*(2x2^3)_NN_d
Tuning for shape 64x512*(2x2^4)_NN_d
Tuning for shape 64x512*(2x2^5)_NN_d
Tuning for shape 64x512*(2x2^1)_NN_d
Tuning for shape 64x512*(2x2^2)_NN_d
Tuning for shape 64x512*(2x2^3)_NN_d
Tuning for shape 64x512*(2x2^4)_NN_d
Tuning for shape 64x512*(2x2^1)_NN_d
Tuning for shape 64x512*(2x2^2)_NN_d
Tuning for shape 64x512*(2x2^3)_NN_d
Tuning for shape 64x512*(2x2^1)_NN_d
Tuning for shape 64x512*(2x2^2)_NN_d
Tuning for shape 64x512*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^9  &  1.000 & 1.000 & 1.069 & 0.935
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 9 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 512] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x to produce Y[256, 512]
Matmul: 256 x 512 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x512*(2x2^1)_NN_d
Tuning for shape 256x512*(2x2^2)_NN_d
Tuning for shape 256x512*(2x2^3)_NN_d
Tuning for shape 256x512*(2x2^4)_NN_d
Tuning for shape 256x512*(2x2^5)_NN_d
Tuning for shape 256x512*(2x2^6)_NN_d
Tuning for shape 256x512*(2x2^7)_NN_d
Tuning for shape 256x512*(2x2^8)_NN_d
Tuning for shape 256x512*(2x2^9)_NN_d
Tuning for shape 256x512*(2x2^1)_NN_d
Tuning for shape 256x512*(2x2^2)_NN_d
Tuning for shape 256x512*(2x2^3)_NN_d
Tuning for shape 256x512*(2x2^4)_NN_d
Tuning for shape 256x512*(2x2^5)_NN_d
Tuning for shape 256x512*(2x2^6)_NN_d
Tuning for shape 256x512*(2x2^7)_NN_d
Tuning for shape 256x512*(2x2^8)_NN_d
Tuning for shape 256x512*(2x2^1)_NN_d
Tuning for shape 256x512*(2x2^2)_NN_d
Tuning for shape 256x512*(2x2^3)_NN_d
Tuning for shape 256x512*(2x2^4)_NN_d
Tuning for shape 256x512*(2x2^5)_NN_d
Tuning for shape 256x512*(2x2^6)_NN_d
Tuning for shape 256x512*(2x2^7)_NN_d
Tuning for shape 256x512*(2x2^1)_NN_d
Tuning for shape 256x512*(2x2^2)_NN_d
Tuning for shape 256x512*(2x2^3)_NN_d
Tuning for shape 256x512*(2x2^4)_NN_d
Tuning for shape 256x512*(2x2^5)_NN_d
Tuning for shape 256x512*(2x2^6)_NN_d
Tuning for shape 256x512*(2x2^1)_NN_d
Tuning for shape 256x512*(2x2^2)_NN_d
Tuning for shape 256x512*(2x2^3)_NN_d
Tuning for shape 256x512*(2x2^4)_NN_d
Tuning for shape 256x512*(2x2^5)_NN_d
Tuning for shape 256x512*(2x2^1)_NN_d
Tuning for shape 256x512*(2x2^2)_NN_d
Tuning for shape 256x512*(2x2^3)_NN_d
Tuning for shape 256x512*(2x2^4)_NN_d
Tuning for shape 256x512*(2x2^1)_NN_d
Tuning for shape 256x512*(2x2^2)_NN_d
Tuning for shape 256x512*(2x2^3)_NN_d
Tuning for shape 256x512*(2x2^1)_NN_d
Tuning for shape 256x512*(2x2^2)_NN_d
Tuning for shape 256x512*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^9  &  1.000 & 1.000 & 4.248 & 0.235
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 9 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 512] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x to produce Y[1024, 512]
Matmul: 1024 x 512 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x512*(2x2^1)_NN_d
Tuning for shape 1024x512*(2x2^2)_NN_d
Tuning for shape 1024x512*(2x2^3)_NN_d
Tuning for shape 1024x512*(2x2^4)_NN_d
Tuning for shape 1024x512*(2x2^5)_NN_d
Tuning for shape 1024x512*(2x2^6)_NN_d
Tuning for shape 1024x512*(2x2^7)_NN_d
Tuning for shape 1024x512*(2x2^8)_NN_d
Tuning for shape 1024x512*(2x2^9)_NN_d
Tuning for shape 1024x512*(2x2^1)_NN_d
Tuning for shape 1024x512*(2x2^2)_NN_d
Tuning for shape 1024x512*(2x2^3)_NN_d
Tuning for shape 1024x512*(2x2^4)_NN_d
Tuning for shape 1024x512*(2x2^5)_NN_d
Tuning for shape 1024x512*(2x2^6)_NN_d
Tuning for shape 1024x512*(2x2^7)_NN_d
Tuning for shape 1024x512*(2x2^8)_NN_d
Tuning for shape 1024x512*(2x2^1)_NN_d
Tuning for shape 1024x512*(2x2^2)_NN_d
Tuning for shape 1024x512*(2x2^3)_NN_d
Tuning for shape 1024x512*(2x2^4)_NN_d
Tuning for shape 1024x512*(2x2^5)_NN_d
Tuning for shape 1024x512*(2x2^6)_NN_d
Tuning for shape 1024x512*(2x2^7)_NN_d
Tuning for shape 1024x512*(2x2^1)_NN_d
Tuning for shape 1024x512*(2x2^2)_NN_d
Tuning for shape 1024x512*(2x2^3)_NN_d
Tuning for shape 1024x512*(2x2^4)_NN_d
Tuning for shape 1024x512*(2x2^5)_NN_d
Tuning for shape 1024x512*(2x2^6)_NN_d
Tuning for shape 1024x512*(2x2^1)_NN_d
Tuning for shape 1024x512*(2x2^2)_NN_d
Tuning for shape 1024x512*(2x2^3)_NN_d
Tuning for shape 1024x512*(2x2^4)_NN_d
Tuning for shape 1024x512*(2x2^5)_NN_d
Tuning for shape 1024x512*(2x2^1)_NN_d
Tuning for shape 1024x512*(2x2^2)_NN_d
Tuning for shape 1024x512*(2x2^3)_NN_d
Tuning for shape 1024x512*(2x2^4)_NN_d
Tuning for shape 1024x512*(2x2^1)_NN_d
Tuning for shape 1024x512*(2x2^2)_NN_d
Tuning for shape 1024x512*(2x2^3)_NN_d
Tuning for shape 1024x512*(2x2^1)_NN_d
Tuning for shape 1024x512*(2x2^2)_NN_d
Tuning for shape 1024x512*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^9  &  1.000 & 1.000 & 16.579 & 0.060
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 10 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1024] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x to produce Y[1, 1024]
Matmul: 1 x 1024 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(2x2^1)_NN_d
Tuning for shape 1x1024*(2x2^2)_NN_d
Tuning for shape 1x1024*(2x2^3)_NN_d
Tuning for shape 1x1024*(2x2^4)_NN_d
Tuning for shape 1x1024*(2x2^5)_NN_d
Tuning for shape 1x1024*(2x2^6)_NN_d
Tuning for shape 1x1024*(2x2^7)_NN_d
Tuning for shape 1x1024*(2x2^8)_NN_d
Tuning for shape 1x1024*(2x2^9)_NN_d
Tuning for shape 1x1024*(2x2^10)_NN_d
Tuning for shape 1x1024*(2x2^1)_NN_d
Tuning for shape 1x1024*(2x2^2)_NN_d
Tuning for shape 1x1024*(2x2^3)_NN_d
Tuning for shape 1x1024*(2x2^4)_NN_d
Tuning for shape 1x1024*(2x2^5)_NN_d
Tuning for shape 1x1024*(2x2^6)_NN_d
Tuning for shape 1x1024*(2x2^7)_NN_d
Tuning for shape 1x1024*(2x2^8)_NN_d
Tuning for shape 1x1024*(2x2^9)_NN_d
Tuning for shape 1x1024*(2x2^1)_NN_d
Tuning for shape 1x1024*(2x2^2)_NN_d
Tuning for shape 1x1024*(2x2^3)_NN_d
Tuning for shape 1x1024*(2x2^4)_NN_d
Tuning for shape 1x1024*(2x2^5)_NN_d
Tuning for shape 1x1024*(2x2^6)_NN_d
Tuning for shape 1x1024*(2x2^7)_NN_d
Tuning for shape 1x1024*(2x2^8)_NN_d
Tuning for shape 1x1024*(2x2^1)_NN_d
Tuning for shape 1x1024*(2x2^2)_NN_d
Tuning for shape 1x1024*(2x2^3)_NN_d
Tuning for shape 1x1024*(2x2^4)_NN_d
Tuning for shape 1x1024*(2x2^5)_NN_d
Tuning for shape 1x1024*(2x2^6)_NN_d
Tuning for shape 1x1024*(2x2^7)_NN_d
Tuning for shape 1x1024*(2x2^1)_NN_d
Tuning for shape 1x1024*(2x2^2)_NN_d
Tuning for shape 1x1024*(2x2^3)_NN_d
Tuning for shape 1x1024*(2x2^4)_NN_d
Tuning for shape 1x1024*(2x2^5)_NN_d
Tuning for shape 1x1024*(2x2^6)_NN_d
Tuning for shape 1x1024*(2x2^1)_NN_d
Tuning for shape 1x1024*(2x2^2)_NN_d
Tuning for shape 1x1024*(2x2^3)_NN_d
Tuning for shape 1x1024*(2x2^4)_NN_d
Tuning for shape 1x1024*(2x2^5)_NN_d
Tuning for shape 1x1024*(2x2^1)_NN_d
Tuning for shape 1x1024*(2x2^2)_NN_d
Tuning for shape 1x1024*(2x2^3)_NN_d
Tuning for shape 1x1024*(2x2^4)_NN_d
Tuning for shape 1x1024*(2x2^1)_NN_d
Tuning for shape 1x1024*(2x2^2)_NN_d
Tuning for shape 1x1024*(2x2^3)_NN_d
Tuning for shape 1x1024*(2x2^1)_NN_d
Tuning for shape 1x1024*(2x2^2)_NN_d
Tuning for shape 1x1024*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^10  &  1.000 & 1.000 & 0.029 & 34.802
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 10 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1024] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x to produce Y[4, 1024]
Matmul: 4 x 1024 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1024*(2x2^1)_NN_d
Tuning for shape 4x1024*(2x2^2)_NN_d
Tuning for shape 4x1024*(2x2^3)_NN_d
Tuning for shape 4x1024*(2x2^4)_NN_d
Tuning for shape 4x1024*(2x2^5)_NN_d
Tuning for shape 4x1024*(2x2^6)_NN_d
Tuning for shape 4x1024*(2x2^7)_NN_d
Tuning for shape 4x1024*(2x2^8)_NN_d
Tuning for shape 4x1024*(2x2^9)_NN_d
Tuning for shape 4x1024*(2x2^10)_NN_d
Tuning for shape 4x1024*(2x2^1)_NN_d
Tuning for shape 4x1024*(2x2^2)_NN_d
Tuning for shape 4x1024*(2x2^3)_NN_d
Tuning for shape 4x1024*(2x2^4)_NN_d
Tuning for shape 4x1024*(2x2^5)_NN_d
Tuning for shape 4x1024*(2x2^6)_NN_d
Tuning for shape 4x1024*(2x2^7)_NN_d
Tuning for shape 4x1024*(2x2^8)_NN_d
Tuning for shape 4x1024*(2x2^9)_NN_d
Tuning for shape 4x1024*(2x2^1)_NN_d
Tuning for shape 4x1024*(2x2^2)_NN_d
Tuning for shape 4x1024*(2x2^3)_NN_d
Tuning for shape 4x1024*(2x2^4)_NN_d
Tuning for shape 4x1024*(2x2^5)_NN_d
Tuning for shape 4x1024*(2x2^6)_NN_d
Tuning for shape 4x1024*(2x2^7)_NN_d
Tuning for shape 4x1024*(2x2^8)_NN_d
Tuning for shape 4x1024*(2x2^1)_NN_d
Tuning for shape 4x1024*(2x2^2)_NN_d
Tuning for shape 4x1024*(2x2^3)_NN_d
Tuning for shape 4x1024*(2x2^4)_NN_d
Tuning for shape 4x1024*(2x2^5)_NN_d
Tuning for shape 4x1024*(2x2^6)_NN_d
Tuning for shape 4x1024*(2x2^7)_NN_d
Tuning for shape 4x1024*(2x2^1)_NN_d
Tuning for shape 4x1024*(2x2^2)_NN_d
Tuning for shape 4x1024*(2x2^3)_NN_d
Tuning for shape 4x1024*(2x2^4)_NN_d
Tuning for shape 4x1024*(2x2^5)_NN_d
Tuning for shape 4x1024*(2x2^6)_NN_d
Tuning for shape 4x1024*(2x2^1)_NN_d
Tuning for shape 4x1024*(2x2^2)_NN_d
Tuning for shape 4x1024*(2x2^3)_NN_d
Tuning for shape 4x1024*(2x2^4)_NN_d
Tuning for shape 4x1024*(2x2^5)_NN_d
Tuning for shape 4x1024*(2x2^1)_NN_d
Tuning for shape 4x1024*(2x2^2)_NN_d
Tuning for shape 4x1024*(2x2^3)_NN_d
Tuning for shape 4x1024*(2x2^4)_NN_d
Tuning for shape 4x1024*(2x2^1)_NN_d
Tuning for shape 4x1024*(2x2^2)_NN_d
Tuning for shape 4x1024*(2x2^3)_NN_d
Tuning for shape 4x1024*(2x2^1)_NN_d
Tuning for shape 4x1024*(2x2^2)_NN_d
Tuning for shape 4x1024*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^10  &  1.000 & 1.000 & 0.134 & 7.485
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 10 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1024] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x to produce Y[16, 1024]
Matmul: 16 x 1024 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1024*(2x2^1)_NN_d
Tuning for shape 16x1024*(2x2^2)_NN_d
Tuning for shape 16x1024*(2x2^3)_NN_d
Tuning for shape 16x1024*(2x2^4)_NN_d
Tuning for shape 16x1024*(2x2^5)_NN_d
Tuning for shape 16x1024*(2x2^6)_NN_d
Tuning for shape 16x1024*(2x2^7)_NN_d
Tuning for shape 16x1024*(2x2^8)_NN_d
Tuning for shape 16x1024*(2x2^9)_NN_d
Tuning for shape 16x1024*(2x2^10)_NN_d
Tuning for shape 16x1024*(2x2^1)_NN_d
Tuning for shape 16x1024*(2x2^2)_NN_d
Tuning for shape 16x1024*(2x2^3)_NN_d
Tuning for shape 16x1024*(2x2^4)_NN_d
Tuning for shape 16x1024*(2x2^5)_NN_d
Tuning for shape 16x1024*(2x2^6)_NN_d
Tuning for shape 16x1024*(2x2^7)_NN_d
Tuning for shape 16x1024*(2x2^8)_NN_d
Tuning for shape 16x1024*(2x2^9)_NN_d
Tuning for shape 16x1024*(2x2^1)_NN_d
Tuning for shape 16x1024*(2x2^2)_NN_d
Tuning for shape 16x1024*(2x2^3)_NN_d
Tuning for shape 16x1024*(2x2^4)_NN_d
Tuning for shape 16x1024*(2x2^5)_NN_d
Tuning for shape 16x1024*(2x2^6)_NN_d
Tuning for shape 16x1024*(2x2^7)_NN_d
Tuning for shape 16x1024*(2x2^8)_NN_d
Tuning for shape 16x1024*(2x2^1)_NN_d
Tuning for shape 16x1024*(2x2^2)_NN_d
Tuning for shape 16x1024*(2x2^3)_NN_d
Tuning for shape 16x1024*(2x2^4)_NN_d
Tuning for shape 16x1024*(2x2^5)_NN_d
Tuning for shape 16x1024*(2x2^6)_NN_d
Tuning for shape 16x1024*(2x2^7)_NN_d
Tuning for shape 16x1024*(2x2^1)_NN_d
Tuning for shape 16x1024*(2x2^2)_NN_d
Tuning for shape 16x1024*(2x2^3)_NN_d
Tuning for shape 16x1024*(2x2^4)_NN_d
Tuning for shape 16x1024*(2x2^5)_NN_d
Tuning for shape 16x1024*(2x2^6)_NN_d
Tuning for shape 16x1024*(2x2^1)_NN_d
Tuning for shape 16x1024*(2x2^2)_NN_d
Tuning for shape 16x1024*(2x2^3)_NN_d
Tuning for shape 16x1024*(2x2^4)_NN_d
Tuning for shape 16x1024*(2x2^5)_NN_d
Tuning for shape 16x1024*(2x2^1)_NN_d
Tuning for shape 16x1024*(2x2^2)_NN_d
Tuning for shape 16x1024*(2x2^3)_NN_d
Tuning for shape 16x1024*(2x2^4)_NN_d
Tuning for shape 16x1024*(2x2^1)_NN_d
Tuning for shape 16x1024*(2x2^2)_NN_d
Tuning for shape 16x1024*(2x2^3)_NN_d
Tuning for shape 16x1024*(2x2^1)_NN_d
Tuning for shape 16x1024*(2x2^2)_NN_d
Tuning for shape 16x1024*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^10  &  1.000 & 1.000 & 0.545 & 1.836
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 10 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1024] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x to produce Y[64, 1024]
Matmul: 64 x 1024 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1024*(2x2^1)_NN_d
Tuning for shape 64x1024*(2x2^2)_NN_d
Tuning for shape 64x1024*(2x2^3)_NN_d
Tuning for shape 64x1024*(2x2^4)_NN_d
Tuning for shape 64x1024*(2x2^5)_NN_d
Tuning for shape 64x1024*(2x2^6)_NN_d
Tuning for shape 64x1024*(2x2^7)_NN_d
Tuning for shape 64x1024*(2x2^8)_NN_d
Tuning for shape 64x1024*(2x2^9)_NN_d
Tuning for shape 64x1024*(2x2^10)_NN_d
Tuning for shape 64x1024*(2x2^1)_NN_d
Tuning for shape 64x1024*(2x2^2)_NN_d
Tuning for shape 64x1024*(2x2^3)_NN_d
Tuning for shape 64x1024*(2x2^4)_NN_d
Tuning for shape 64x1024*(2x2^5)_NN_d
Tuning for shape 64x1024*(2x2^6)_NN_d
Tuning for shape 64x1024*(2x2^7)_NN_d
Tuning for shape 64x1024*(2x2^8)_NN_d
Tuning for shape 64x1024*(2x2^9)_NN_d
Tuning for shape 64x1024*(2x2^1)_NN_d
Tuning for shape 64x1024*(2x2^2)_NN_d
Tuning for shape 64x1024*(2x2^3)_NN_d
Tuning for shape 64x1024*(2x2^4)_NN_d
Tuning for shape 64x1024*(2x2^5)_NN_d
Tuning for shape 64x1024*(2x2^6)_NN_d
Tuning for shape 64x1024*(2x2^7)_NN_d
Tuning for shape 64x1024*(2x2^8)_NN_d
Tuning for shape 64x1024*(2x2^1)_NN_d
Tuning for shape 64x1024*(2x2^2)_NN_d
Tuning for shape 64x1024*(2x2^3)_NN_d
Tuning for shape 64x1024*(2x2^4)_NN_d
Tuning for shape 64x1024*(2x2^5)_NN_d
Tuning for shape 64x1024*(2x2^6)_NN_d
Tuning for shape 64x1024*(2x2^7)_NN_d
Tuning for shape 64x1024*(2x2^1)_NN_d
Tuning for shape 64x1024*(2x2^2)_NN_d
Tuning for shape 64x1024*(2x2^3)_NN_d
Tuning for shape 64x1024*(2x2^4)_NN_d
Tuning for shape 64x1024*(2x2^5)_NN_d
Tuning for shape 64x1024*(2x2^6)_NN_d
Tuning for shape 64x1024*(2x2^1)_NN_d
Tuning for shape 64x1024*(2x2^2)_NN_d
Tuning for shape 64x1024*(2x2^3)_NN_d
Tuning for shape 64x1024*(2x2^4)_NN_d
Tuning for shape 64x1024*(2x2^5)_NN_d
Tuning for shape 64x1024*(2x2^1)_NN_d
Tuning for shape 64x1024*(2x2^2)_NN_d
Tuning for shape 64x1024*(2x2^3)_NN_d
Tuning for shape 64x1024*(2x2^4)_NN_d
Tuning for shape 64x1024*(2x2^1)_NN_d
Tuning for shape 64x1024*(2x2^2)_NN_d
Tuning for shape 64x1024*(2x2^3)_NN_d
Tuning for shape 64x1024*(2x2^1)_NN_d
Tuning for shape 64x1024*(2x2^2)_NN_d
Tuning for shape 64x1024*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^10  &  1.000 & 1.000 & 2.241 & 0.446
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 10 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1024] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x to produce Y[256, 1024]
Matmul: 256 x 1024 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1024*(2x2^1)_NN_d
Tuning for shape 256x1024*(2x2^2)_NN_d
Tuning for shape 256x1024*(2x2^3)_NN_d
Tuning for shape 256x1024*(2x2^4)_NN_d
Tuning for shape 256x1024*(2x2^5)_NN_d
Tuning for shape 256x1024*(2x2^6)_NN_d
Tuning for shape 256x1024*(2x2^7)_NN_d
Tuning for shape 256x1024*(2x2^8)_NN_d
Tuning for shape 256x1024*(2x2^9)_NN_d
Tuning for shape 256x1024*(2x2^10)_NN_d
Tuning for shape 256x1024*(2x2^1)_NN_d
Tuning for shape 256x1024*(2x2^2)_NN_d
Tuning for shape 256x1024*(2x2^3)_NN_d
Tuning for shape 256x1024*(2x2^4)_NN_d
Tuning for shape 256x1024*(2x2^5)_NN_d
Tuning for shape 256x1024*(2x2^6)_NN_d
Tuning for shape 256x1024*(2x2^7)_NN_d
Tuning for shape 256x1024*(2x2^8)_NN_d
Tuning for shape 256x1024*(2x2^9)_NN_d
Tuning for shape 256x1024*(2x2^1)_NN_d
Tuning for shape 256x1024*(2x2^2)_NN_d
Tuning for shape 256x1024*(2x2^3)_NN_d
Tuning for shape 256x1024*(2x2^4)_NN_d
Tuning for shape 256x1024*(2x2^5)_NN_d
Tuning for shape 256x1024*(2x2^6)_NN_d
Tuning for shape 256x1024*(2x2^7)_NN_d
Tuning for shape 256x1024*(2x2^8)_NN_d
Tuning for shape 256x1024*(2x2^1)_NN_d
Tuning for shape 256x1024*(2x2^2)_NN_d
Tuning for shape 256x1024*(2x2^3)_NN_d
Tuning for shape 256x1024*(2x2^4)_NN_d
Tuning for shape 256x1024*(2x2^5)_NN_d
Tuning for shape 256x1024*(2x2^6)_NN_d
Tuning for shape 256x1024*(2x2^7)_NN_d
Tuning for shape 256x1024*(2x2^1)_NN_d
Tuning for shape 256x1024*(2x2^2)_NN_d
Tuning for shape 256x1024*(2x2^3)_NN_d
Tuning for shape 256x1024*(2x2^4)_NN_d
Tuning for shape 256x1024*(2x2^5)_NN_d
Tuning for shape 256x1024*(2x2^6)_NN_d
Tuning for shape 256x1024*(2x2^1)_NN_d
Tuning for shape 256x1024*(2x2^2)_NN_d
Tuning for shape 256x1024*(2x2^3)_NN_d
Tuning for shape 256x1024*(2x2^4)_NN_d
Tuning for shape 256x1024*(2x2^5)_NN_d
Tuning for shape 256x1024*(2x2^1)_NN_d
Tuning for shape 256x1024*(2x2^2)_NN_d
Tuning for shape 256x1024*(2x2^3)_NN_d
Tuning for shape 256x1024*(2x2^4)_NN_d
Tuning for shape 256x1024*(2x2^1)_NN_d
Tuning for shape 256x1024*(2x2^2)_NN_d
Tuning for shape 256x1024*(2x2^3)_NN_d
Tuning for shape 256x1024*(2x2^1)_NN_d
Tuning for shape 256x1024*(2x2^2)_NN_d
Tuning for shape 256x1024*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^10  &  1.000 & 1.000 & 8.636 & 0.116
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 10 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 1024] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x to produce Y[1024, 1024]
Matmul: 1024 x 1024 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x1024*(2x2^1)_NN_d
Tuning for shape 1024x1024*(2x2^2)_NN_d
Tuning for shape 1024x1024*(2x2^3)_NN_d
Tuning for shape 1024x1024*(2x2^4)_NN_d
Tuning for shape 1024x1024*(2x2^5)_NN_d
Tuning for shape 1024x1024*(2x2^6)_NN_d
Tuning for shape 1024x1024*(2x2^7)_NN_d
Tuning for shape 1024x1024*(2x2^8)_NN_d
Tuning for shape 1024x1024*(2x2^9)_NN_d
Tuning for shape 1024x1024*(2x2^10)_NN_d
Tuning for shape 1024x1024*(2x2^1)_NN_d
Tuning for shape 1024x1024*(2x2^2)_NN_d
Tuning for shape 1024x1024*(2x2^3)_NN_d
Tuning for shape 1024x1024*(2x2^4)_NN_d
Tuning for shape 1024x1024*(2x2^5)_NN_d
Tuning for shape 1024x1024*(2x2^6)_NN_d
Tuning for shape 1024x1024*(2x2^7)_NN_d
Tuning for shape 1024x1024*(2x2^8)_NN_d
Tuning for shape 1024x1024*(2x2^9)_NN_d
Tuning for shape 1024x1024*(2x2^1)_NN_d
Tuning for shape 1024x1024*(2x2^2)_NN_d
Tuning for shape 1024x1024*(2x2^3)_NN_d
Tuning for shape 1024x1024*(2x2^4)_NN_d
Tuning for shape 1024x1024*(2x2^5)_NN_d
Tuning for shape 1024x1024*(2x2^6)_NN_d
Tuning for shape 1024x1024*(2x2^7)_NN_d
Tuning for shape 1024x1024*(2x2^8)_NN_d
Tuning for shape 1024x1024*(2x2^1)_NN_d
Tuning for shape 1024x1024*(2x2^2)_NN_d
Tuning for shape 1024x1024*(2x2^3)_NN_d
Tuning for shape 1024x1024*(2x2^4)_NN_d
Tuning for shape 1024x1024*(2x2^5)_NN_d
Tuning for shape 1024x1024*(2x2^6)_NN_d
Tuning for shape 1024x1024*(2x2^7)_NN_d
Tuning for shape 1024x1024*(2x2^1)_NN_d
Tuning for shape 1024x1024*(2x2^2)_NN_d
Tuning for shape 1024x1024*(2x2^3)_NN_d
Tuning for shape 1024x1024*(2x2^4)_NN_d
Tuning for shape 1024x1024*(2x2^5)_NN_d
Tuning for shape 1024x1024*(2x2^6)_NN_d
Tuning for shape 1024x1024*(2x2^1)_NN_d
Tuning for shape 1024x1024*(2x2^2)_NN_d
Tuning for shape 1024x1024*(2x2^3)_NN_d
Tuning for shape 1024x1024*(2x2^4)_NN_d
Tuning for shape 1024x1024*(2x2^5)_NN_d
Tuning for shape 1024x1024*(2x2^1)_NN_d
Tuning for shape 1024x1024*(2x2^2)_NN_d
Tuning for shape 1024x1024*(2x2^3)_NN_d
Tuning for shape 1024x1024*(2x2^4)_NN_d
Tuning for shape 1024x1024*(2x2^1)_NN_d
Tuning for shape 1024x1024*(2x2^2)_NN_d
Tuning for shape 1024x1024*(2x2^3)_NN_d
Tuning for shape 1024x1024*(2x2^1)_NN_d
Tuning for shape 1024x1024*(2x2^2)_NN_d
Tuning for shape 1024x1024*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^10  &  1.000 & 1.000 & 17.511 & 0.057
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 11 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2048] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x to produce Y[1, 2048]
Matmul: 1 x 2048 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^3)_NN_d
Tuning for shape 1x2048*(2x2^4)_NN_d
Tuning for shape 1x2048*(2x2^5)_NN_d
Tuning for shape 1x2048*(2x2^6)_NN_d
Tuning for shape 1x2048*(2x2^7)_NN_d
Tuning for shape 1x2048*(2x2^8)_NN_d
Tuning for shape 1x2048*(2x2^9)_NN_d
Tuning for shape 1x2048*(2x2^10)_NN_d
Tuning for shape 1x2048*(2x2^11)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^3)_NN_d
Tuning for shape 1x2048*(2x2^4)_NN_d
Tuning for shape 1x2048*(2x2^5)_NN_d
Tuning for shape 1x2048*(2x2^6)_NN_d
Tuning for shape 1x2048*(2x2^7)_NN_d
Tuning for shape 1x2048*(2x2^8)_NN_d
Tuning for shape 1x2048*(2x2^9)_NN_d
Tuning for shape 1x2048*(2x2^10)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^3)_NN_d
Tuning for shape 1x2048*(2x2^4)_NN_d
Tuning for shape 1x2048*(2x2^5)_NN_d
Tuning for shape 1x2048*(2x2^6)_NN_d
Tuning for shape 1x2048*(2x2^7)_NN_d
Tuning for shape 1x2048*(2x2^8)_NN_d
Tuning for shape 1x2048*(2x2^9)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^3)_NN_d
Tuning for shape 1x2048*(2x2^4)_NN_d
Tuning for shape 1x2048*(2x2^5)_NN_d
Tuning for shape 1x2048*(2x2^6)_NN_d
Tuning for shape 1x2048*(2x2^7)_NN_d
Tuning for shape 1x2048*(2x2^8)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^3)_NN_d
Tuning for shape 1x2048*(2x2^4)_NN_d
Tuning for shape 1x2048*(2x2^5)_NN_d
Tuning for shape 1x2048*(2x2^6)_NN_d
Tuning for shape 1x2048*(2x2^7)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^3)_NN_d
Tuning for shape 1x2048*(2x2^4)_NN_d
Tuning for shape 1x2048*(2x2^5)_NN_d
Tuning for shape 1x2048*(2x2^6)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^3)_NN_d
Tuning for shape 1x2048*(2x2^4)_NN_d
Tuning for shape 1x2048*(2x2^5)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^3)_NN_d
Tuning for shape 1x2048*(2x2^4)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^3)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Tuning for shape 1x2048*(2x2^2)_NN_d
Tuning for shape 1x2048*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^11  &  1.000 & 1.000 & 0.068 & 14.626
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 11 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2048] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x to produce Y[4, 2048]
Matmul: 4 x 2048 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^3)_NN_d
Tuning for shape 4x2048*(2x2^4)_NN_d
Tuning for shape 4x2048*(2x2^5)_NN_d
Tuning for shape 4x2048*(2x2^6)_NN_d
Tuning for shape 4x2048*(2x2^7)_NN_d
Tuning for shape 4x2048*(2x2^8)_NN_d
Tuning for shape 4x2048*(2x2^9)_NN_d
Tuning for shape 4x2048*(2x2^10)_NN_d
Tuning for shape 4x2048*(2x2^11)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^3)_NN_d
Tuning for shape 4x2048*(2x2^4)_NN_d
Tuning for shape 4x2048*(2x2^5)_NN_d
Tuning for shape 4x2048*(2x2^6)_NN_d
Tuning for shape 4x2048*(2x2^7)_NN_d
Tuning for shape 4x2048*(2x2^8)_NN_d
Tuning for shape 4x2048*(2x2^9)_NN_d
Tuning for shape 4x2048*(2x2^10)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^3)_NN_d
Tuning for shape 4x2048*(2x2^4)_NN_d
Tuning for shape 4x2048*(2x2^5)_NN_d
Tuning for shape 4x2048*(2x2^6)_NN_d
Tuning for shape 4x2048*(2x2^7)_NN_d
Tuning for shape 4x2048*(2x2^8)_NN_d
Tuning for shape 4x2048*(2x2^9)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^3)_NN_d
Tuning for shape 4x2048*(2x2^4)_NN_d
Tuning for shape 4x2048*(2x2^5)_NN_d
Tuning for shape 4x2048*(2x2^6)_NN_d
Tuning for shape 4x2048*(2x2^7)_NN_d
Tuning for shape 4x2048*(2x2^8)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^3)_NN_d
Tuning for shape 4x2048*(2x2^4)_NN_d
Tuning for shape 4x2048*(2x2^5)_NN_d
Tuning for shape 4x2048*(2x2^6)_NN_d
Tuning for shape 4x2048*(2x2^7)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^3)_NN_d
Tuning for shape 4x2048*(2x2^4)_NN_d
Tuning for shape 4x2048*(2x2^5)_NN_d
Tuning for shape 4x2048*(2x2^6)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^3)_NN_d
Tuning for shape 4x2048*(2x2^4)_NN_d
Tuning for shape 4x2048*(2x2^5)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^3)_NN_d
Tuning for shape 4x2048*(2x2^4)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^3)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Tuning for shape 4x2048*(2x2^2)_NN_d
Tuning for shape 4x2048*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^11  &  1.000 & 1.000 & 0.284 & 3.515
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 11 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2048] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x to produce Y[16, 2048]
Matmul: 16 x 2048 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^3)_NN_d
Tuning for shape 16x2048*(2x2^4)_NN_d
Tuning for shape 16x2048*(2x2^5)_NN_d
Tuning for shape 16x2048*(2x2^6)_NN_d
Tuning for shape 16x2048*(2x2^7)_NN_d
Tuning for shape 16x2048*(2x2^8)_NN_d
Tuning for shape 16x2048*(2x2^9)_NN_d
Tuning for shape 16x2048*(2x2^10)_NN_d
Tuning for shape 16x2048*(2x2^11)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^3)_NN_d
Tuning for shape 16x2048*(2x2^4)_NN_d
Tuning for shape 16x2048*(2x2^5)_NN_d
Tuning for shape 16x2048*(2x2^6)_NN_d
Tuning for shape 16x2048*(2x2^7)_NN_d
Tuning for shape 16x2048*(2x2^8)_NN_d
Tuning for shape 16x2048*(2x2^9)_NN_d
Tuning for shape 16x2048*(2x2^10)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^3)_NN_d
Tuning for shape 16x2048*(2x2^4)_NN_d
Tuning for shape 16x2048*(2x2^5)_NN_d
Tuning for shape 16x2048*(2x2^6)_NN_d
Tuning for shape 16x2048*(2x2^7)_NN_d
Tuning for shape 16x2048*(2x2^8)_NN_d
Tuning for shape 16x2048*(2x2^9)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^3)_NN_d
Tuning for shape 16x2048*(2x2^4)_NN_d
Tuning for shape 16x2048*(2x2^5)_NN_d
Tuning for shape 16x2048*(2x2^6)_NN_d
Tuning for shape 16x2048*(2x2^7)_NN_d
Tuning for shape 16x2048*(2x2^8)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^3)_NN_d
Tuning for shape 16x2048*(2x2^4)_NN_d
Tuning for shape 16x2048*(2x2^5)_NN_d
Tuning for shape 16x2048*(2x2^6)_NN_d
Tuning for shape 16x2048*(2x2^7)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^3)_NN_d
Tuning for shape 16x2048*(2x2^4)_NN_d
Tuning for shape 16x2048*(2x2^5)_NN_d
Tuning for shape 16x2048*(2x2^6)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^3)_NN_d
Tuning for shape 16x2048*(2x2^4)_NN_d
Tuning for shape 16x2048*(2x2^5)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^3)_NN_d
Tuning for shape 16x2048*(2x2^4)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^3)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Tuning for shape 16x2048*(2x2^2)_NN_d
Tuning for shape 16x2048*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^11  &  1.000 & 1.000 & 1.061 & 0.943
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 11 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2048] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x to produce Y[64, 2048]
Matmul: 64 x 2048 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^3)_NN_d
Tuning for shape 64x2048*(2x2^4)_NN_d
Tuning for shape 64x2048*(2x2^5)_NN_d
Tuning for shape 64x2048*(2x2^6)_NN_d
Tuning for shape 64x2048*(2x2^7)_NN_d
Tuning for shape 64x2048*(2x2^8)_NN_d
Tuning for shape 64x2048*(2x2^9)_NN_d
Tuning for shape 64x2048*(2x2^10)_NN_d
Tuning for shape 64x2048*(2x2^11)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^3)_NN_d
Tuning for shape 64x2048*(2x2^4)_NN_d
Tuning for shape 64x2048*(2x2^5)_NN_d
Tuning for shape 64x2048*(2x2^6)_NN_d
Tuning for shape 64x2048*(2x2^7)_NN_d
Tuning for shape 64x2048*(2x2^8)_NN_d
Tuning for shape 64x2048*(2x2^9)_NN_d
Tuning for shape 64x2048*(2x2^10)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^3)_NN_d
Tuning for shape 64x2048*(2x2^4)_NN_d
Tuning for shape 64x2048*(2x2^5)_NN_d
Tuning for shape 64x2048*(2x2^6)_NN_d
Tuning for shape 64x2048*(2x2^7)_NN_d
Tuning for shape 64x2048*(2x2^8)_NN_d
Tuning for shape 64x2048*(2x2^9)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^3)_NN_d
Tuning for shape 64x2048*(2x2^4)_NN_d
Tuning for shape 64x2048*(2x2^5)_NN_d
Tuning for shape 64x2048*(2x2^6)_NN_d
Tuning for shape 64x2048*(2x2^7)_NN_d
Tuning for shape 64x2048*(2x2^8)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^3)_NN_d
Tuning for shape 64x2048*(2x2^4)_NN_d
Tuning for shape 64x2048*(2x2^5)_NN_d
Tuning for shape 64x2048*(2x2^6)_NN_d
Tuning for shape 64x2048*(2x2^7)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^3)_NN_d
Tuning for shape 64x2048*(2x2^4)_NN_d
Tuning for shape 64x2048*(2x2^5)_NN_d
Tuning for shape 64x2048*(2x2^6)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^3)_NN_d
Tuning for shape 64x2048*(2x2^4)_NN_d
Tuning for shape 64x2048*(2x2^5)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^3)_NN_d
Tuning for shape 64x2048*(2x2^4)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^3)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Tuning for shape 64x2048*(2x2^2)_NN_d
Tuning for shape 64x2048*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^11  &  1.000 & 1.000 & 4.379 & 0.228
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 11 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2048] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x to produce Y[256, 2048]
Matmul: 256 x 2048 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^3)_NN_d
Tuning for shape 256x2048*(2x2^4)_NN_d
Tuning for shape 256x2048*(2x2^5)_NN_d
Tuning for shape 256x2048*(2x2^6)_NN_d
Tuning for shape 256x2048*(2x2^7)_NN_d
Tuning for shape 256x2048*(2x2^8)_NN_d
Tuning for shape 256x2048*(2x2^9)_NN_d
Tuning for shape 256x2048*(2x2^10)_NN_d
Tuning for shape 256x2048*(2x2^11)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^3)_NN_d
Tuning for shape 256x2048*(2x2^4)_NN_d
Tuning for shape 256x2048*(2x2^5)_NN_d
Tuning for shape 256x2048*(2x2^6)_NN_d
Tuning for shape 256x2048*(2x2^7)_NN_d
Tuning for shape 256x2048*(2x2^8)_NN_d
Tuning for shape 256x2048*(2x2^9)_NN_d
Tuning for shape 256x2048*(2x2^10)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^3)_NN_d
Tuning for shape 256x2048*(2x2^4)_NN_d
Tuning for shape 256x2048*(2x2^5)_NN_d
Tuning for shape 256x2048*(2x2^6)_NN_d
Tuning for shape 256x2048*(2x2^7)_NN_d
Tuning for shape 256x2048*(2x2^8)_NN_d
Tuning for shape 256x2048*(2x2^9)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^3)_NN_d
Tuning for shape 256x2048*(2x2^4)_NN_d
Tuning for shape 256x2048*(2x2^5)_NN_d
Tuning for shape 256x2048*(2x2^6)_NN_d
Tuning for shape 256x2048*(2x2^7)_NN_d
Tuning for shape 256x2048*(2x2^8)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^3)_NN_d
Tuning for shape 256x2048*(2x2^4)_NN_d
Tuning for shape 256x2048*(2x2^5)_NN_d
Tuning for shape 256x2048*(2x2^6)_NN_d
Tuning for shape 256x2048*(2x2^7)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^3)_NN_d
Tuning for shape 256x2048*(2x2^4)_NN_d
Tuning for shape 256x2048*(2x2^5)_NN_d
Tuning for shape 256x2048*(2x2^6)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^3)_NN_d
Tuning for shape 256x2048*(2x2^4)_NN_d
Tuning for shape 256x2048*(2x2^5)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^3)_NN_d
Tuning for shape 256x2048*(2x2^4)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^3)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Tuning for shape 256x2048*(2x2^2)_NN_d
Tuning for shape 256x2048*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^11  &  1.000 & 1.000 & 17.042 & 0.059
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 11 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 2048] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x to produce Y[1024, 2048]
Matmul: 1024 x 2048 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^3)_NN_d
Tuning for shape 1024x2048*(2x2^4)_NN_d
Tuning for shape 1024x2048*(2x2^5)_NN_d
Tuning for shape 1024x2048*(2x2^6)_NN_d
Tuning for shape 1024x2048*(2x2^7)_NN_d
Tuning for shape 1024x2048*(2x2^8)_NN_d
Tuning for shape 1024x2048*(2x2^9)_NN_d
Tuning for shape 1024x2048*(2x2^10)_NN_d
Tuning for shape 1024x2048*(2x2^11)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^3)_NN_d
Tuning for shape 1024x2048*(2x2^4)_NN_d
Tuning for shape 1024x2048*(2x2^5)_NN_d
Tuning for shape 1024x2048*(2x2^6)_NN_d
Tuning for shape 1024x2048*(2x2^7)_NN_d
Tuning for shape 1024x2048*(2x2^8)_NN_d
Tuning for shape 1024x2048*(2x2^9)_NN_d
Tuning for shape 1024x2048*(2x2^10)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^3)_NN_d
Tuning for shape 1024x2048*(2x2^4)_NN_d
Tuning for shape 1024x2048*(2x2^5)_NN_d
Tuning for shape 1024x2048*(2x2^6)_NN_d
Tuning for shape 1024x2048*(2x2^7)_NN_d
Tuning for shape 1024x2048*(2x2^8)_NN_d
Tuning for shape 1024x2048*(2x2^9)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^3)_NN_d
Tuning for shape 1024x2048*(2x2^4)_NN_d
Tuning for shape 1024x2048*(2x2^5)_NN_d
Tuning for shape 1024x2048*(2x2^6)_NN_d
Tuning for shape 1024x2048*(2x2^7)_NN_d
Tuning for shape 1024x2048*(2x2^8)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^3)_NN_d
Tuning for shape 1024x2048*(2x2^4)_NN_d
Tuning for shape 1024x2048*(2x2^5)_NN_d
Tuning for shape 1024x2048*(2x2^6)_NN_d
Tuning for shape 1024x2048*(2x2^7)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^3)_NN_d
Tuning for shape 1024x2048*(2x2^4)_NN_d
Tuning for shape 1024x2048*(2x2^5)_NN_d
Tuning for shape 1024x2048*(2x2^6)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^3)_NN_d
Tuning for shape 1024x2048*(2x2^4)_NN_d
Tuning for shape 1024x2048*(2x2^5)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^3)_NN_d
Tuning for shape 1024x2048*(2x2^4)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^3)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Tuning for shape 1024x2048*(2x2^2)_NN_d
Tuning for shape 1024x2048*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^11  &  1.000 & 1.000 & 17.743 & 0.056
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 12 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 4096, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^4)_NN_d
Tuning for shape 1x4096*(2x2^5)_NN_d
Tuning for shape 1x4096*(2x2^6)_NN_d
Tuning for shape 1x4096*(2x2^7)_NN_d
Tuning for shape 1x4096*(2x2^8)_NN_d
Tuning for shape 1x4096*(2x2^9)_NN_d
Tuning for shape 1x4096*(2x2^10)_NN_d
Tuning for shape 1x4096*(2x2^11)_NN_d
Tuning for shape 1x4096*(2x2^12)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^4)_NN_d
Tuning for shape 1x4096*(2x2^5)_NN_d
Tuning for shape 1x4096*(2x2^6)_NN_d
Tuning for shape 1x4096*(2x2^7)_NN_d
Tuning for shape 1x4096*(2x2^8)_NN_d
Tuning for shape 1x4096*(2x2^9)_NN_d
Tuning for shape 1x4096*(2x2^10)_NN_d
Tuning for shape 1x4096*(2x2^11)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^4)_NN_d
Tuning for shape 1x4096*(2x2^5)_NN_d
Tuning for shape 1x4096*(2x2^6)_NN_d
Tuning for shape 1x4096*(2x2^7)_NN_d
Tuning for shape 1x4096*(2x2^8)_NN_d
Tuning for shape 1x4096*(2x2^9)_NN_d
Tuning for shape 1x4096*(2x2^10)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^4)_NN_d
Tuning for shape 1x4096*(2x2^5)_NN_d
Tuning for shape 1x4096*(2x2^6)_NN_d
Tuning for shape 1x4096*(2x2^7)_NN_d
Tuning for shape 1x4096*(2x2^8)_NN_d
Tuning for shape 1x4096*(2x2^9)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^4)_NN_d
Tuning for shape 1x4096*(2x2^5)_NN_d
Tuning for shape 1x4096*(2x2^6)_NN_d
Tuning for shape 1x4096*(2x2^7)_NN_d
Tuning for shape 1x4096*(2x2^8)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^4)_NN_d
Tuning for shape 1x4096*(2x2^5)_NN_d
Tuning for shape 1x4096*(2x2^6)_NN_d
Tuning for shape 1x4096*(2x2^7)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^4)_NN_d
Tuning for shape 1x4096*(2x2^5)_NN_d
Tuning for shape 1x4096*(2x2^6)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^4)_NN_d
Tuning for shape 1x4096*(2x2^5)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^4)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^3)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Tuning for shape 1x4096*(2x2^2)_NN_d
Tuning for shape 1x4096*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^12  &  1.000 & 1.000 & 0.133 & 7.499
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 12 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 4096, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^4)_NN_d
Tuning for shape 4x4096*(2x2^5)_NN_d
Tuning for shape 4x4096*(2x2^6)_NN_d
Tuning for shape 4x4096*(2x2^7)_NN_d
Tuning for shape 4x4096*(2x2^8)_NN_d
Tuning for shape 4x4096*(2x2^9)_NN_d
Tuning for shape 4x4096*(2x2^10)_NN_d
Tuning for shape 4x4096*(2x2^11)_NN_d
Tuning for shape 4x4096*(2x2^12)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^4)_NN_d
Tuning for shape 4x4096*(2x2^5)_NN_d
Tuning for shape 4x4096*(2x2^6)_NN_d
Tuning for shape 4x4096*(2x2^7)_NN_d
Tuning for shape 4x4096*(2x2^8)_NN_d
Tuning for shape 4x4096*(2x2^9)_NN_d
Tuning for shape 4x4096*(2x2^10)_NN_d
Tuning for shape 4x4096*(2x2^11)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^4)_NN_d
Tuning for shape 4x4096*(2x2^5)_NN_d
Tuning for shape 4x4096*(2x2^6)_NN_d
Tuning for shape 4x4096*(2x2^7)_NN_d
Tuning for shape 4x4096*(2x2^8)_NN_d
Tuning for shape 4x4096*(2x2^9)_NN_d
Tuning for shape 4x4096*(2x2^10)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^4)_NN_d
Tuning for shape 4x4096*(2x2^5)_NN_d
Tuning for shape 4x4096*(2x2^6)_NN_d
Tuning for shape 4x4096*(2x2^7)_NN_d
Tuning for shape 4x4096*(2x2^8)_NN_d
Tuning for shape 4x4096*(2x2^9)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^4)_NN_d
Tuning for shape 4x4096*(2x2^5)_NN_d
Tuning for shape 4x4096*(2x2^6)_NN_d
Tuning for shape 4x4096*(2x2^7)_NN_d
Tuning for shape 4x4096*(2x2^8)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^4)_NN_d
Tuning for shape 4x4096*(2x2^5)_NN_d
Tuning for shape 4x4096*(2x2^6)_NN_d
Tuning for shape 4x4096*(2x2^7)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^4)_NN_d
Tuning for shape 4x4096*(2x2^5)_NN_d
Tuning for shape 4x4096*(2x2^6)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^4)_NN_d
Tuning for shape 4x4096*(2x2^5)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^4)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^3)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Tuning for shape 4x4096*(2x2^2)_NN_d
Tuning for shape 4x4096*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^12  &  1.000 & 1.000 & 0.576 & 1.736
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 12 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 4096, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^4)_NN_d
Tuning for shape 16x4096*(2x2^5)_NN_d
Tuning for shape 16x4096*(2x2^6)_NN_d
Tuning for shape 16x4096*(2x2^7)_NN_d
Tuning for shape 16x4096*(2x2^8)_NN_d
Tuning for shape 16x4096*(2x2^9)_NN_d
Tuning for shape 16x4096*(2x2^10)_NN_d
Tuning for shape 16x4096*(2x2^11)_NN_d
Tuning for shape 16x4096*(2x2^12)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^4)_NN_d
Tuning for shape 16x4096*(2x2^5)_NN_d
Tuning for shape 16x4096*(2x2^6)_NN_d
Tuning for shape 16x4096*(2x2^7)_NN_d
Tuning for shape 16x4096*(2x2^8)_NN_d
Tuning for shape 16x4096*(2x2^9)_NN_d
Tuning for shape 16x4096*(2x2^10)_NN_d
Tuning for shape 16x4096*(2x2^11)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^4)_NN_d
Tuning for shape 16x4096*(2x2^5)_NN_d
Tuning for shape 16x4096*(2x2^6)_NN_d
Tuning for shape 16x4096*(2x2^7)_NN_d
Tuning for shape 16x4096*(2x2^8)_NN_d
Tuning for shape 16x4096*(2x2^9)_NN_d
Tuning for shape 16x4096*(2x2^10)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^4)_NN_d
Tuning for shape 16x4096*(2x2^5)_NN_d
Tuning for shape 16x4096*(2x2^6)_NN_d
Tuning for shape 16x4096*(2x2^7)_NN_d
Tuning for shape 16x4096*(2x2^8)_NN_d
Tuning for shape 16x4096*(2x2^9)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^4)_NN_d
Tuning for shape 16x4096*(2x2^5)_NN_d
Tuning for shape 16x4096*(2x2^6)_NN_d
Tuning for shape 16x4096*(2x2^7)_NN_d
Tuning for shape 16x4096*(2x2^8)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^4)_NN_d
Tuning for shape 16x4096*(2x2^5)_NN_d
Tuning for shape 16x4096*(2x2^6)_NN_d
Tuning for shape 16x4096*(2x2^7)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^4)_NN_d
Tuning for shape 16x4096*(2x2^5)_NN_d
Tuning for shape 16x4096*(2x2^6)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^4)_NN_d
Tuning for shape 16x4096*(2x2^5)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^4)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^3)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Tuning for shape 16x4096*(2x2^2)_NN_d
Tuning for shape 16x4096*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^12  &  1.000 & 1.000 & 2.288 & 0.437
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 12 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 4096, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^4)_NN_d
Tuning for shape 64x4096*(2x2^5)_NN_d
Tuning for shape 64x4096*(2x2^6)_NN_d
Tuning for shape 64x4096*(2x2^7)_NN_d
Tuning for shape 64x4096*(2x2^8)_NN_d
Tuning for shape 64x4096*(2x2^9)_NN_d
Tuning for shape 64x4096*(2x2^10)_NN_d
Tuning for shape 64x4096*(2x2^11)_NN_d
Tuning for shape 64x4096*(2x2^12)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^4)_NN_d
Tuning for shape 64x4096*(2x2^5)_NN_d
Tuning for shape 64x4096*(2x2^6)_NN_d
Tuning for shape 64x4096*(2x2^7)_NN_d
Tuning for shape 64x4096*(2x2^8)_NN_d
Tuning for shape 64x4096*(2x2^9)_NN_d
Tuning for shape 64x4096*(2x2^10)_NN_d
Tuning for shape 64x4096*(2x2^11)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^4)_NN_d
Tuning for shape 64x4096*(2x2^5)_NN_d
Tuning for shape 64x4096*(2x2^6)_NN_d
Tuning for shape 64x4096*(2x2^7)_NN_d
Tuning for shape 64x4096*(2x2^8)_NN_d
Tuning for shape 64x4096*(2x2^9)_NN_d
Tuning for shape 64x4096*(2x2^10)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^4)_NN_d
Tuning for shape 64x4096*(2x2^5)_NN_d
Tuning for shape 64x4096*(2x2^6)_NN_d
Tuning for shape 64x4096*(2x2^7)_NN_d
Tuning for shape 64x4096*(2x2^8)_NN_d
Tuning for shape 64x4096*(2x2^9)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^4)_NN_d
Tuning for shape 64x4096*(2x2^5)_NN_d
Tuning for shape 64x4096*(2x2^6)_NN_d
Tuning for shape 64x4096*(2x2^7)_NN_d
Tuning for shape 64x4096*(2x2^8)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^4)_NN_d
Tuning for shape 64x4096*(2x2^5)_NN_d
Tuning for shape 64x4096*(2x2^6)_NN_d
Tuning for shape 64x4096*(2x2^7)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^4)_NN_d
Tuning for shape 64x4096*(2x2^5)_NN_d
Tuning for shape 64x4096*(2x2^6)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^4)_NN_d
Tuning for shape 64x4096*(2x2^5)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^4)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^3)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Tuning for shape 64x4096*(2x2^2)_NN_d
Tuning for shape 64x4096*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^12  &  1.000 & 1.000 & 8.788 & 0.114
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 12 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 4096, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^4)_NN_d
Tuning for shape 256x4096*(2x2^5)_NN_d
Tuning for shape 256x4096*(2x2^6)_NN_d
Tuning for shape 256x4096*(2x2^7)_NN_d
Tuning for shape 256x4096*(2x2^8)_NN_d
Tuning for shape 256x4096*(2x2^9)_NN_d
Tuning for shape 256x4096*(2x2^10)_NN_d
Tuning for shape 256x4096*(2x2^11)_NN_d
Tuning for shape 256x4096*(2x2^12)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^4)_NN_d
Tuning for shape 256x4096*(2x2^5)_NN_d
Tuning for shape 256x4096*(2x2^6)_NN_d
Tuning for shape 256x4096*(2x2^7)_NN_d
Tuning for shape 256x4096*(2x2^8)_NN_d
Tuning for shape 256x4096*(2x2^9)_NN_d
Tuning for shape 256x4096*(2x2^10)_NN_d
Tuning for shape 256x4096*(2x2^11)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^4)_NN_d
Tuning for shape 256x4096*(2x2^5)_NN_d
Tuning for shape 256x4096*(2x2^6)_NN_d
Tuning for shape 256x4096*(2x2^7)_NN_d
Tuning for shape 256x4096*(2x2^8)_NN_d
Tuning for shape 256x4096*(2x2^9)_NN_d
Tuning for shape 256x4096*(2x2^10)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^4)_NN_d
Tuning for shape 256x4096*(2x2^5)_NN_d
Tuning for shape 256x4096*(2x2^6)_NN_d
Tuning for shape 256x4096*(2x2^7)_NN_d
Tuning for shape 256x4096*(2x2^8)_NN_d
Tuning for shape 256x4096*(2x2^9)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^4)_NN_d
Tuning for shape 256x4096*(2x2^5)_NN_d
Tuning for shape 256x4096*(2x2^6)_NN_d
Tuning for shape 256x4096*(2x2^7)_NN_d
Tuning for shape 256x4096*(2x2^8)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^4)_NN_d
Tuning for shape 256x4096*(2x2^5)_NN_d
Tuning for shape 256x4096*(2x2^6)_NN_d
Tuning for shape 256x4096*(2x2^7)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^4)_NN_d
Tuning for shape 256x4096*(2x2^5)_NN_d
Tuning for shape 256x4096*(2x2^6)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^4)_NN_d
Tuning for shape 256x4096*(2x2^5)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^4)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^3)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Tuning for shape 256x4096*(2x2^2)_NN_d
Tuning for shape 256x4096*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^12  &  1.000 & 1.000 & 18.981 & 0.053
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 12 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 4096, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^4)_NN_d
Tuning for shape 1024x4096*(2x2^5)_NN_d
Tuning for shape 1024x4096*(2x2^6)_NN_d
Tuning for shape 1024x4096*(2x2^7)_NN_d
Tuning for shape 1024x4096*(2x2^8)_NN_d
Tuning for shape 1024x4096*(2x2^9)_NN_d
Tuning for shape 1024x4096*(2x2^10)_NN_d
Tuning for shape 1024x4096*(2x2^11)_NN_d
Tuning for shape 1024x4096*(2x2^12)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^4)_NN_d
Tuning for shape 1024x4096*(2x2^5)_NN_d
Tuning for shape 1024x4096*(2x2^6)_NN_d
Tuning for shape 1024x4096*(2x2^7)_NN_d
Tuning for shape 1024x4096*(2x2^8)_NN_d
Tuning for shape 1024x4096*(2x2^9)_NN_d
Tuning for shape 1024x4096*(2x2^10)_NN_d
Tuning for shape 1024x4096*(2x2^11)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^4)_NN_d
Tuning for shape 1024x4096*(2x2^5)_NN_d
Tuning for shape 1024x4096*(2x2^6)_NN_d
Tuning for shape 1024x4096*(2x2^7)_NN_d
Tuning for shape 1024x4096*(2x2^8)_NN_d
Tuning for shape 1024x4096*(2x2^9)_NN_d
Tuning for shape 1024x4096*(2x2^10)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^4)_NN_d
Tuning for shape 1024x4096*(2x2^5)_NN_d
Tuning for shape 1024x4096*(2x2^6)_NN_d
Tuning for shape 1024x4096*(2x2^7)_NN_d
Tuning for shape 1024x4096*(2x2^8)_NN_d
Tuning for shape 1024x4096*(2x2^9)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^4)_NN_d
Tuning for shape 1024x4096*(2x2^5)_NN_d
Tuning for shape 1024x4096*(2x2^6)_NN_d
Tuning for shape 1024x4096*(2x2^7)_NN_d
Tuning for shape 1024x4096*(2x2^8)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^4)_NN_d
Tuning for shape 1024x4096*(2x2^5)_NN_d
Tuning for shape 1024x4096*(2x2^6)_NN_d
Tuning for shape 1024x4096*(2x2^7)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^4)_NN_d
Tuning for shape 1024x4096*(2x2^5)_NN_d
Tuning for shape 1024x4096*(2x2^6)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^4)_NN_d
Tuning for shape 1024x4096*(2x2^5)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^4)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^3)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Tuning for shape 1024x4096*(2x2^2)_NN_d
Tuning for shape 1024x4096*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^12  &  1.000 & 1.000 & 19.693 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 13 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8192] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x to produce Y[1, 8192]
Matmul: 1 x 8192 x 8192, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^5)_NN_d
Tuning for shape 1x8192*(2x2^6)_NN_d
Tuning for shape 1x8192*(2x2^7)_NN_d
Tuning for shape 1x8192*(2x2^8)_NN_d
Tuning for shape 1x8192*(2x2^9)_NN_d
Tuning for shape 1x8192*(2x2^10)_NN_d
Tuning for shape 1x8192*(2x2^11)_NN_d
Tuning for shape 1x8192*(2x2^12)_NN_d
Tuning for shape 1x8192*(2x2^13)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^5)_NN_d
Tuning for shape 1x8192*(2x2^6)_NN_d
Tuning for shape 1x8192*(2x2^7)_NN_d
Tuning for shape 1x8192*(2x2^8)_NN_d
Tuning for shape 1x8192*(2x2^9)_NN_d
Tuning for shape 1x8192*(2x2^10)_NN_d
Tuning for shape 1x8192*(2x2^11)_NN_d
Tuning for shape 1x8192*(2x2^12)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^5)_NN_d
Tuning for shape 1x8192*(2x2^6)_NN_d
Tuning for shape 1x8192*(2x2^7)_NN_d
Tuning for shape 1x8192*(2x2^8)_NN_d
Tuning for shape 1x8192*(2x2^9)_NN_d
Tuning for shape 1x8192*(2x2^10)_NN_d
Tuning for shape 1x8192*(2x2^11)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^5)_NN_d
Tuning for shape 1x8192*(2x2^6)_NN_d
Tuning for shape 1x8192*(2x2^7)_NN_d
Tuning for shape 1x8192*(2x2^8)_NN_d
Tuning for shape 1x8192*(2x2^9)_NN_d
Tuning for shape 1x8192*(2x2^10)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^5)_NN_d
Tuning for shape 1x8192*(2x2^6)_NN_d
Tuning for shape 1x8192*(2x2^7)_NN_d
Tuning for shape 1x8192*(2x2^8)_NN_d
Tuning for shape 1x8192*(2x2^9)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^5)_NN_d
Tuning for shape 1x8192*(2x2^6)_NN_d
Tuning for shape 1x8192*(2x2^7)_NN_d
Tuning for shape 1x8192*(2x2^8)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^5)_NN_d
Tuning for shape 1x8192*(2x2^6)_NN_d
Tuning for shape 1x8192*(2x2^7)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^5)_NN_d
Tuning for shape 1x8192*(2x2^6)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^5)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^4)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^3)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Tuning for shape 1x8192*(2x2^2)_NN_d
Tuning for shape 1x8192*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^13  &  1.000 & 1.000 & 0.296 & 3.376
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 13 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8192] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x to produce Y[4, 8192]
Matmul: 4 x 8192 x 8192, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^5)_NN_d
Tuning for shape 4x8192*(2x2^6)_NN_d
Tuning for shape 4x8192*(2x2^7)_NN_d
Tuning for shape 4x8192*(2x2^8)_NN_d
Tuning for shape 4x8192*(2x2^9)_NN_d
Tuning for shape 4x8192*(2x2^10)_NN_d
Tuning for shape 4x8192*(2x2^11)_NN_d
Tuning for shape 4x8192*(2x2^12)_NN_d
Tuning for shape 4x8192*(2x2^13)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^5)_NN_d
Tuning for shape 4x8192*(2x2^6)_NN_d
Tuning for shape 4x8192*(2x2^7)_NN_d
Tuning for shape 4x8192*(2x2^8)_NN_d
Tuning for shape 4x8192*(2x2^9)_NN_d
Tuning for shape 4x8192*(2x2^10)_NN_d
Tuning for shape 4x8192*(2x2^11)_NN_d
Tuning for shape 4x8192*(2x2^12)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^5)_NN_d
Tuning for shape 4x8192*(2x2^6)_NN_d
Tuning for shape 4x8192*(2x2^7)_NN_d
Tuning for shape 4x8192*(2x2^8)_NN_d
Tuning for shape 4x8192*(2x2^9)_NN_d
Tuning for shape 4x8192*(2x2^10)_NN_d
Tuning for shape 4x8192*(2x2^11)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^5)_NN_d
Tuning for shape 4x8192*(2x2^6)_NN_d
Tuning for shape 4x8192*(2x2^7)_NN_d
Tuning for shape 4x8192*(2x2^8)_NN_d
Tuning for shape 4x8192*(2x2^9)_NN_d
Tuning for shape 4x8192*(2x2^10)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^5)_NN_d
Tuning for shape 4x8192*(2x2^6)_NN_d
Tuning for shape 4x8192*(2x2^7)_NN_d
Tuning for shape 4x8192*(2x2^8)_NN_d
Tuning for shape 4x8192*(2x2^9)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^5)_NN_d
Tuning for shape 4x8192*(2x2^6)_NN_d
Tuning for shape 4x8192*(2x2^7)_NN_d
Tuning for shape 4x8192*(2x2^8)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^5)_NN_d
Tuning for shape 4x8192*(2x2^6)_NN_d
Tuning for shape 4x8192*(2x2^7)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^5)_NN_d
Tuning for shape 4x8192*(2x2^6)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^5)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^4)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^3)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Tuning for shape 4x8192*(2x2^2)_NN_d
Tuning for shape 4x8192*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^13  &  1.000 & 1.000 & 1.180 & 0.848
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 13 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8192] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x to produce Y[16, 8192]
Matmul: 16 x 8192 x 8192, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^5)_NN_d
Tuning for shape 16x8192*(2x2^6)_NN_d
Tuning for shape 16x8192*(2x2^7)_NN_d
Tuning for shape 16x8192*(2x2^8)_NN_d
Tuning for shape 16x8192*(2x2^9)_NN_d
Tuning for shape 16x8192*(2x2^10)_NN_d
Tuning for shape 16x8192*(2x2^11)_NN_d
Tuning for shape 16x8192*(2x2^12)_NN_d
Tuning for shape 16x8192*(2x2^13)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^5)_NN_d
Tuning for shape 16x8192*(2x2^6)_NN_d
Tuning for shape 16x8192*(2x2^7)_NN_d
Tuning for shape 16x8192*(2x2^8)_NN_d
Tuning for shape 16x8192*(2x2^9)_NN_d
Tuning for shape 16x8192*(2x2^10)_NN_d
Tuning for shape 16x8192*(2x2^11)_NN_d
Tuning for shape 16x8192*(2x2^12)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^5)_NN_d
Tuning for shape 16x8192*(2x2^6)_NN_d
Tuning for shape 16x8192*(2x2^7)_NN_d
Tuning for shape 16x8192*(2x2^8)_NN_d
Tuning for shape 16x8192*(2x2^9)_NN_d
Tuning for shape 16x8192*(2x2^10)_NN_d
Tuning for shape 16x8192*(2x2^11)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^5)_NN_d
Tuning for shape 16x8192*(2x2^6)_NN_d
Tuning for shape 16x8192*(2x2^7)_NN_d
Tuning for shape 16x8192*(2x2^8)_NN_d
Tuning for shape 16x8192*(2x2^9)_NN_d
Tuning for shape 16x8192*(2x2^10)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^5)_NN_d
Tuning for shape 16x8192*(2x2^6)_NN_d
Tuning for shape 16x8192*(2x2^7)_NN_d
Tuning for shape 16x8192*(2x2^8)_NN_d
Tuning for shape 16x8192*(2x2^9)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^5)_NN_d
Tuning for shape 16x8192*(2x2^6)_NN_d
Tuning for shape 16x8192*(2x2^7)_NN_d
Tuning for shape 16x8192*(2x2^8)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^5)_NN_d
Tuning for shape 16x8192*(2x2^6)_NN_d
Tuning for shape 16x8192*(2x2^7)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^5)_NN_d
Tuning for shape 16x8192*(2x2^6)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^5)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^4)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^3)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Tuning for shape 16x8192*(2x2^2)_NN_d
Tuning for shape 16x8192*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^13  &  1.000 & 1.000 & 4.671 & 0.214
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 13 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8192] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x to produce Y[64, 8192]
Matmul: 64 x 8192 x 8192, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^5)_NN_d
Tuning for shape 64x8192*(2x2^6)_NN_d
Tuning for shape 64x8192*(2x2^7)_NN_d
Tuning for shape 64x8192*(2x2^8)_NN_d
Tuning for shape 64x8192*(2x2^9)_NN_d
Tuning for shape 64x8192*(2x2^10)_NN_d
Tuning for shape 64x8192*(2x2^11)_NN_d
Tuning for shape 64x8192*(2x2^12)_NN_d
Tuning for shape 64x8192*(2x2^13)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^5)_NN_d
Tuning for shape 64x8192*(2x2^6)_NN_d
Tuning for shape 64x8192*(2x2^7)_NN_d
Tuning for shape 64x8192*(2x2^8)_NN_d
Tuning for shape 64x8192*(2x2^9)_NN_d
Tuning for shape 64x8192*(2x2^10)_NN_d
Tuning for shape 64x8192*(2x2^11)_NN_d
Tuning for shape 64x8192*(2x2^12)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^5)_NN_d
Tuning for shape 64x8192*(2x2^6)_NN_d
Tuning for shape 64x8192*(2x2^7)_NN_d
Tuning for shape 64x8192*(2x2^8)_NN_d
Tuning for shape 64x8192*(2x2^9)_NN_d
Tuning for shape 64x8192*(2x2^10)_NN_d
Tuning for shape 64x8192*(2x2^11)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^5)_NN_d
Tuning for shape 64x8192*(2x2^6)_NN_d
Tuning for shape 64x8192*(2x2^7)_NN_d
Tuning for shape 64x8192*(2x2^8)_NN_d
Tuning for shape 64x8192*(2x2^9)_NN_d
Tuning for shape 64x8192*(2x2^10)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^5)_NN_d
Tuning for shape 64x8192*(2x2^6)_NN_d
Tuning for shape 64x8192*(2x2^7)_NN_d
Tuning for shape 64x8192*(2x2^8)_NN_d
Tuning for shape 64x8192*(2x2^9)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^5)_NN_d
Tuning for shape 64x8192*(2x2^6)_NN_d
Tuning for shape 64x8192*(2x2^7)_NN_d
Tuning for shape 64x8192*(2x2^8)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^5)_NN_d
Tuning for shape 64x8192*(2x2^6)_NN_d
Tuning for shape 64x8192*(2x2^7)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^5)_NN_d
Tuning for shape 64x8192*(2x2^6)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^5)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^4)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^3)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Tuning for shape 64x8192*(2x2^2)_NN_d
Tuning for shape 64x8192*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^13  &  1.000 & 1.000 & 18.034 & 0.055
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 13 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8192] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x to produce Y[256, 8192]
Matmul: 256 x 8192 x 8192, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^5)_NN_d
Tuning for shape 256x8192*(2x2^6)_NN_d
Tuning for shape 256x8192*(2x2^7)_NN_d
Tuning for shape 256x8192*(2x2^8)_NN_d
Tuning for shape 256x8192*(2x2^9)_NN_d
Tuning for shape 256x8192*(2x2^10)_NN_d
Tuning for shape 256x8192*(2x2^11)_NN_d
Tuning for shape 256x8192*(2x2^12)_NN_d
Tuning for shape 256x8192*(2x2^13)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^5)_NN_d
Tuning for shape 256x8192*(2x2^6)_NN_d
Tuning for shape 256x8192*(2x2^7)_NN_d
Tuning for shape 256x8192*(2x2^8)_NN_d
Tuning for shape 256x8192*(2x2^9)_NN_d
Tuning for shape 256x8192*(2x2^10)_NN_d
Tuning for shape 256x8192*(2x2^11)_NN_d
Tuning for shape 256x8192*(2x2^12)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^5)_NN_d
Tuning for shape 256x8192*(2x2^6)_NN_d
Tuning for shape 256x8192*(2x2^7)_NN_d
Tuning for shape 256x8192*(2x2^8)_NN_d
Tuning for shape 256x8192*(2x2^9)_NN_d
Tuning for shape 256x8192*(2x2^10)_NN_d
Tuning for shape 256x8192*(2x2^11)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^5)_NN_d
Tuning for shape 256x8192*(2x2^6)_NN_d
Tuning for shape 256x8192*(2x2^7)_NN_d
Tuning for shape 256x8192*(2x2^8)_NN_d
Tuning for shape 256x8192*(2x2^9)_NN_d
Tuning for shape 256x8192*(2x2^10)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^5)_NN_d
Tuning for shape 256x8192*(2x2^6)_NN_d
Tuning for shape 256x8192*(2x2^7)_NN_d
Tuning for shape 256x8192*(2x2^8)_NN_d
Tuning for shape 256x8192*(2x2^9)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^5)_NN_d
Tuning for shape 256x8192*(2x2^6)_NN_d
Tuning for shape 256x8192*(2x2^7)_NN_d
Tuning for shape 256x8192*(2x2^8)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^5)_NN_d
Tuning for shape 256x8192*(2x2^6)_NN_d
Tuning for shape 256x8192*(2x2^7)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^5)_NN_d
Tuning for shape 256x8192*(2x2^6)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^5)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^4)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^3)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Tuning for shape 256x8192*(2x2^2)_NN_d
Tuning for shape 256x8192*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^13  &  1.000 & 1.000 & 18.970 & 0.053
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 13 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8192] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x to produce Y[1024, 8192]
Matmul: 1024 x 8192 x 8192, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^5)_NN_d
Tuning for shape 1024x8192*(2x2^6)_NN_d
Tuning for shape 1024x8192*(2x2^7)_NN_d
Tuning for shape 1024x8192*(2x2^8)_NN_d
Tuning for shape 1024x8192*(2x2^9)_NN_d
Tuning for shape 1024x8192*(2x2^10)_NN_d
Tuning for shape 1024x8192*(2x2^11)_NN_d
Tuning for shape 1024x8192*(2x2^12)_NN_d
Tuning for shape 1024x8192*(2x2^13)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^5)_NN_d
Tuning for shape 1024x8192*(2x2^6)_NN_d
Tuning for shape 1024x8192*(2x2^7)_NN_d
Tuning for shape 1024x8192*(2x2^8)_NN_d
Tuning for shape 1024x8192*(2x2^9)_NN_d
Tuning for shape 1024x8192*(2x2^10)_NN_d
Tuning for shape 1024x8192*(2x2^11)_NN_d
Tuning for shape 1024x8192*(2x2^12)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^5)_NN_d
Tuning for shape 1024x8192*(2x2^6)_NN_d
Tuning for shape 1024x8192*(2x2^7)_NN_d
Tuning for shape 1024x8192*(2x2^8)_NN_d
Tuning for shape 1024x8192*(2x2^9)_NN_d
Tuning for shape 1024x8192*(2x2^10)_NN_d
Tuning for shape 1024x8192*(2x2^11)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^5)_NN_d
Tuning for shape 1024x8192*(2x2^6)_NN_d
Tuning for shape 1024x8192*(2x2^7)_NN_d
Tuning for shape 1024x8192*(2x2^8)_NN_d
Tuning for shape 1024x8192*(2x2^9)_NN_d
Tuning for shape 1024x8192*(2x2^10)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^5)_NN_d
Tuning for shape 1024x8192*(2x2^6)_NN_d
Tuning for shape 1024x8192*(2x2^7)_NN_d
Tuning for shape 1024x8192*(2x2^8)_NN_d
Tuning for shape 1024x8192*(2x2^9)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^5)_NN_d
Tuning for shape 1024x8192*(2x2^6)_NN_d
Tuning for shape 1024x8192*(2x2^7)_NN_d
Tuning for shape 1024x8192*(2x2^8)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^5)_NN_d
Tuning for shape 1024x8192*(2x2^6)_NN_d
Tuning for shape 1024x8192*(2x2^7)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^5)_NN_d
Tuning for shape 1024x8192*(2x2^6)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^5)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^4)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^3)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Tuning for shape 1024x8192*(2x2^2)_NN_d
Tuning for shape 1024x8192*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^13  &  1.000 & 1.000 & 19.910 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 14 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16384] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x to produce Y[1, 16384]
Matmul: 1 x 16384 x 16384, Num KP Factors: 14
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^6)_NN_d
Tuning for shape 1x16384*(2x2^7)_NN_d
Tuning for shape 1x16384*(2x2^8)_NN_d
Tuning for shape 1x16384*(2x2^9)_NN_d
Tuning for shape 1x16384*(2x2^10)_NN_d
Tuning for shape 1x16384*(2x2^11)_NN_d
Tuning for shape 1x16384*(2x2^12)_NN_d
Tuning for shape 1x16384*(2x2^13)_NN_d
Tuning for shape 1x16384*(2x2^14)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^6)_NN_d
Tuning for shape 1x16384*(2x2^7)_NN_d
Tuning for shape 1x16384*(2x2^8)_NN_d
Tuning for shape 1x16384*(2x2^9)_NN_d
Tuning for shape 1x16384*(2x2^10)_NN_d
Tuning for shape 1x16384*(2x2^11)_NN_d
Tuning for shape 1x16384*(2x2^12)_NN_d
Tuning for shape 1x16384*(2x2^13)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^6)_NN_d
Tuning for shape 1x16384*(2x2^7)_NN_d
Tuning for shape 1x16384*(2x2^8)_NN_d
Tuning for shape 1x16384*(2x2^9)_NN_d
Tuning for shape 1x16384*(2x2^10)_NN_d
Tuning for shape 1x16384*(2x2^11)_NN_d
Tuning for shape 1x16384*(2x2^12)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^6)_NN_d
Tuning for shape 1x16384*(2x2^7)_NN_d
Tuning for shape 1x16384*(2x2^8)_NN_d
Tuning for shape 1x16384*(2x2^9)_NN_d
Tuning for shape 1x16384*(2x2^10)_NN_d
Tuning for shape 1x16384*(2x2^11)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^6)_NN_d
Tuning for shape 1x16384*(2x2^7)_NN_d
Tuning for shape 1x16384*(2x2^8)_NN_d
Tuning for shape 1x16384*(2x2^9)_NN_d
Tuning for shape 1x16384*(2x2^10)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^6)_NN_d
Tuning for shape 1x16384*(2x2^7)_NN_d
Tuning for shape 1x16384*(2x2^8)_NN_d
Tuning for shape 1x16384*(2x2^9)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^6)_NN_d
Tuning for shape 1x16384*(2x2^7)_NN_d
Tuning for shape 1x16384*(2x2^8)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^6)_NN_d
Tuning for shape 1x16384*(2x2^7)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^6)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^5)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^4)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^3)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Tuning for shape 1x16384*(2x2^2)_NN_d
Tuning for shape 1x16384*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^14  &  1.000 & 1.000 & 0.615 & 1.627
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 14 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16384] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x to produce Y[4, 16384]
Matmul: 4 x 16384 x 16384, Num KP Factors: 14
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^6)_NN_d
Tuning for shape 4x16384*(2x2^7)_NN_d
Tuning for shape 4x16384*(2x2^8)_NN_d
Tuning for shape 4x16384*(2x2^9)_NN_d
Tuning for shape 4x16384*(2x2^10)_NN_d
Tuning for shape 4x16384*(2x2^11)_NN_d
Tuning for shape 4x16384*(2x2^12)_NN_d
Tuning for shape 4x16384*(2x2^13)_NN_d
Tuning for shape 4x16384*(2x2^14)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^6)_NN_d
Tuning for shape 4x16384*(2x2^7)_NN_d
Tuning for shape 4x16384*(2x2^8)_NN_d
Tuning for shape 4x16384*(2x2^9)_NN_d
Tuning for shape 4x16384*(2x2^10)_NN_d
Tuning for shape 4x16384*(2x2^11)_NN_d
Tuning for shape 4x16384*(2x2^12)_NN_d
Tuning for shape 4x16384*(2x2^13)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^6)_NN_d
Tuning for shape 4x16384*(2x2^7)_NN_d
Tuning for shape 4x16384*(2x2^8)_NN_d
Tuning for shape 4x16384*(2x2^9)_NN_d
Tuning for shape 4x16384*(2x2^10)_NN_d
Tuning for shape 4x16384*(2x2^11)_NN_d
Tuning for shape 4x16384*(2x2^12)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^6)_NN_d
Tuning for shape 4x16384*(2x2^7)_NN_d
Tuning for shape 4x16384*(2x2^8)_NN_d
Tuning for shape 4x16384*(2x2^9)_NN_d
Tuning for shape 4x16384*(2x2^10)_NN_d
Tuning for shape 4x16384*(2x2^11)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^6)_NN_d
Tuning for shape 4x16384*(2x2^7)_NN_d
Tuning for shape 4x16384*(2x2^8)_NN_d
Tuning for shape 4x16384*(2x2^9)_NN_d
Tuning for shape 4x16384*(2x2^10)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^6)_NN_d
Tuning for shape 4x16384*(2x2^7)_NN_d
Tuning for shape 4x16384*(2x2^8)_NN_d
Tuning for shape 4x16384*(2x2^9)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^6)_NN_d
Tuning for shape 4x16384*(2x2^7)_NN_d
Tuning for shape 4x16384*(2x2^8)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^6)_NN_d
Tuning for shape 4x16384*(2x2^7)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^6)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^5)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^4)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^3)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Tuning for shape 4x16384*(2x2^2)_NN_d
Tuning for shape 4x16384*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^14  &  1.000 & 1.000 & 2.449 & 0.408
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 14 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16384] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x to produce Y[16, 16384]
Matmul: 16 x 16384 x 16384, Num KP Factors: 14
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^6)_NN_d
Tuning for shape 16x16384*(2x2^7)_NN_d
Tuning for shape 16x16384*(2x2^8)_NN_d
Tuning for shape 16x16384*(2x2^9)_NN_d
Tuning for shape 16x16384*(2x2^10)_NN_d
Tuning for shape 16x16384*(2x2^11)_NN_d
Tuning for shape 16x16384*(2x2^12)_NN_d
Tuning for shape 16x16384*(2x2^13)_NN_d
Tuning for shape 16x16384*(2x2^14)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^6)_NN_d
Tuning for shape 16x16384*(2x2^7)_NN_d
Tuning for shape 16x16384*(2x2^8)_NN_d
Tuning for shape 16x16384*(2x2^9)_NN_d
Tuning for shape 16x16384*(2x2^10)_NN_d
Tuning for shape 16x16384*(2x2^11)_NN_d
Tuning for shape 16x16384*(2x2^12)_NN_d
Tuning for shape 16x16384*(2x2^13)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^6)_NN_d
Tuning for shape 16x16384*(2x2^7)_NN_d
Tuning for shape 16x16384*(2x2^8)_NN_d
Tuning for shape 16x16384*(2x2^9)_NN_d
Tuning for shape 16x16384*(2x2^10)_NN_d
Tuning for shape 16x16384*(2x2^11)_NN_d
Tuning for shape 16x16384*(2x2^12)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^6)_NN_d
Tuning for shape 16x16384*(2x2^7)_NN_d
Tuning for shape 16x16384*(2x2^8)_NN_d
Tuning for shape 16x16384*(2x2^9)_NN_d
Tuning for shape 16x16384*(2x2^10)_NN_d
Tuning for shape 16x16384*(2x2^11)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^6)_NN_d
Tuning for shape 16x16384*(2x2^7)_NN_d
Tuning for shape 16x16384*(2x2^8)_NN_d
Tuning for shape 16x16384*(2x2^9)_NN_d
Tuning for shape 16x16384*(2x2^10)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^6)_NN_d
Tuning for shape 16x16384*(2x2^7)_NN_d
Tuning for shape 16x16384*(2x2^8)_NN_d
Tuning for shape 16x16384*(2x2^9)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^6)_NN_d
Tuning for shape 16x16384*(2x2^7)_NN_d
Tuning for shape 16x16384*(2x2^8)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^6)_NN_d
Tuning for shape 16x16384*(2x2^7)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^6)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^5)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^4)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^3)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Tuning for shape 16x16384*(2x2^2)_NN_d
Tuning for shape 16x16384*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^14  &  1.000 & 1.000 & 9.177 & 0.109
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 14 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16384] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x to produce Y[64, 16384]
Matmul: 64 x 16384 x 16384, Num KP Factors: 14
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^6)_NN_d
Tuning for shape 64x16384*(2x2^7)_NN_d
Tuning for shape 64x16384*(2x2^8)_NN_d
Tuning for shape 64x16384*(2x2^9)_NN_d
Tuning for shape 64x16384*(2x2^10)_NN_d
Tuning for shape 64x16384*(2x2^11)_NN_d
Tuning for shape 64x16384*(2x2^12)_NN_d
Tuning for shape 64x16384*(2x2^13)_NN_d
Tuning for shape 64x16384*(2x2^14)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^6)_NN_d
Tuning for shape 64x16384*(2x2^7)_NN_d
Tuning for shape 64x16384*(2x2^8)_NN_d
Tuning for shape 64x16384*(2x2^9)_NN_d
Tuning for shape 64x16384*(2x2^10)_NN_d
Tuning for shape 64x16384*(2x2^11)_NN_d
Tuning for shape 64x16384*(2x2^12)_NN_d
Tuning for shape 64x16384*(2x2^13)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^6)_NN_d
Tuning for shape 64x16384*(2x2^7)_NN_d
Tuning for shape 64x16384*(2x2^8)_NN_d
Tuning for shape 64x16384*(2x2^9)_NN_d
Tuning for shape 64x16384*(2x2^10)_NN_d
Tuning for shape 64x16384*(2x2^11)_NN_d
Tuning for shape 64x16384*(2x2^12)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^6)_NN_d
Tuning for shape 64x16384*(2x2^7)_NN_d
Tuning for shape 64x16384*(2x2^8)_NN_d
Tuning for shape 64x16384*(2x2^9)_NN_d
Tuning for shape 64x16384*(2x2^10)_NN_d
Tuning for shape 64x16384*(2x2^11)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^6)_NN_d
Tuning for shape 64x16384*(2x2^7)_NN_d
Tuning for shape 64x16384*(2x2^8)_NN_d
Tuning for shape 64x16384*(2x2^9)_NN_d
Tuning for shape 64x16384*(2x2^10)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^6)_NN_d
Tuning for shape 64x16384*(2x2^7)_NN_d
Tuning for shape 64x16384*(2x2^8)_NN_d
Tuning for shape 64x16384*(2x2^9)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^6)_NN_d
Tuning for shape 64x16384*(2x2^7)_NN_d
Tuning for shape 64x16384*(2x2^8)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^6)_NN_d
Tuning for shape 64x16384*(2x2^7)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^6)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^5)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^4)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^3)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Tuning for shape 64x16384*(2x2^2)_NN_d
Tuning for shape 64x16384*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^14  &  1.000 & 1.000 & 19.287 & 0.052
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 14 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16384] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x to produce Y[256, 16384]
Matmul: 256 x 16384 x 16384, Num KP Factors: 14
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^6)_NN_d
Tuning for shape 256x16384*(2x2^7)_NN_d
Tuning for shape 256x16384*(2x2^8)_NN_d
Tuning for shape 256x16384*(2x2^9)_NN_d
Tuning for shape 256x16384*(2x2^10)_NN_d
Tuning for shape 256x16384*(2x2^11)_NN_d
Tuning for shape 256x16384*(2x2^12)_NN_d
Tuning for shape 256x16384*(2x2^13)_NN_d
Tuning for shape 256x16384*(2x2^14)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^6)_NN_d
Tuning for shape 256x16384*(2x2^7)_NN_d
Tuning for shape 256x16384*(2x2^8)_NN_d
Tuning for shape 256x16384*(2x2^9)_NN_d
Tuning for shape 256x16384*(2x2^10)_NN_d
Tuning for shape 256x16384*(2x2^11)_NN_d
Tuning for shape 256x16384*(2x2^12)_NN_d
Tuning for shape 256x16384*(2x2^13)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^6)_NN_d
Tuning for shape 256x16384*(2x2^7)_NN_d
Tuning for shape 256x16384*(2x2^8)_NN_d
Tuning for shape 256x16384*(2x2^9)_NN_d
Tuning for shape 256x16384*(2x2^10)_NN_d
Tuning for shape 256x16384*(2x2^11)_NN_d
Tuning for shape 256x16384*(2x2^12)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^6)_NN_d
Tuning for shape 256x16384*(2x2^7)_NN_d
Tuning for shape 256x16384*(2x2^8)_NN_d
Tuning for shape 256x16384*(2x2^9)_NN_d
Tuning for shape 256x16384*(2x2^10)_NN_d
Tuning for shape 256x16384*(2x2^11)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^6)_NN_d
Tuning for shape 256x16384*(2x2^7)_NN_d
Tuning for shape 256x16384*(2x2^8)_NN_d
Tuning for shape 256x16384*(2x2^9)_NN_d
Tuning for shape 256x16384*(2x2^10)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^6)_NN_d
Tuning for shape 256x16384*(2x2^7)_NN_d
Tuning for shape 256x16384*(2x2^8)_NN_d
Tuning for shape 256x16384*(2x2^9)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^6)_NN_d
Tuning for shape 256x16384*(2x2^7)_NN_d
Tuning for shape 256x16384*(2x2^8)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^6)_NN_d
Tuning for shape 256x16384*(2x2^7)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^6)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^5)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^4)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^3)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Tuning for shape 256x16384*(2x2^2)_NN_d
Tuning for shape 256x16384*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^14  &  1.000 & 1.000 & 19.716 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 14 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16384] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x to produce Y[1024, 16384]
Matmul: 1024 x 16384 x 16384, Num KP Factors: 14
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^6)_NN_d
Tuning for shape 1024x16384*(2x2^7)_NN_d
Tuning for shape 1024x16384*(2x2^8)_NN_d
Tuning for shape 1024x16384*(2x2^9)_NN_d
Tuning for shape 1024x16384*(2x2^10)_NN_d
Tuning for shape 1024x16384*(2x2^11)_NN_d
Tuning for shape 1024x16384*(2x2^12)_NN_d
Tuning for shape 1024x16384*(2x2^13)_NN_d
Tuning for shape 1024x16384*(2x2^14)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^6)_NN_d
Tuning for shape 1024x16384*(2x2^7)_NN_d
Tuning for shape 1024x16384*(2x2^8)_NN_d
Tuning for shape 1024x16384*(2x2^9)_NN_d
Tuning for shape 1024x16384*(2x2^10)_NN_d
Tuning for shape 1024x16384*(2x2^11)_NN_d
Tuning for shape 1024x16384*(2x2^12)_NN_d
Tuning for shape 1024x16384*(2x2^13)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^6)_NN_d
Tuning for shape 1024x16384*(2x2^7)_NN_d
Tuning for shape 1024x16384*(2x2^8)_NN_d
Tuning for shape 1024x16384*(2x2^9)_NN_d
Tuning for shape 1024x16384*(2x2^10)_NN_d
Tuning for shape 1024x16384*(2x2^11)_NN_d
Tuning for shape 1024x16384*(2x2^12)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^6)_NN_d
Tuning for shape 1024x16384*(2x2^7)_NN_d
Tuning for shape 1024x16384*(2x2^8)_NN_d
Tuning for shape 1024x16384*(2x2^9)_NN_d
Tuning for shape 1024x16384*(2x2^10)_NN_d
Tuning for shape 1024x16384*(2x2^11)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^6)_NN_d
Tuning for shape 1024x16384*(2x2^7)_NN_d
Tuning for shape 1024x16384*(2x2^8)_NN_d
Tuning for shape 1024x16384*(2x2^9)_NN_d
Tuning for shape 1024x16384*(2x2^10)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^6)_NN_d
Tuning for shape 1024x16384*(2x2^7)_NN_d
Tuning for shape 1024x16384*(2x2^8)_NN_d
Tuning for shape 1024x16384*(2x2^9)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^6)_NN_d
Tuning for shape 1024x16384*(2x2^7)_NN_d
Tuning for shape 1024x16384*(2x2^8)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^6)_NN_d
Tuning for shape 1024x16384*(2x2^7)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^6)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^5)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^4)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^3)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Tuning for shape 1024x16384*(2x2^2)_NN_d
Tuning for shape 1024x16384*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^14  &  1.000 & 1.000 & 19.999 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 15 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32768] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x to produce Y[1, 32768]
Matmul: 1 x 32768 x 32768, Num KP Factors: 15
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^7)_NN_d
Tuning for shape 1x32768*(2x2^8)_NN_d
Tuning for shape 1x32768*(2x2^9)_NN_d
Tuning for shape 1x32768*(2x2^10)_NN_d
Tuning for shape 1x32768*(2x2^11)_NN_d
Tuning for shape 1x32768*(2x2^12)_NN_d
Tuning for shape 1x32768*(2x2^13)_NN_d
Tuning for shape 1x32768*(2x2^14)_NN_d
Tuning for shape 1x32768*(2x2^15)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^7)_NN_d
Tuning for shape 1x32768*(2x2^8)_NN_d
Tuning for shape 1x32768*(2x2^9)_NN_d
Tuning for shape 1x32768*(2x2^10)_NN_d
Tuning for shape 1x32768*(2x2^11)_NN_d
Tuning for shape 1x32768*(2x2^12)_NN_d
Tuning for shape 1x32768*(2x2^13)_NN_d
Tuning for shape 1x32768*(2x2^14)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^7)_NN_d
Tuning for shape 1x32768*(2x2^8)_NN_d
Tuning for shape 1x32768*(2x2^9)_NN_d
Tuning for shape 1x32768*(2x2^10)_NN_d
Tuning for shape 1x32768*(2x2^11)_NN_d
Tuning for shape 1x32768*(2x2^12)_NN_d
Tuning for shape 1x32768*(2x2^13)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^7)_NN_d
Tuning for shape 1x32768*(2x2^8)_NN_d
Tuning for shape 1x32768*(2x2^9)_NN_d
Tuning for shape 1x32768*(2x2^10)_NN_d
Tuning for shape 1x32768*(2x2^11)_NN_d
Tuning for shape 1x32768*(2x2^12)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^7)_NN_d
Tuning for shape 1x32768*(2x2^8)_NN_d
Tuning for shape 1x32768*(2x2^9)_NN_d
Tuning for shape 1x32768*(2x2^10)_NN_d
Tuning for shape 1x32768*(2x2^11)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^7)_NN_d
Tuning for shape 1x32768*(2x2^8)_NN_d
Tuning for shape 1x32768*(2x2^9)_NN_d
Tuning for shape 1x32768*(2x2^10)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^7)_NN_d
Tuning for shape 1x32768*(2x2^8)_NN_d
Tuning for shape 1x32768*(2x2^9)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^7)_NN_d
Tuning for shape 1x32768*(2x2^8)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^7)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^6)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^5)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^4)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^3)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Tuning for shape 1x32768*(2x2^2)_NN_d
Tuning for shape 1x32768*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^15  &  1.000 & 1.000 & 1.205 & 0.830
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 15 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32768] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x to produce Y[4, 32768]
Matmul: 4 x 32768 x 32768, Num KP Factors: 15
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^7)_NN_d
Tuning for shape 4x32768*(2x2^8)_NN_d
Tuning for shape 4x32768*(2x2^9)_NN_d
Tuning for shape 4x32768*(2x2^10)_NN_d
Tuning for shape 4x32768*(2x2^11)_NN_d
Tuning for shape 4x32768*(2x2^12)_NN_d
Tuning for shape 4x32768*(2x2^13)_NN_d
Tuning for shape 4x32768*(2x2^14)_NN_d
Tuning for shape 4x32768*(2x2^15)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^7)_NN_d
Tuning for shape 4x32768*(2x2^8)_NN_d
Tuning for shape 4x32768*(2x2^9)_NN_d
Tuning for shape 4x32768*(2x2^10)_NN_d
Tuning for shape 4x32768*(2x2^11)_NN_d
Tuning for shape 4x32768*(2x2^12)_NN_d
Tuning for shape 4x32768*(2x2^13)_NN_d
Tuning for shape 4x32768*(2x2^14)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^7)_NN_d
Tuning for shape 4x32768*(2x2^8)_NN_d
Tuning for shape 4x32768*(2x2^9)_NN_d
Tuning for shape 4x32768*(2x2^10)_NN_d
Tuning for shape 4x32768*(2x2^11)_NN_d
Tuning for shape 4x32768*(2x2^12)_NN_d
Tuning for shape 4x32768*(2x2^13)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^7)_NN_d
Tuning for shape 4x32768*(2x2^8)_NN_d
Tuning for shape 4x32768*(2x2^9)_NN_d
Tuning for shape 4x32768*(2x2^10)_NN_d
Tuning for shape 4x32768*(2x2^11)_NN_d
Tuning for shape 4x32768*(2x2^12)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^7)_NN_d
Tuning for shape 4x32768*(2x2^8)_NN_d
Tuning for shape 4x32768*(2x2^9)_NN_d
Tuning for shape 4x32768*(2x2^10)_NN_d
Tuning for shape 4x32768*(2x2^11)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^7)_NN_d
Tuning for shape 4x32768*(2x2^8)_NN_d
Tuning for shape 4x32768*(2x2^9)_NN_d
Tuning for shape 4x32768*(2x2^10)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^7)_NN_d
Tuning for shape 4x32768*(2x2^8)_NN_d
Tuning for shape 4x32768*(2x2^9)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^7)_NN_d
Tuning for shape 4x32768*(2x2^8)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^7)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^6)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^5)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^4)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^3)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Tuning for shape 4x32768*(2x2^2)_NN_d
Tuning for shape 4x32768*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^15  &  1.000 & 1.000 & 4.385 & 0.228
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 15 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32768] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x to produce Y[16, 32768]
Matmul: 16 x 32768 x 32768, Num KP Factors: 15
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^7)_NN_d
Tuning for shape 16x32768*(2x2^8)_NN_d
Tuning for shape 16x32768*(2x2^9)_NN_d
Tuning for shape 16x32768*(2x2^10)_NN_d
Tuning for shape 16x32768*(2x2^11)_NN_d
Tuning for shape 16x32768*(2x2^12)_NN_d
Tuning for shape 16x32768*(2x2^13)_NN_d
Tuning for shape 16x32768*(2x2^14)_NN_d
Tuning for shape 16x32768*(2x2^15)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^7)_NN_d
Tuning for shape 16x32768*(2x2^8)_NN_d
Tuning for shape 16x32768*(2x2^9)_NN_d
Tuning for shape 16x32768*(2x2^10)_NN_d
Tuning for shape 16x32768*(2x2^11)_NN_d
Tuning for shape 16x32768*(2x2^12)_NN_d
Tuning for shape 16x32768*(2x2^13)_NN_d
Tuning for shape 16x32768*(2x2^14)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^7)_NN_d
Tuning for shape 16x32768*(2x2^8)_NN_d
Tuning for shape 16x32768*(2x2^9)_NN_d
Tuning for shape 16x32768*(2x2^10)_NN_d
Tuning for shape 16x32768*(2x2^11)_NN_d
Tuning for shape 16x32768*(2x2^12)_NN_d
Tuning for shape 16x32768*(2x2^13)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^7)_NN_d
Tuning for shape 16x32768*(2x2^8)_NN_d
Tuning for shape 16x32768*(2x2^9)_NN_d
Tuning for shape 16x32768*(2x2^10)_NN_d
Tuning for shape 16x32768*(2x2^11)_NN_d
Tuning for shape 16x32768*(2x2^12)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^7)_NN_d
Tuning for shape 16x32768*(2x2^8)_NN_d
Tuning for shape 16x32768*(2x2^9)_NN_d
Tuning for shape 16x32768*(2x2^10)_NN_d
Tuning for shape 16x32768*(2x2^11)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^7)_NN_d
Tuning for shape 16x32768*(2x2^8)_NN_d
Tuning for shape 16x32768*(2x2^9)_NN_d
Tuning for shape 16x32768*(2x2^10)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^7)_NN_d
Tuning for shape 16x32768*(2x2^8)_NN_d
Tuning for shape 16x32768*(2x2^9)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^7)_NN_d
Tuning for shape 16x32768*(2x2^8)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^7)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^6)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^5)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^4)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^3)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Tuning for shape 16x32768*(2x2^2)_NN_d
Tuning for shape 16x32768*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^15  &  1.000 & 1.000 & 18.194 & 0.055
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 15 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 32768] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x to produce Y[64, 32768]
Matmul: 64 x 32768 x 32768, Num KP Factors: 15
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^7)_NN_d
Tuning for shape 64x32768*(2x2^8)_NN_d
Tuning for shape 64x32768*(2x2^9)_NN_d
Tuning for shape 64x32768*(2x2^10)_NN_d
Tuning for shape 64x32768*(2x2^11)_NN_d
Tuning for shape 64x32768*(2x2^12)_NN_d
Tuning for shape 64x32768*(2x2^13)_NN_d
Tuning for shape 64x32768*(2x2^14)_NN_d
Tuning for shape 64x32768*(2x2^15)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^7)_NN_d
Tuning for shape 64x32768*(2x2^8)_NN_d
Tuning for shape 64x32768*(2x2^9)_NN_d
Tuning for shape 64x32768*(2x2^10)_NN_d
Tuning for shape 64x32768*(2x2^11)_NN_d
Tuning for shape 64x32768*(2x2^12)_NN_d
Tuning for shape 64x32768*(2x2^13)_NN_d
Tuning for shape 64x32768*(2x2^14)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^7)_NN_d
Tuning for shape 64x32768*(2x2^8)_NN_d
Tuning for shape 64x32768*(2x2^9)_NN_d
Tuning for shape 64x32768*(2x2^10)_NN_d
Tuning for shape 64x32768*(2x2^11)_NN_d
Tuning for shape 64x32768*(2x2^12)_NN_d
Tuning for shape 64x32768*(2x2^13)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^7)_NN_d
Tuning for shape 64x32768*(2x2^8)_NN_d
Tuning for shape 64x32768*(2x2^9)_NN_d
Tuning for shape 64x32768*(2x2^10)_NN_d
Tuning for shape 64x32768*(2x2^11)_NN_d
Tuning for shape 64x32768*(2x2^12)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^7)_NN_d
Tuning for shape 64x32768*(2x2^8)_NN_d
Tuning for shape 64x32768*(2x2^9)_NN_d
Tuning for shape 64x32768*(2x2^10)_NN_d
Tuning for shape 64x32768*(2x2^11)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^7)_NN_d
Tuning for shape 64x32768*(2x2^8)_NN_d
Tuning for shape 64x32768*(2x2^9)_NN_d
Tuning for shape 64x32768*(2x2^10)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^7)_NN_d
Tuning for shape 64x32768*(2x2^8)_NN_d
Tuning for shape 64x32768*(2x2^9)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^7)_NN_d
Tuning for shape 64x32768*(2x2^8)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^7)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^6)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^5)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^4)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^3)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Tuning for shape 64x32768*(2x2^2)_NN_d
Tuning for shape 64x32768*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^15  &  1.000 & 1.000 & 19.538 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 15 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 32768] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x to produce Y[256, 32768]
Matmul: 256 x 32768 x 32768, Num KP Factors: 15
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^7)_NN_d
Tuning for shape 256x32768*(2x2^8)_NN_d
Tuning for shape 256x32768*(2x2^9)_NN_d
Tuning for shape 256x32768*(2x2^10)_NN_d
Tuning for shape 256x32768*(2x2^11)_NN_d
Tuning for shape 256x32768*(2x2^12)_NN_d
Tuning for shape 256x32768*(2x2^13)_NN_d
Tuning for shape 256x32768*(2x2^14)_NN_d
Tuning for shape 256x32768*(2x2^15)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^7)_NN_d
Tuning for shape 256x32768*(2x2^8)_NN_d
Tuning for shape 256x32768*(2x2^9)_NN_d
Tuning for shape 256x32768*(2x2^10)_NN_d
Tuning for shape 256x32768*(2x2^11)_NN_d
Tuning for shape 256x32768*(2x2^12)_NN_d
Tuning for shape 256x32768*(2x2^13)_NN_d
Tuning for shape 256x32768*(2x2^14)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^7)_NN_d
Tuning for shape 256x32768*(2x2^8)_NN_d
Tuning for shape 256x32768*(2x2^9)_NN_d
Tuning for shape 256x32768*(2x2^10)_NN_d
Tuning for shape 256x32768*(2x2^11)_NN_d
Tuning for shape 256x32768*(2x2^12)_NN_d
Tuning for shape 256x32768*(2x2^13)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^7)_NN_d
Tuning for shape 256x32768*(2x2^8)_NN_d
Tuning for shape 256x32768*(2x2^9)_NN_d
Tuning for shape 256x32768*(2x2^10)_NN_d
Tuning for shape 256x32768*(2x2^11)_NN_d
Tuning for shape 256x32768*(2x2^12)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^7)_NN_d
Tuning for shape 256x32768*(2x2^8)_NN_d
Tuning for shape 256x32768*(2x2^9)_NN_d
Tuning for shape 256x32768*(2x2^10)_NN_d
Tuning for shape 256x32768*(2x2^11)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^7)_NN_d
Tuning for shape 256x32768*(2x2^8)_NN_d
Tuning for shape 256x32768*(2x2^9)_NN_d
Tuning for shape 256x32768*(2x2^10)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^7)_NN_d
Tuning for shape 256x32768*(2x2^8)_NN_d
Tuning for shape 256x32768*(2x2^9)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^7)_NN_d
Tuning for shape 256x32768*(2x2^8)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^7)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^6)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^5)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^4)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^3)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Tuning for shape 256x32768*(2x2^2)_NN_d
Tuning for shape 256x32768*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^15  &  1.000 & 1.000 & 19.757 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 15 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 32768] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x to produce Y[1024, 32768]
Matmul: 1024 x 32768 x 32768, Num KP Factors: 15
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^7)_NN_d
Tuning for shape 1024x32768*(2x2^8)_NN_d
Tuning for shape 1024x32768*(2x2^9)_NN_d
Tuning for shape 1024x32768*(2x2^10)_NN_d
Tuning for shape 1024x32768*(2x2^11)_NN_d
Tuning for shape 1024x32768*(2x2^12)_NN_d
Tuning for shape 1024x32768*(2x2^13)_NN_d
Tuning for shape 1024x32768*(2x2^14)_NN_d
Tuning for shape 1024x32768*(2x2^15)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^7)_NN_d
Tuning for shape 1024x32768*(2x2^8)_NN_d
Tuning for shape 1024x32768*(2x2^9)_NN_d
Tuning for shape 1024x32768*(2x2^10)_NN_d
Tuning for shape 1024x32768*(2x2^11)_NN_d
Tuning for shape 1024x32768*(2x2^12)_NN_d
Tuning for shape 1024x32768*(2x2^13)_NN_d
Tuning for shape 1024x32768*(2x2^14)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^7)_NN_d
Tuning for shape 1024x32768*(2x2^8)_NN_d
Tuning for shape 1024x32768*(2x2^9)_NN_d
Tuning for shape 1024x32768*(2x2^10)_NN_d
Tuning for shape 1024x32768*(2x2^11)_NN_d
Tuning for shape 1024x32768*(2x2^12)_NN_d
Tuning for shape 1024x32768*(2x2^13)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^7)_NN_d
Tuning for shape 1024x32768*(2x2^8)_NN_d
Tuning for shape 1024x32768*(2x2^9)_NN_d
Tuning for shape 1024x32768*(2x2^10)_NN_d
Tuning for shape 1024x32768*(2x2^11)_NN_d
Tuning for shape 1024x32768*(2x2^12)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^7)_NN_d
Tuning for shape 1024x32768*(2x2^8)_NN_d
Tuning for shape 1024x32768*(2x2^9)_NN_d
Tuning for shape 1024x32768*(2x2^10)_NN_d
Tuning for shape 1024x32768*(2x2^11)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^7)_NN_d
Tuning for shape 1024x32768*(2x2^8)_NN_d
Tuning for shape 1024x32768*(2x2^9)_NN_d
Tuning for shape 1024x32768*(2x2^10)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^7)_NN_d
Tuning for shape 1024x32768*(2x2^8)_NN_d
Tuning for shape 1024x32768*(2x2^9)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^7)_NN_d
Tuning for shape 1024x32768*(2x2^8)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^7)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^6)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^5)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^4)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^3)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Tuning for shape 1024x32768*(2x2^2)_NN_d
Tuning for shape 1024x32768*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^15  &  1.000 & 1.000 & 19.809 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 16 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 65536] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x to produce Y[1, 65536]
Matmul: 1 x 65536 x 65536, Num KP Factors: 16
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^8)_NN_d
Tuning for shape 1x65536*(2x2^9)_NN_d
Tuning for shape 1x65536*(2x2^10)_NN_d
Tuning for shape 1x65536*(2x2^11)_NN_d
Tuning for shape 1x65536*(2x2^12)_NN_d
Tuning for shape 1x65536*(2x2^13)_NN_d
Tuning for shape 1x65536*(2x2^14)_NN_d
Tuning for shape 1x65536*(2x2^15)_NN_d
Tuning for shape 1x65536*(2x2^16)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^8)_NN_d
Tuning for shape 1x65536*(2x2^9)_NN_d
Tuning for shape 1x65536*(2x2^10)_NN_d
Tuning for shape 1x65536*(2x2^11)_NN_d
Tuning for shape 1x65536*(2x2^12)_NN_d
Tuning for shape 1x65536*(2x2^13)_NN_d
Tuning for shape 1x65536*(2x2^14)_NN_d
Tuning for shape 1x65536*(2x2^15)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^8)_NN_d
Tuning for shape 1x65536*(2x2^9)_NN_d
Tuning for shape 1x65536*(2x2^10)_NN_d
Tuning for shape 1x65536*(2x2^11)_NN_d
Tuning for shape 1x65536*(2x2^12)_NN_d
Tuning for shape 1x65536*(2x2^13)_NN_d
Tuning for shape 1x65536*(2x2^14)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^8)_NN_d
Tuning for shape 1x65536*(2x2^9)_NN_d
Tuning for shape 1x65536*(2x2^10)_NN_d
Tuning for shape 1x65536*(2x2^11)_NN_d
Tuning for shape 1x65536*(2x2^12)_NN_d
Tuning for shape 1x65536*(2x2^13)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^8)_NN_d
Tuning for shape 1x65536*(2x2^9)_NN_d
Tuning for shape 1x65536*(2x2^10)_NN_d
Tuning for shape 1x65536*(2x2^11)_NN_d
Tuning for shape 1x65536*(2x2^12)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^8)_NN_d
Tuning for shape 1x65536*(2x2^9)_NN_d
Tuning for shape 1x65536*(2x2^10)_NN_d
Tuning for shape 1x65536*(2x2^11)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^8)_NN_d
Tuning for shape 1x65536*(2x2^9)_NN_d
Tuning for shape 1x65536*(2x2^10)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^8)_NN_d
Tuning for shape 1x65536*(2x2^9)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^8)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^7)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^6)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^5)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^4)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^3)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Tuning for shape 1x65536*(2x2^2)_NN_d
Tuning for shape 1x65536*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^16  &  1.000 & 1.000 & 2.464 & 0.406
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 16 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 65536] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x to produce Y[4, 65536]
Matmul: 4 x 65536 x 65536, Num KP Factors: 16
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^8)_NN_d
Tuning for shape 4x65536*(2x2^9)_NN_d
Tuning for shape 4x65536*(2x2^10)_NN_d
Tuning for shape 4x65536*(2x2^11)_NN_d
Tuning for shape 4x65536*(2x2^12)_NN_d
Tuning for shape 4x65536*(2x2^13)_NN_d
Tuning for shape 4x65536*(2x2^14)_NN_d
Tuning for shape 4x65536*(2x2^15)_NN_d
Tuning for shape 4x65536*(2x2^16)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^8)_NN_d
Tuning for shape 4x65536*(2x2^9)_NN_d
Tuning for shape 4x65536*(2x2^10)_NN_d
Tuning for shape 4x65536*(2x2^11)_NN_d
Tuning for shape 4x65536*(2x2^12)_NN_d
Tuning for shape 4x65536*(2x2^13)_NN_d
Tuning for shape 4x65536*(2x2^14)_NN_d
Tuning for shape 4x65536*(2x2^15)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^8)_NN_d
Tuning for shape 4x65536*(2x2^9)_NN_d
Tuning for shape 4x65536*(2x2^10)_NN_d
Tuning for shape 4x65536*(2x2^11)_NN_d
Tuning for shape 4x65536*(2x2^12)_NN_d
Tuning for shape 4x65536*(2x2^13)_NN_d
Tuning for shape 4x65536*(2x2^14)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^8)_NN_d
Tuning for shape 4x65536*(2x2^9)_NN_d
Tuning for shape 4x65536*(2x2^10)_NN_d
Tuning for shape 4x65536*(2x2^11)_NN_d
Tuning for shape 4x65536*(2x2^12)_NN_d
Tuning for shape 4x65536*(2x2^13)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^8)_NN_d
Tuning for shape 4x65536*(2x2^9)_NN_d
Tuning for shape 4x65536*(2x2^10)_NN_d
Tuning for shape 4x65536*(2x2^11)_NN_d
Tuning for shape 4x65536*(2x2^12)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^8)_NN_d
Tuning for shape 4x65536*(2x2^9)_NN_d
Tuning for shape 4x65536*(2x2^10)_NN_d
Tuning for shape 4x65536*(2x2^11)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^8)_NN_d
Tuning for shape 4x65536*(2x2^9)_NN_d
Tuning for shape 4x65536*(2x2^10)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^8)_NN_d
Tuning for shape 4x65536*(2x2^9)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^8)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^7)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^6)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^5)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^4)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^3)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Tuning for shape 4x65536*(2x2^2)_NN_d
Tuning for shape 4x65536*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^16  &  1.000 & 1.000 & 9.374 & 0.107
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 16 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 65536] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x to produce Y[16, 65536]
Matmul: 16 x 65536 x 65536, Num KP Factors: 16
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^8)_NN_d
Tuning for shape 16x65536*(2x2^9)_NN_d
Tuning for shape 16x65536*(2x2^10)_NN_d
Tuning for shape 16x65536*(2x2^11)_NN_d
Tuning for shape 16x65536*(2x2^12)_NN_d
Tuning for shape 16x65536*(2x2^13)_NN_d
Tuning for shape 16x65536*(2x2^14)_NN_d
Tuning for shape 16x65536*(2x2^15)_NN_d
Tuning for shape 16x65536*(2x2^16)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^8)_NN_d
Tuning for shape 16x65536*(2x2^9)_NN_d
Tuning for shape 16x65536*(2x2^10)_NN_d
Tuning for shape 16x65536*(2x2^11)_NN_d
Tuning for shape 16x65536*(2x2^12)_NN_d
Tuning for shape 16x65536*(2x2^13)_NN_d
Tuning for shape 16x65536*(2x2^14)_NN_d
Tuning for shape 16x65536*(2x2^15)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^8)_NN_d
Tuning for shape 16x65536*(2x2^9)_NN_d
Tuning for shape 16x65536*(2x2^10)_NN_d
Tuning for shape 16x65536*(2x2^11)_NN_d
Tuning for shape 16x65536*(2x2^12)_NN_d
Tuning for shape 16x65536*(2x2^13)_NN_d
Tuning for shape 16x65536*(2x2^14)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^8)_NN_d
Tuning for shape 16x65536*(2x2^9)_NN_d
Tuning for shape 16x65536*(2x2^10)_NN_d
Tuning for shape 16x65536*(2x2^11)_NN_d
Tuning for shape 16x65536*(2x2^12)_NN_d
Tuning for shape 16x65536*(2x2^13)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^8)_NN_d
Tuning for shape 16x65536*(2x2^9)_NN_d
Tuning for shape 16x65536*(2x2^10)_NN_d
Tuning for shape 16x65536*(2x2^11)_NN_d
Tuning for shape 16x65536*(2x2^12)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^8)_NN_d
Tuning for shape 16x65536*(2x2^9)_NN_d
Tuning for shape 16x65536*(2x2^10)_NN_d
Tuning for shape 16x65536*(2x2^11)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^8)_NN_d
Tuning for shape 16x65536*(2x2^9)_NN_d
Tuning for shape 16x65536*(2x2^10)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^8)_NN_d
Tuning for shape 16x65536*(2x2^9)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^8)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^7)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^6)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^5)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^4)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^3)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Tuning for shape 16x65536*(2x2^2)_NN_d
Tuning for shape 16x65536*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^16  &  1.000 & 1.000 & 19.309 & 0.052
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 16 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 65536] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x to produce Y[64, 65536]
Matmul: 64 x 65536 x 65536, Num KP Factors: 16
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^8)_NN_d
Tuning for shape 64x65536*(2x2^9)_NN_d
Tuning for shape 64x65536*(2x2^10)_NN_d
Tuning for shape 64x65536*(2x2^11)_NN_d
Tuning for shape 64x65536*(2x2^12)_NN_d
Tuning for shape 64x65536*(2x2^13)_NN_d
Tuning for shape 64x65536*(2x2^14)_NN_d
Tuning for shape 64x65536*(2x2^15)_NN_d
Tuning for shape 64x65536*(2x2^16)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^8)_NN_d
Tuning for shape 64x65536*(2x2^9)_NN_d
Tuning for shape 64x65536*(2x2^10)_NN_d
Tuning for shape 64x65536*(2x2^11)_NN_d
Tuning for shape 64x65536*(2x2^12)_NN_d
Tuning for shape 64x65536*(2x2^13)_NN_d
Tuning for shape 64x65536*(2x2^14)_NN_d
Tuning for shape 64x65536*(2x2^15)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^8)_NN_d
Tuning for shape 64x65536*(2x2^9)_NN_d
Tuning for shape 64x65536*(2x2^10)_NN_d
Tuning for shape 64x65536*(2x2^11)_NN_d
Tuning for shape 64x65536*(2x2^12)_NN_d
Tuning for shape 64x65536*(2x2^13)_NN_d
Tuning for shape 64x65536*(2x2^14)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^8)_NN_d
Tuning for shape 64x65536*(2x2^9)_NN_d
Tuning for shape 64x65536*(2x2^10)_NN_d
Tuning for shape 64x65536*(2x2^11)_NN_d
Tuning for shape 64x65536*(2x2^12)_NN_d
Tuning for shape 64x65536*(2x2^13)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^8)_NN_d
Tuning for shape 64x65536*(2x2^9)_NN_d
Tuning for shape 64x65536*(2x2^10)_NN_d
Tuning for shape 64x65536*(2x2^11)_NN_d
Tuning for shape 64x65536*(2x2^12)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^8)_NN_d
Tuning for shape 64x65536*(2x2^9)_NN_d
Tuning for shape 64x65536*(2x2^10)_NN_d
Tuning for shape 64x65536*(2x2^11)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^8)_NN_d
Tuning for shape 64x65536*(2x2^9)_NN_d
Tuning for shape 64x65536*(2x2^10)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^8)_NN_d
Tuning for shape 64x65536*(2x2^9)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^8)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^7)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^6)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^5)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^4)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^3)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Tuning for shape 64x65536*(2x2^2)_NN_d
Tuning for shape 64x65536*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^16  &  1.000 & 1.000 & 19.784 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 16 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 65536] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x to produce Y[256, 65536]
Matmul: 256 x 65536 x 65536, Num KP Factors: 16
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^8)_NN_d
Tuning for shape 256x65536*(2x2^9)_NN_d
Tuning for shape 256x65536*(2x2^10)_NN_d
Tuning for shape 256x65536*(2x2^11)_NN_d
Tuning for shape 256x65536*(2x2^12)_NN_d
Tuning for shape 256x65536*(2x2^13)_NN_d
Tuning for shape 256x65536*(2x2^14)_NN_d
Tuning for shape 256x65536*(2x2^15)_NN_d
Tuning for shape 256x65536*(2x2^16)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^8)_NN_d
Tuning for shape 256x65536*(2x2^9)_NN_d
Tuning for shape 256x65536*(2x2^10)_NN_d
Tuning for shape 256x65536*(2x2^11)_NN_d
Tuning for shape 256x65536*(2x2^12)_NN_d
Tuning for shape 256x65536*(2x2^13)_NN_d
Tuning for shape 256x65536*(2x2^14)_NN_d
Tuning for shape 256x65536*(2x2^15)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^8)_NN_d
Tuning for shape 256x65536*(2x2^9)_NN_d
Tuning for shape 256x65536*(2x2^10)_NN_d
Tuning for shape 256x65536*(2x2^11)_NN_d
Tuning for shape 256x65536*(2x2^12)_NN_d
Tuning for shape 256x65536*(2x2^13)_NN_d
Tuning for shape 256x65536*(2x2^14)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^8)_NN_d
Tuning for shape 256x65536*(2x2^9)_NN_d
Tuning for shape 256x65536*(2x2^10)_NN_d
Tuning for shape 256x65536*(2x2^11)_NN_d
Tuning for shape 256x65536*(2x2^12)_NN_d
Tuning for shape 256x65536*(2x2^13)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^8)_NN_d
Tuning for shape 256x65536*(2x2^9)_NN_d
Tuning for shape 256x65536*(2x2^10)_NN_d
Tuning for shape 256x65536*(2x2^11)_NN_d
Tuning for shape 256x65536*(2x2^12)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^8)_NN_d
Tuning for shape 256x65536*(2x2^9)_NN_d
Tuning for shape 256x65536*(2x2^10)_NN_d
Tuning for shape 256x65536*(2x2^11)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^8)_NN_d
Tuning for shape 256x65536*(2x2^9)_NN_d
Tuning for shape 256x65536*(2x2^10)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^8)_NN_d
Tuning for shape 256x65536*(2x2^9)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^8)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^7)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^6)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^5)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^4)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^3)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Tuning for shape 256x65536*(2x2^2)_NN_d
Tuning for shape 256x65536*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^16  &  1.000 & 1.000 & 19.680 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 16 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 65536] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x to produce Y[1024, 65536]
Matmul: 1024 x 65536 x 65536, Num KP Factors: 16
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^8)_NN_d
Tuning for shape 1024x65536*(2x2^9)_NN_d
Tuning for shape 1024x65536*(2x2^10)_NN_d
Tuning for shape 1024x65536*(2x2^11)_NN_d
Tuning for shape 1024x65536*(2x2^12)_NN_d
Tuning for shape 1024x65536*(2x2^13)_NN_d
Tuning for shape 1024x65536*(2x2^14)_NN_d
Tuning for shape 1024x65536*(2x2^15)_NN_d
Tuning for shape 1024x65536*(2x2^16)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^8)_NN_d
Tuning for shape 1024x65536*(2x2^9)_NN_d
Tuning for shape 1024x65536*(2x2^10)_NN_d
Tuning for shape 1024x65536*(2x2^11)_NN_d
Tuning for shape 1024x65536*(2x2^12)_NN_d
Tuning for shape 1024x65536*(2x2^13)_NN_d
Tuning for shape 1024x65536*(2x2^14)_NN_d
Tuning for shape 1024x65536*(2x2^15)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^8)_NN_d
Tuning for shape 1024x65536*(2x2^9)_NN_d
Tuning for shape 1024x65536*(2x2^10)_NN_d
Tuning for shape 1024x65536*(2x2^11)_NN_d
Tuning for shape 1024x65536*(2x2^12)_NN_d
Tuning for shape 1024x65536*(2x2^13)_NN_d
Tuning for shape 1024x65536*(2x2^14)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^8)_NN_d
Tuning for shape 1024x65536*(2x2^9)_NN_d
Tuning for shape 1024x65536*(2x2^10)_NN_d
Tuning for shape 1024x65536*(2x2^11)_NN_d
Tuning for shape 1024x65536*(2x2^12)_NN_d
Tuning for shape 1024x65536*(2x2^13)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^8)_NN_d
Tuning for shape 1024x65536*(2x2^9)_NN_d
Tuning for shape 1024x65536*(2x2^10)_NN_d
Tuning for shape 1024x65536*(2x2^11)_NN_d
Tuning for shape 1024x65536*(2x2^12)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^8)_NN_d
Tuning for shape 1024x65536*(2x2^9)_NN_d
Tuning for shape 1024x65536*(2x2^10)_NN_d
Tuning for shape 1024x65536*(2x2^11)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^8)_NN_d
Tuning for shape 1024x65536*(2x2^9)_NN_d
Tuning for shape 1024x65536*(2x2^10)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^8)_NN_d
Tuning for shape 1024x65536*(2x2^9)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^8)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^7)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^6)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^5)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^4)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^3)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Tuning for shape 1024x65536*(2x2^2)_NN_d
Tuning for shape 1024x65536*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^16  &  1.000 & 1.000 & 19.664 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 17 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 131072] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x to produce Y[1, 131072]
Matmul: 1 x 131072 x 131072, Num KP Factors: 17
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^9)_NN_d
Tuning for shape 1x131072*(2x2^10)_NN_d
Tuning for shape 1x131072*(2x2^11)_NN_d
Tuning for shape 1x131072*(2x2^12)_NN_d
Tuning for shape 1x131072*(2x2^13)_NN_d
Tuning for shape 1x131072*(2x2^14)_NN_d
Tuning for shape 1x131072*(2x2^15)_NN_d
Tuning for shape 1x131072*(2x2^16)_NN_d
Tuning for shape 1x131072*(2x2^17)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^9)_NN_d
Tuning for shape 1x131072*(2x2^10)_NN_d
Tuning for shape 1x131072*(2x2^11)_NN_d
Tuning for shape 1x131072*(2x2^12)_NN_d
Tuning for shape 1x131072*(2x2^13)_NN_d
Tuning for shape 1x131072*(2x2^14)_NN_d
Tuning for shape 1x131072*(2x2^15)_NN_d
Tuning for shape 1x131072*(2x2^16)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^9)_NN_d
Tuning for shape 1x131072*(2x2^10)_NN_d
Tuning for shape 1x131072*(2x2^11)_NN_d
Tuning for shape 1x131072*(2x2^12)_NN_d
Tuning for shape 1x131072*(2x2^13)_NN_d
Tuning for shape 1x131072*(2x2^14)_NN_d
Tuning for shape 1x131072*(2x2^15)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^9)_NN_d
Tuning for shape 1x131072*(2x2^10)_NN_d
Tuning for shape 1x131072*(2x2^11)_NN_d
Tuning for shape 1x131072*(2x2^12)_NN_d
Tuning for shape 1x131072*(2x2^13)_NN_d
Tuning for shape 1x131072*(2x2^14)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^9)_NN_d
Tuning for shape 1x131072*(2x2^10)_NN_d
Tuning for shape 1x131072*(2x2^11)_NN_d
Tuning for shape 1x131072*(2x2^12)_NN_d
Tuning for shape 1x131072*(2x2^13)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^9)_NN_d
Tuning for shape 1x131072*(2x2^10)_NN_d
Tuning for shape 1x131072*(2x2^11)_NN_d
Tuning for shape 1x131072*(2x2^12)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^9)_NN_d
Tuning for shape 1x131072*(2x2^10)_NN_d
Tuning for shape 1x131072*(2x2^11)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^9)_NN_d
Tuning for shape 1x131072*(2x2^10)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^9)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^8)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^7)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^6)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^5)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^4)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^3)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Tuning for shape 1x131072*(2x2^2)_NN_d
Tuning for shape 1x131072*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^17  &  1.000 & 1.000 & 4.997 & 0.200
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 17 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 131072] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x to produce Y[4, 131072]
Matmul: 4 x 131072 x 131072, Num KP Factors: 17
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^9)_NN_d
Tuning for shape 4x131072*(2x2^10)_NN_d
Tuning for shape 4x131072*(2x2^11)_NN_d
Tuning for shape 4x131072*(2x2^12)_NN_d
Tuning for shape 4x131072*(2x2^13)_NN_d
Tuning for shape 4x131072*(2x2^14)_NN_d
Tuning for shape 4x131072*(2x2^15)_NN_d
Tuning for shape 4x131072*(2x2^16)_NN_d
Tuning for shape 4x131072*(2x2^17)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^9)_NN_d
Tuning for shape 4x131072*(2x2^10)_NN_d
Tuning for shape 4x131072*(2x2^11)_NN_d
Tuning for shape 4x131072*(2x2^12)_NN_d
Tuning for shape 4x131072*(2x2^13)_NN_d
Tuning for shape 4x131072*(2x2^14)_NN_d
Tuning for shape 4x131072*(2x2^15)_NN_d
Tuning for shape 4x131072*(2x2^16)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^9)_NN_d
Tuning for shape 4x131072*(2x2^10)_NN_d
Tuning for shape 4x131072*(2x2^11)_NN_d
Tuning for shape 4x131072*(2x2^12)_NN_d
Tuning for shape 4x131072*(2x2^13)_NN_d
Tuning for shape 4x131072*(2x2^14)_NN_d
Tuning for shape 4x131072*(2x2^15)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^9)_NN_d
Tuning for shape 4x131072*(2x2^10)_NN_d
Tuning for shape 4x131072*(2x2^11)_NN_d
Tuning for shape 4x131072*(2x2^12)_NN_d
Tuning for shape 4x131072*(2x2^13)_NN_d
Tuning for shape 4x131072*(2x2^14)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^9)_NN_d
Tuning for shape 4x131072*(2x2^10)_NN_d
Tuning for shape 4x131072*(2x2^11)_NN_d
Tuning for shape 4x131072*(2x2^12)_NN_d
Tuning for shape 4x131072*(2x2^13)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^9)_NN_d
Tuning for shape 4x131072*(2x2^10)_NN_d
Tuning for shape 4x131072*(2x2^11)_NN_d
Tuning for shape 4x131072*(2x2^12)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^9)_NN_d
Tuning for shape 4x131072*(2x2^10)_NN_d
Tuning for shape 4x131072*(2x2^11)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^9)_NN_d
Tuning for shape 4x131072*(2x2^10)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^9)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^8)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^7)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^6)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^5)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^4)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^3)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Tuning for shape 4x131072*(2x2^2)_NN_d
Tuning for shape 4x131072*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^17  &  1.000 & 1.000 & 18.771 & 0.053
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 17 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 131072] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x to produce Y[16, 131072]
Matmul: 16 x 131072 x 131072, Num KP Factors: 17
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^9)_NN_d
Tuning for shape 16x131072*(2x2^10)_NN_d
Tuning for shape 16x131072*(2x2^11)_NN_d
Tuning for shape 16x131072*(2x2^12)_NN_d
Tuning for shape 16x131072*(2x2^13)_NN_d
Tuning for shape 16x131072*(2x2^14)_NN_d
Tuning for shape 16x131072*(2x2^15)_NN_d
Tuning for shape 16x131072*(2x2^16)_NN_d
Tuning for shape 16x131072*(2x2^17)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^9)_NN_d
Tuning for shape 16x131072*(2x2^10)_NN_d
Tuning for shape 16x131072*(2x2^11)_NN_d
Tuning for shape 16x131072*(2x2^12)_NN_d
Tuning for shape 16x131072*(2x2^13)_NN_d
Tuning for shape 16x131072*(2x2^14)_NN_d
Tuning for shape 16x131072*(2x2^15)_NN_d
Tuning for shape 16x131072*(2x2^16)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^9)_NN_d
Tuning for shape 16x131072*(2x2^10)_NN_d
Tuning for shape 16x131072*(2x2^11)_NN_d
Tuning for shape 16x131072*(2x2^12)_NN_d
Tuning for shape 16x131072*(2x2^13)_NN_d
Tuning for shape 16x131072*(2x2^14)_NN_d
Tuning for shape 16x131072*(2x2^15)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^9)_NN_d
Tuning for shape 16x131072*(2x2^10)_NN_d
Tuning for shape 16x131072*(2x2^11)_NN_d
Tuning for shape 16x131072*(2x2^12)_NN_d
Tuning for shape 16x131072*(2x2^13)_NN_d
Tuning for shape 16x131072*(2x2^14)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^9)_NN_d
Tuning for shape 16x131072*(2x2^10)_NN_d
Tuning for shape 16x131072*(2x2^11)_NN_d
Tuning for shape 16x131072*(2x2^12)_NN_d
Tuning for shape 16x131072*(2x2^13)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^9)_NN_d
Tuning for shape 16x131072*(2x2^10)_NN_d
Tuning for shape 16x131072*(2x2^11)_NN_d
Tuning for shape 16x131072*(2x2^12)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^9)_NN_d
Tuning for shape 16x131072*(2x2^10)_NN_d
Tuning for shape 16x131072*(2x2^11)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^9)_NN_d
Tuning for shape 16x131072*(2x2^10)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^9)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^8)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^7)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^6)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^5)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^4)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^3)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Tuning for shape 16x131072*(2x2^2)_NN_d
Tuning for shape 16x131072*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^17  &  1.000 & 1.000 & 19.550 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 17 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 131072] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x to produce Y[64, 131072]
Matmul: 64 x 131072 x 131072, Num KP Factors: 17
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^9)_NN_d
Tuning for shape 64x131072*(2x2^10)_NN_d
Tuning for shape 64x131072*(2x2^11)_NN_d
Tuning for shape 64x131072*(2x2^12)_NN_d
Tuning for shape 64x131072*(2x2^13)_NN_d
Tuning for shape 64x131072*(2x2^14)_NN_d
Tuning for shape 64x131072*(2x2^15)_NN_d
Tuning for shape 64x131072*(2x2^16)_NN_d
Tuning for shape 64x131072*(2x2^17)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^9)_NN_d
Tuning for shape 64x131072*(2x2^10)_NN_d
Tuning for shape 64x131072*(2x2^11)_NN_d
Tuning for shape 64x131072*(2x2^12)_NN_d
Tuning for shape 64x131072*(2x2^13)_NN_d
Tuning for shape 64x131072*(2x2^14)_NN_d
Tuning for shape 64x131072*(2x2^15)_NN_d
Tuning for shape 64x131072*(2x2^16)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^9)_NN_d
Tuning for shape 64x131072*(2x2^10)_NN_d
Tuning for shape 64x131072*(2x2^11)_NN_d
Tuning for shape 64x131072*(2x2^12)_NN_d
Tuning for shape 64x131072*(2x2^13)_NN_d
Tuning for shape 64x131072*(2x2^14)_NN_d
Tuning for shape 64x131072*(2x2^15)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^9)_NN_d
Tuning for shape 64x131072*(2x2^10)_NN_d
Tuning for shape 64x131072*(2x2^11)_NN_d
Tuning for shape 64x131072*(2x2^12)_NN_d
Tuning for shape 64x131072*(2x2^13)_NN_d
Tuning for shape 64x131072*(2x2^14)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^9)_NN_d
Tuning for shape 64x131072*(2x2^10)_NN_d
Tuning for shape 64x131072*(2x2^11)_NN_d
Tuning for shape 64x131072*(2x2^12)_NN_d
Tuning for shape 64x131072*(2x2^13)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^9)_NN_d
Tuning for shape 64x131072*(2x2^10)_NN_d
Tuning for shape 64x131072*(2x2^11)_NN_d
Tuning for shape 64x131072*(2x2^12)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^9)_NN_d
Tuning for shape 64x131072*(2x2^10)_NN_d
Tuning for shape 64x131072*(2x2^11)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^9)_NN_d
Tuning for shape 64x131072*(2x2^10)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^9)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^8)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^7)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^6)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^5)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^4)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^3)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Tuning for shape 64x131072*(2x2^2)_NN_d
Tuning for shape 64x131072*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^17  &  1.000 & 1.000 & 19.981 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 17 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 131072] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x to produce Y[256, 131072]
Matmul: 256 x 131072 x 131072, Num KP Factors: 17
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^9)_NN_d
Tuning for shape 256x131072*(2x2^10)_NN_d
Tuning for shape 256x131072*(2x2^11)_NN_d
Tuning for shape 256x131072*(2x2^12)_NN_d
Tuning for shape 256x131072*(2x2^13)_NN_d
Tuning for shape 256x131072*(2x2^14)_NN_d
Tuning for shape 256x131072*(2x2^15)_NN_d
Tuning for shape 256x131072*(2x2^16)_NN_d
Tuning for shape 256x131072*(2x2^17)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^9)_NN_d
Tuning for shape 256x131072*(2x2^10)_NN_d
Tuning for shape 256x131072*(2x2^11)_NN_d
Tuning for shape 256x131072*(2x2^12)_NN_d
Tuning for shape 256x131072*(2x2^13)_NN_d
Tuning for shape 256x131072*(2x2^14)_NN_d
Tuning for shape 256x131072*(2x2^15)_NN_d
Tuning for shape 256x131072*(2x2^16)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^9)_NN_d
Tuning for shape 256x131072*(2x2^10)_NN_d
Tuning for shape 256x131072*(2x2^11)_NN_d
Tuning for shape 256x131072*(2x2^12)_NN_d
Tuning for shape 256x131072*(2x2^13)_NN_d
Tuning for shape 256x131072*(2x2^14)_NN_d
Tuning for shape 256x131072*(2x2^15)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^9)_NN_d
Tuning for shape 256x131072*(2x2^10)_NN_d
Tuning for shape 256x131072*(2x2^11)_NN_d
Tuning for shape 256x131072*(2x2^12)_NN_d
Tuning for shape 256x131072*(2x2^13)_NN_d
Tuning for shape 256x131072*(2x2^14)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^9)_NN_d
Tuning for shape 256x131072*(2x2^10)_NN_d
Tuning for shape 256x131072*(2x2^11)_NN_d
Tuning for shape 256x131072*(2x2^12)_NN_d
Tuning for shape 256x131072*(2x2^13)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^9)_NN_d
Tuning for shape 256x131072*(2x2^10)_NN_d
Tuning for shape 256x131072*(2x2^11)_NN_d
Tuning for shape 256x131072*(2x2^12)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^9)_NN_d
Tuning for shape 256x131072*(2x2^10)_NN_d
Tuning for shape 256x131072*(2x2^11)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^9)_NN_d
Tuning for shape 256x131072*(2x2^10)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^9)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^8)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^7)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^6)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^5)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^4)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^3)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Tuning for shape 256x131072*(2x2^2)_NN_d
Tuning for shape 256x131072*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^17  &  1.000 & 1.000 & 19.567 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 17 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 131072] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x to produce Y[1024, 131072]
Matmul: 1024 x 131072 x 131072, Num KP Factors: 17
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^9)_NN_d
Tuning for shape 1024x131072*(2x2^10)_NN_d
Tuning for shape 1024x131072*(2x2^11)_NN_d
Tuning for shape 1024x131072*(2x2^12)_NN_d
Tuning for shape 1024x131072*(2x2^13)_NN_d
Tuning for shape 1024x131072*(2x2^14)_NN_d
Tuning for shape 1024x131072*(2x2^15)_NN_d
Tuning for shape 1024x131072*(2x2^16)_NN_d
Tuning for shape 1024x131072*(2x2^17)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^9)_NN_d
Tuning for shape 1024x131072*(2x2^10)_NN_d
Tuning for shape 1024x131072*(2x2^11)_NN_d
Tuning for shape 1024x131072*(2x2^12)_NN_d
Tuning for shape 1024x131072*(2x2^13)_NN_d
Tuning for shape 1024x131072*(2x2^14)_NN_d
Tuning for shape 1024x131072*(2x2^15)_NN_d
Tuning for shape 1024x131072*(2x2^16)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^9)_NN_d
Tuning for shape 1024x131072*(2x2^10)_NN_d
Tuning for shape 1024x131072*(2x2^11)_NN_d
Tuning for shape 1024x131072*(2x2^12)_NN_d
Tuning for shape 1024x131072*(2x2^13)_NN_d
Tuning for shape 1024x131072*(2x2^14)_NN_d
Tuning for shape 1024x131072*(2x2^15)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^9)_NN_d
Tuning for shape 1024x131072*(2x2^10)_NN_d
Tuning for shape 1024x131072*(2x2^11)_NN_d
Tuning for shape 1024x131072*(2x2^12)_NN_d
Tuning for shape 1024x131072*(2x2^13)_NN_d
Tuning for shape 1024x131072*(2x2^14)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^9)_NN_d
Tuning for shape 1024x131072*(2x2^10)_NN_d
Tuning for shape 1024x131072*(2x2^11)_NN_d
Tuning for shape 1024x131072*(2x2^12)_NN_d
Tuning for shape 1024x131072*(2x2^13)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^9)_NN_d
Tuning for shape 1024x131072*(2x2^10)_NN_d
Tuning for shape 1024x131072*(2x2^11)_NN_d
Tuning for shape 1024x131072*(2x2^12)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^9)_NN_d
Tuning for shape 1024x131072*(2x2^10)_NN_d
Tuning for shape 1024x131072*(2x2^11)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^9)_NN_d
Tuning for shape 1024x131072*(2x2^10)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^9)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^8)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^7)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^6)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^5)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^4)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^3)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Tuning for shape 1024x131072*(2x2^2)_NN_d
Tuning for shape 1024x131072*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^17  &  1.000 & 1.000 & 19.604 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 18 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 262144] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 262144, Num KP Factors: 18
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^10)_NN_d
Tuning for shape 1x262144*(2x2^11)_NN_d
Tuning for shape 1x262144*(2x2^12)_NN_d
Tuning for shape 1x262144*(2x2^13)_NN_d
Tuning for shape 1x262144*(2x2^14)_NN_d
Tuning for shape 1x262144*(2x2^15)_NN_d
Tuning for shape 1x262144*(2x2^16)_NN_d
Tuning for shape 1x262144*(2x2^17)_NN_d
Tuning for shape 1x262144*(2x2^18)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^10)_NN_d
Tuning for shape 1x262144*(2x2^11)_NN_d
Tuning for shape 1x262144*(2x2^12)_NN_d
Tuning for shape 1x262144*(2x2^13)_NN_d
Tuning for shape 1x262144*(2x2^14)_NN_d
Tuning for shape 1x262144*(2x2^15)_NN_d
Tuning for shape 1x262144*(2x2^16)_NN_d
Tuning for shape 1x262144*(2x2^17)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^10)_NN_d
Tuning for shape 1x262144*(2x2^11)_NN_d
Tuning for shape 1x262144*(2x2^12)_NN_d
Tuning for shape 1x262144*(2x2^13)_NN_d
Tuning for shape 1x262144*(2x2^14)_NN_d
Tuning for shape 1x262144*(2x2^15)_NN_d
Tuning for shape 1x262144*(2x2^16)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^10)_NN_d
Tuning for shape 1x262144*(2x2^11)_NN_d
Tuning for shape 1x262144*(2x2^12)_NN_d
Tuning for shape 1x262144*(2x2^13)_NN_d
Tuning for shape 1x262144*(2x2^14)_NN_d
Tuning for shape 1x262144*(2x2^15)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^10)_NN_d
Tuning for shape 1x262144*(2x2^11)_NN_d
Tuning for shape 1x262144*(2x2^12)_NN_d
Tuning for shape 1x262144*(2x2^13)_NN_d
Tuning for shape 1x262144*(2x2^14)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^10)_NN_d
Tuning for shape 1x262144*(2x2^11)_NN_d
Tuning for shape 1x262144*(2x2^12)_NN_d
Tuning for shape 1x262144*(2x2^13)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^10)_NN_d
Tuning for shape 1x262144*(2x2^11)_NN_d
Tuning for shape 1x262144*(2x2^12)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^10)_NN_d
Tuning for shape 1x262144*(2x2^11)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^10)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^9)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^8)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^7)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^6)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^5)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^4)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^3)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Tuning for shape 1x262144*(2x2^2)_NN_d
Tuning for shape 1x262144*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^18  &  1.000 & 1.000 & 9.975 & 0.100
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 18 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 262144] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 262144, Num KP Factors: 18
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^10)_NN_d
Tuning for shape 4x262144*(2x2^11)_NN_d
Tuning for shape 4x262144*(2x2^12)_NN_d
Tuning for shape 4x262144*(2x2^13)_NN_d
Tuning for shape 4x262144*(2x2^14)_NN_d
Tuning for shape 4x262144*(2x2^15)_NN_d
Tuning for shape 4x262144*(2x2^16)_NN_d
Tuning for shape 4x262144*(2x2^17)_NN_d
Tuning for shape 4x262144*(2x2^18)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^10)_NN_d
Tuning for shape 4x262144*(2x2^11)_NN_d
Tuning for shape 4x262144*(2x2^12)_NN_d
Tuning for shape 4x262144*(2x2^13)_NN_d
Tuning for shape 4x262144*(2x2^14)_NN_d
Tuning for shape 4x262144*(2x2^15)_NN_d
Tuning for shape 4x262144*(2x2^16)_NN_d
Tuning for shape 4x262144*(2x2^17)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^10)_NN_d
Tuning for shape 4x262144*(2x2^11)_NN_d
Tuning for shape 4x262144*(2x2^12)_NN_d
Tuning for shape 4x262144*(2x2^13)_NN_d
Tuning for shape 4x262144*(2x2^14)_NN_d
Tuning for shape 4x262144*(2x2^15)_NN_d
Tuning for shape 4x262144*(2x2^16)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^10)_NN_d
Tuning for shape 4x262144*(2x2^11)_NN_d
Tuning for shape 4x262144*(2x2^12)_NN_d
Tuning for shape 4x262144*(2x2^13)_NN_d
Tuning for shape 4x262144*(2x2^14)_NN_d
Tuning for shape 4x262144*(2x2^15)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^10)_NN_d
Tuning for shape 4x262144*(2x2^11)_NN_d
Tuning for shape 4x262144*(2x2^12)_NN_d
Tuning for shape 4x262144*(2x2^13)_NN_d
Tuning for shape 4x262144*(2x2^14)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^10)_NN_d
Tuning for shape 4x262144*(2x2^11)_NN_d
Tuning for shape 4x262144*(2x2^12)_NN_d
Tuning for shape 4x262144*(2x2^13)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^10)_NN_d
Tuning for shape 4x262144*(2x2^11)_NN_d
Tuning for shape 4x262144*(2x2^12)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^10)_NN_d
Tuning for shape 4x262144*(2x2^11)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^10)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^9)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^8)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^7)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^6)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^5)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^4)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^3)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Tuning for shape 4x262144*(2x2^2)_NN_d
Tuning for shape 4x262144*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^18  &  1.000 & 1.000 & 13.153 & 0.076
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 18 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 262144] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 262144, Num KP Factors: 18
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^10)_NN_d
Tuning for shape 16x262144*(2x2^11)_NN_d
Tuning for shape 16x262144*(2x2^12)_NN_d
Tuning for shape 16x262144*(2x2^13)_NN_d
Tuning for shape 16x262144*(2x2^14)_NN_d
Tuning for shape 16x262144*(2x2^15)_NN_d
Tuning for shape 16x262144*(2x2^16)_NN_d
Tuning for shape 16x262144*(2x2^17)_NN_d
Tuning for shape 16x262144*(2x2^18)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^10)_NN_d
Tuning for shape 16x262144*(2x2^11)_NN_d
Tuning for shape 16x262144*(2x2^12)_NN_d
Tuning for shape 16x262144*(2x2^13)_NN_d
Tuning for shape 16x262144*(2x2^14)_NN_d
Tuning for shape 16x262144*(2x2^15)_NN_d
Tuning for shape 16x262144*(2x2^16)_NN_d
Tuning for shape 16x262144*(2x2^17)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^10)_NN_d
Tuning for shape 16x262144*(2x2^11)_NN_d
Tuning for shape 16x262144*(2x2^12)_NN_d
Tuning for shape 16x262144*(2x2^13)_NN_d
Tuning for shape 16x262144*(2x2^14)_NN_d
Tuning for shape 16x262144*(2x2^15)_NN_d
Tuning for shape 16x262144*(2x2^16)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^10)_NN_d
Tuning for shape 16x262144*(2x2^11)_NN_d
Tuning for shape 16x262144*(2x2^12)_NN_d
Tuning for shape 16x262144*(2x2^13)_NN_d
Tuning for shape 16x262144*(2x2^14)_NN_d
Tuning for shape 16x262144*(2x2^15)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^10)_NN_d
Tuning for shape 16x262144*(2x2^11)_NN_d
Tuning for shape 16x262144*(2x2^12)_NN_d
Tuning for shape 16x262144*(2x2^13)_NN_d
Tuning for shape 16x262144*(2x2^14)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^10)_NN_d
Tuning for shape 16x262144*(2x2^11)_NN_d
Tuning for shape 16x262144*(2x2^12)_NN_d
Tuning for shape 16x262144*(2x2^13)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^10)_NN_d
Tuning for shape 16x262144*(2x2^11)_NN_d
Tuning for shape 16x262144*(2x2^12)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^10)_NN_d
Tuning for shape 16x262144*(2x2^11)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^10)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^9)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^8)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^7)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^6)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^5)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^4)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^3)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Tuning for shape 16x262144*(2x2^2)_NN_d
Tuning for shape 16x262144*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^18  &  1.000 & 1.000 & 19.855 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 18 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 262144] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x to produce Y[64, 262144]
Matmul: 64 x 262144 x 262144, Num KP Factors: 18
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^10)_NN_d
Tuning for shape 64x262144*(2x2^11)_NN_d
Tuning for shape 64x262144*(2x2^12)_NN_d
Tuning for shape 64x262144*(2x2^13)_NN_d
Tuning for shape 64x262144*(2x2^14)_NN_d
Tuning for shape 64x262144*(2x2^15)_NN_d
Tuning for shape 64x262144*(2x2^16)_NN_d
Tuning for shape 64x262144*(2x2^17)_NN_d
Tuning for shape 64x262144*(2x2^18)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^10)_NN_d
Tuning for shape 64x262144*(2x2^11)_NN_d
Tuning for shape 64x262144*(2x2^12)_NN_d
Tuning for shape 64x262144*(2x2^13)_NN_d
Tuning for shape 64x262144*(2x2^14)_NN_d
Tuning for shape 64x262144*(2x2^15)_NN_d
Tuning for shape 64x262144*(2x2^16)_NN_d
Tuning for shape 64x262144*(2x2^17)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^10)_NN_d
Tuning for shape 64x262144*(2x2^11)_NN_d
Tuning for shape 64x262144*(2x2^12)_NN_d
Tuning for shape 64x262144*(2x2^13)_NN_d
Tuning for shape 64x262144*(2x2^14)_NN_d
Tuning for shape 64x262144*(2x2^15)_NN_d
Tuning for shape 64x262144*(2x2^16)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^10)_NN_d
Tuning for shape 64x262144*(2x2^11)_NN_d
Tuning for shape 64x262144*(2x2^12)_NN_d
Tuning for shape 64x262144*(2x2^13)_NN_d
Tuning for shape 64x262144*(2x2^14)_NN_d
Tuning for shape 64x262144*(2x2^15)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^10)_NN_d
Tuning for shape 64x262144*(2x2^11)_NN_d
Tuning for shape 64x262144*(2x2^12)_NN_d
Tuning for shape 64x262144*(2x2^13)_NN_d
Tuning for shape 64x262144*(2x2^14)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^10)_NN_d
Tuning for shape 64x262144*(2x2^11)_NN_d
Tuning for shape 64x262144*(2x2^12)_NN_d
Tuning for shape 64x262144*(2x2^13)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^10)_NN_d
Tuning for shape 64x262144*(2x2^11)_NN_d
Tuning for shape 64x262144*(2x2^12)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^10)_NN_d
Tuning for shape 64x262144*(2x2^11)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^10)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^9)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^8)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^7)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^6)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^5)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^4)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^3)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Tuning for shape 64x262144*(2x2^2)_NN_d
Tuning for shape 64x262144*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^18  &  1.000 & 1.000 & 20.086 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 18 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 262144] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x to produce Y[256, 262144]
Matmul: 256 x 262144 x 262144, Num KP Factors: 18
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^10)_NN_d
Tuning for shape 256x262144*(2x2^11)_NN_d
Tuning for shape 256x262144*(2x2^12)_NN_d
Tuning for shape 256x262144*(2x2^13)_NN_d
Tuning for shape 256x262144*(2x2^14)_NN_d
Tuning for shape 256x262144*(2x2^15)_NN_d
Tuning for shape 256x262144*(2x2^16)_NN_d
Tuning for shape 256x262144*(2x2^17)_NN_d
Tuning for shape 256x262144*(2x2^18)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^10)_NN_d
Tuning for shape 256x262144*(2x2^11)_NN_d
Tuning for shape 256x262144*(2x2^12)_NN_d
Tuning for shape 256x262144*(2x2^13)_NN_d
Tuning for shape 256x262144*(2x2^14)_NN_d
Tuning for shape 256x262144*(2x2^15)_NN_d
Tuning for shape 256x262144*(2x2^16)_NN_d
Tuning for shape 256x262144*(2x2^17)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^10)_NN_d
Tuning for shape 256x262144*(2x2^11)_NN_d
Tuning for shape 256x262144*(2x2^12)_NN_d
Tuning for shape 256x262144*(2x2^13)_NN_d
Tuning for shape 256x262144*(2x2^14)_NN_d
Tuning for shape 256x262144*(2x2^15)_NN_d
Tuning for shape 256x262144*(2x2^16)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^10)_NN_d
Tuning for shape 256x262144*(2x2^11)_NN_d
Tuning for shape 256x262144*(2x2^12)_NN_d
Tuning for shape 256x262144*(2x2^13)_NN_d
Tuning for shape 256x262144*(2x2^14)_NN_d
Tuning for shape 256x262144*(2x2^15)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^10)_NN_d
Tuning for shape 256x262144*(2x2^11)_NN_d
Tuning for shape 256x262144*(2x2^12)_NN_d
Tuning for shape 256x262144*(2x2^13)_NN_d
Tuning for shape 256x262144*(2x2^14)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^10)_NN_d
Tuning for shape 256x262144*(2x2^11)_NN_d
Tuning for shape 256x262144*(2x2^12)_NN_d
Tuning for shape 256x262144*(2x2^13)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^10)_NN_d
Tuning for shape 256x262144*(2x2^11)_NN_d
Tuning for shape 256x262144*(2x2^12)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^10)_NN_d
Tuning for shape 256x262144*(2x2^11)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^10)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^9)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^8)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^7)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^6)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^5)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^4)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^3)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Tuning for shape 256x262144*(2x2^2)_NN_d
Tuning for shape 256x262144*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^18  &  1.000 & 1.000 & 19.609 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 18 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 262144] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x to produce Y[1024, 262144]
Matmul: 1024 x 262144 x 262144, Num KP Factors: 18
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^10)_NN_d
Tuning for shape 1024x262144*(2x2^11)_NN_d
Tuning for shape 1024x262144*(2x2^12)_NN_d
Tuning for shape 1024x262144*(2x2^13)_NN_d
Tuning for shape 1024x262144*(2x2^14)_NN_d
Tuning for shape 1024x262144*(2x2^15)_NN_d
Tuning for shape 1024x262144*(2x2^16)_NN_d
Tuning for shape 1024x262144*(2x2^17)_NN_d
Tuning for shape 1024x262144*(2x2^18)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^10)_NN_d
Tuning for shape 1024x262144*(2x2^11)_NN_d
Tuning for shape 1024x262144*(2x2^12)_NN_d
Tuning for shape 1024x262144*(2x2^13)_NN_d
Tuning for shape 1024x262144*(2x2^14)_NN_d
Tuning for shape 1024x262144*(2x2^15)_NN_d
Tuning for shape 1024x262144*(2x2^16)_NN_d
Tuning for shape 1024x262144*(2x2^17)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^10)_NN_d
Tuning for shape 1024x262144*(2x2^11)_NN_d
Tuning for shape 1024x262144*(2x2^12)_NN_d
Tuning for shape 1024x262144*(2x2^13)_NN_d
Tuning for shape 1024x262144*(2x2^14)_NN_d
Tuning for shape 1024x262144*(2x2^15)_NN_d
Tuning for shape 1024x262144*(2x2^16)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^10)_NN_d
Tuning for shape 1024x262144*(2x2^11)_NN_d
Tuning for shape 1024x262144*(2x2^12)_NN_d
Tuning for shape 1024x262144*(2x2^13)_NN_d
Tuning for shape 1024x262144*(2x2^14)_NN_d
Tuning for shape 1024x262144*(2x2^15)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^10)_NN_d
Tuning for shape 1024x262144*(2x2^11)_NN_d
Tuning for shape 1024x262144*(2x2^12)_NN_d
Tuning for shape 1024x262144*(2x2^13)_NN_d
Tuning for shape 1024x262144*(2x2^14)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^10)_NN_d
Tuning for shape 1024x262144*(2x2^11)_NN_d
Tuning for shape 1024x262144*(2x2^12)_NN_d
Tuning for shape 1024x262144*(2x2^13)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^10)_NN_d
Tuning for shape 1024x262144*(2x2^11)_NN_d
Tuning for shape 1024x262144*(2x2^12)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^10)_NN_d
Tuning for shape 1024x262144*(2x2^11)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^10)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^9)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^8)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^7)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^6)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^5)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^4)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^3)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Tuning for shape 1024x262144*(2x2^2)_NN_d
Tuning for shape 1024x262144*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^18  &  1.000 & 1.000 & 19.604 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 19 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 524288] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x F_18 [2, 2] x to produce Y[1, 524288]
Matmul: 1 x 524288 x 524288, Num KP Factors: 19
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^11)_NN_d
Tuning for shape 1x524288*(2x2^12)_NN_d
Tuning for shape 1x524288*(2x2^13)_NN_d
Tuning for shape 1x524288*(2x2^14)_NN_d
Tuning for shape 1x524288*(2x2^15)_NN_d
Tuning for shape 1x524288*(2x2^16)_NN_d
Tuning for shape 1x524288*(2x2^17)_NN_d
Tuning for shape 1x524288*(2x2^18)_NN_d
Tuning for shape 1x524288*(2x2^19)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^11)_NN_d
Tuning for shape 1x524288*(2x2^12)_NN_d
Tuning for shape 1x524288*(2x2^13)_NN_d
Tuning for shape 1x524288*(2x2^14)_NN_d
Tuning for shape 1x524288*(2x2^15)_NN_d
Tuning for shape 1x524288*(2x2^16)_NN_d
Tuning for shape 1x524288*(2x2^17)_NN_d
Tuning for shape 1x524288*(2x2^18)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^11)_NN_d
Tuning for shape 1x524288*(2x2^12)_NN_d
Tuning for shape 1x524288*(2x2^13)_NN_d
Tuning for shape 1x524288*(2x2^14)_NN_d
Tuning for shape 1x524288*(2x2^15)_NN_d
Tuning for shape 1x524288*(2x2^16)_NN_d
Tuning for shape 1x524288*(2x2^17)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^11)_NN_d
Tuning for shape 1x524288*(2x2^12)_NN_d
Tuning for shape 1x524288*(2x2^13)_NN_d
Tuning for shape 1x524288*(2x2^14)_NN_d
Tuning for shape 1x524288*(2x2^15)_NN_d
Tuning for shape 1x524288*(2x2^16)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^11)_NN_d
Tuning for shape 1x524288*(2x2^12)_NN_d
Tuning for shape 1x524288*(2x2^13)_NN_d
Tuning for shape 1x524288*(2x2^14)_NN_d
Tuning for shape 1x524288*(2x2^15)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^11)_NN_d
Tuning for shape 1x524288*(2x2^12)_NN_d
Tuning for shape 1x524288*(2x2^13)_NN_d
Tuning for shape 1x524288*(2x2^14)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^11)_NN_d
Tuning for shape 1x524288*(2x2^12)_NN_d
Tuning for shape 1x524288*(2x2^13)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^11)_NN_d
Tuning for shape 1x524288*(2x2^12)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^11)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^10)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^9)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^8)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^7)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^6)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^5)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^4)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^3)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Tuning for shape 1x524288*(2x2^2)_NN_d
Tuning for shape 1x524288*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x2^19  &  1.000 & 1.000 & 19.001 & 0.053
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 19 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 524288] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x F_18 [2, 2] x to produce Y[4, 524288]
Matmul: 4 x 524288 x 524288, Num KP Factors: 19
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^11)_NN_d
Tuning for shape 4x524288*(2x2^12)_NN_d
Tuning for shape 4x524288*(2x2^13)_NN_d
Tuning for shape 4x524288*(2x2^14)_NN_d
Tuning for shape 4x524288*(2x2^15)_NN_d
Tuning for shape 4x524288*(2x2^16)_NN_d
Tuning for shape 4x524288*(2x2^17)_NN_d
Tuning for shape 4x524288*(2x2^18)_NN_d
Tuning for shape 4x524288*(2x2^19)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^11)_NN_d
Tuning for shape 4x524288*(2x2^12)_NN_d
Tuning for shape 4x524288*(2x2^13)_NN_d
Tuning for shape 4x524288*(2x2^14)_NN_d
Tuning for shape 4x524288*(2x2^15)_NN_d
Tuning for shape 4x524288*(2x2^16)_NN_d
Tuning for shape 4x524288*(2x2^17)_NN_d
Tuning for shape 4x524288*(2x2^18)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^11)_NN_d
Tuning for shape 4x524288*(2x2^12)_NN_d
Tuning for shape 4x524288*(2x2^13)_NN_d
Tuning for shape 4x524288*(2x2^14)_NN_d
Tuning for shape 4x524288*(2x2^15)_NN_d
Tuning for shape 4x524288*(2x2^16)_NN_d
Tuning for shape 4x524288*(2x2^17)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^11)_NN_d
Tuning for shape 4x524288*(2x2^12)_NN_d
Tuning for shape 4x524288*(2x2^13)_NN_d
Tuning for shape 4x524288*(2x2^14)_NN_d
Tuning for shape 4x524288*(2x2^15)_NN_d
Tuning for shape 4x524288*(2x2^16)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^11)_NN_d
Tuning for shape 4x524288*(2x2^12)_NN_d
Tuning for shape 4x524288*(2x2^13)_NN_d
Tuning for shape 4x524288*(2x2^14)_NN_d
Tuning for shape 4x524288*(2x2^15)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^11)_NN_d
Tuning for shape 4x524288*(2x2^12)_NN_d
Tuning for shape 4x524288*(2x2^13)_NN_d
Tuning for shape 4x524288*(2x2^14)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^11)_NN_d
Tuning for shape 4x524288*(2x2^12)_NN_d
Tuning for shape 4x524288*(2x2^13)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^11)_NN_d
Tuning for shape 4x524288*(2x2^12)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^11)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^10)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^9)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^8)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^7)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^6)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^5)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^4)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^3)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Tuning for shape 4x524288*(2x2^2)_NN_d
Tuning for shape 4x524288*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x2^19  &  1.000 & 1.000 & 19.596 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 19 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 524288] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x F_18 [2, 2] x to produce Y[16, 524288]
Matmul: 16 x 524288 x 524288, Num KP Factors: 19
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^11)_NN_d
Tuning for shape 16x524288*(2x2^12)_NN_d
Tuning for shape 16x524288*(2x2^13)_NN_d
Tuning for shape 16x524288*(2x2^14)_NN_d
Tuning for shape 16x524288*(2x2^15)_NN_d
Tuning for shape 16x524288*(2x2^16)_NN_d
Tuning for shape 16x524288*(2x2^17)_NN_d
Tuning for shape 16x524288*(2x2^18)_NN_d
Tuning for shape 16x524288*(2x2^19)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^11)_NN_d
Tuning for shape 16x524288*(2x2^12)_NN_d
Tuning for shape 16x524288*(2x2^13)_NN_d
Tuning for shape 16x524288*(2x2^14)_NN_d
Tuning for shape 16x524288*(2x2^15)_NN_d
Tuning for shape 16x524288*(2x2^16)_NN_d
Tuning for shape 16x524288*(2x2^17)_NN_d
Tuning for shape 16x524288*(2x2^18)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^11)_NN_d
Tuning for shape 16x524288*(2x2^12)_NN_d
Tuning for shape 16x524288*(2x2^13)_NN_d
Tuning for shape 16x524288*(2x2^14)_NN_d
Tuning for shape 16x524288*(2x2^15)_NN_d
Tuning for shape 16x524288*(2x2^16)_NN_d
Tuning for shape 16x524288*(2x2^17)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^11)_NN_d
Tuning for shape 16x524288*(2x2^12)_NN_d
Tuning for shape 16x524288*(2x2^13)_NN_d
Tuning for shape 16x524288*(2x2^14)_NN_d
Tuning for shape 16x524288*(2x2^15)_NN_d
Tuning for shape 16x524288*(2x2^16)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^11)_NN_d
Tuning for shape 16x524288*(2x2^12)_NN_d
Tuning for shape 16x524288*(2x2^13)_NN_d
Tuning for shape 16x524288*(2x2^14)_NN_d
Tuning for shape 16x524288*(2x2^15)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^11)_NN_d
Tuning for shape 16x524288*(2x2^12)_NN_d
Tuning for shape 16x524288*(2x2^13)_NN_d
Tuning for shape 16x524288*(2x2^14)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^11)_NN_d
Tuning for shape 16x524288*(2x2^12)_NN_d
Tuning for shape 16x524288*(2x2^13)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^11)_NN_d
Tuning for shape 16x524288*(2x2^12)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^11)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^10)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^9)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^8)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^7)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^6)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^5)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^4)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^3)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Tuning for shape 16x524288*(2x2^2)_NN_d
Tuning for shape 16x524288*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x2^19  &  1.000 & 1.000 & 20.063 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 19 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 524288] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x F_18 [2, 2] x to produce Y[64, 524288]
Matmul: 64 x 524288 x 524288, Num KP Factors: 19
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^11)_NN_d
Tuning for shape 64x524288*(2x2^12)_NN_d
Tuning for shape 64x524288*(2x2^13)_NN_d
Tuning for shape 64x524288*(2x2^14)_NN_d
Tuning for shape 64x524288*(2x2^15)_NN_d
Tuning for shape 64x524288*(2x2^16)_NN_d
Tuning for shape 64x524288*(2x2^17)_NN_d
Tuning for shape 64x524288*(2x2^18)_NN_d
Tuning for shape 64x524288*(2x2^19)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^11)_NN_d
Tuning for shape 64x524288*(2x2^12)_NN_d
Tuning for shape 64x524288*(2x2^13)_NN_d
Tuning for shape 64x524288*(2x2^14)_NN_d
Tuning for shape 64x524288*(2x2^15)_NN_d
Tuning for shape 64x524288*(2x2^16)_NN_d
Tuning for shape 64x524288*(2x2^17)_NN_d
Tuning for shape 64x524288*(2x2^18)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^11)_NN_d
Tuning for shape 64x524288*(2x2^12)_NN_d
Tuning for shape 64x524288*(2x2^13)_NN_d
Tuning for shape 64x524288*(2x2^14)_NN_d
Tuning for shape 64x524288*(2x2^15)_NN_d
Tuning for shape 64x524288*(2x2^16)_NN_d
Tuning for shape 64x524288*(2x2^17)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^11)_NN_d
Tuning for shape 64x524288*(2x2^12)_NN_d
Tuning for shape 64x524288*(2x2^13)_NN_d
Tuning for shape 64x524288*(2x2^14)_NN_d
Tuning for shape 64x524288*(2x2^15)_NN_d
Tuning for shape 64x524288*(2x2^16)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^11)_NN_d
Tuning for shape 64x524288*(2x2^12)_NN_d
Tuning for shape 64x524288*(2x2^13)_NN_d
Tuning for shape 64x524288*(2x2^14)_NN_d
Tuning for shape 64x524288*(2x2^15)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^11)_NN_d
Tuning for shape 64x524288*(2x2^12)_NN_d
Tuning for shape 64x524288*(2x2^13)_NN_d
Tuning for shape 64x524288*(2x2^14)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^11)_NN_d
Tuning for shape 64x524288*(2x2^12)_NN_d
Tuning for shape 64x524288*(2x2^13)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^11)_NN_d
Tuning for shape 64x524288*(2x2^12)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^11)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^10)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^9)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^8)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^7)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^6)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^5)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^4)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^3)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Tuning for shape 64x524288*(2x2^2)_NN_d
Tuning for shape 64x524288*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x2^19  &  1.000 & 1.000 & 20.091 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 19 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 524288] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x F_18 [2, 2] x to produce Y[256, 524288]
Matmul: 256 x 524288 x 524288, Num KP Factors: 19
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^11)_NN_d
Tuning for shape 256x524288*(2x2^12)_NN_d
Tuning for shape 256x524288*(2x2^13)_NN_d
Tuning for shape 256x524288*(2x2^14)_NN_d
Tuning for shape 256x524288*(2x2^15)_NN_d
Tuning for shape 256x524288*(2x2^16)_NN_d
Tuning for shape 256x524288*(2x2^17)_NN_d
Tuning for shape 256x524288*(2x2^18)_NN_d
Tuning for shape 256x524288*(2x2^19)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^11)_NN_d
Tuning for shape 256x524288*(2x2^12)_NN_d
Tuning for shape 256x524288*(2x2^13)_NN_d
Tuning for shape 256x524288*(2x2^14)_NN_d
Tuning for shape 256x524288*(2x2^15)_NN_d
Tuning for shape 256x524288*(2x2^16)_NN_d
Tuning for shape 256x524288*(2x2^17)_NN_d
Tuning for shape 256x524288*(2x2^18)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^11)_NN_d
Tuning for shape 256x524288*(2x2^12)_NN_d
Tuning for shape 256x524288*(2x2^13)_NN_d
Tuning for shape 256x524288*(2x2^14)_NN_d
Tuning for shape 256x524288*(2x2^15)_NN_d
Tuning for shape 256x524288*(2x2^16)_NN_d
Tuning for shape 256x524288*(2x2^17)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^11)_NN_d
Tuning for shape 256x524288*(2x2^12)_NN_d
Tuning for shape 256x524288*(2x2^13)_NN_d
Tuning for shape 256x524288*(2x2^14)_NN_d
Tuning for shape 256x524288*(2x2^15)_NN_d
Tuning for shape 256x524288*(2x2^16)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^11)_NN_d
Tuning for shape 256x524288*(2x2^12)_NN_d
Tuning for shape 256x524288*(2x2^13)_NN_d
Tuning for shape 256x524288*(2x2^14)_NN_d
Tuning for shape 256x524288*(2x2^15)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^11)_NN_d
Tuning for shape 256x524288*(2x2^12)_NN_d
Tuning for shape 256x524288*(2x2^13)_NN_d
Tuning for shape 256x524288*(2x2^14)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^11)_NN_d
Tuning for shape 256x524288*(2x2^12)_NN_d
Tuning for shape 256x524288*(2x2^13)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^11)_NN_d
Tuning for shape 256x524288*(2x2^12)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^11)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^10)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^9)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^8)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^7)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^6)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^5)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^4)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^3)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Tuning for shape 256x524288*(2x2^2)_NN_d
Tuning for shape 256x524288*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x2^19  &  1.000 & 1.000 & 19.691 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 19 -p 2 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 524288] with F_0 [2, 2] x F_1 [2, 2] x F_2 [2, 2] x F_3 [2, 2] x F_4 [2, 2] x F_5 [2, 2] x F_6 [2, 2] x F_7 [2, 2] x F_8 [2, 2] x F_9 [2, 2] x F_10 [2, 2] x F_11 [2, 2] x F_12 [2, 2] x F_13 [2, 2] x F_14 [2, 2] x F_15 [2, 2] x F_16 [2, 2] x F_17 [2, 2] x F_18 [2, 2] x to produce Y[1024, 524288]
Matmul: 1024 x 524288 x 524288, Num KP Factors: 19
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^11)_NN_d
Tuning for shape 1024x524288*(2x2^12)_NN_d
Tuning for shape 1024x524288*(2x2^13)_NN_d
Tuning for shape 1024x524288*(2x2^14)_NN_d
Tuning for shape 1024x524288*(2x2^15)_NN_d
Tuning for shape 1024x524288*(2x2^16)_NN_d
Tuning for shape 1024x524288*(2x2^17)_NN_d
Tuning for shape 1024x524288*(2x2^18)_NN_d
Tuning for shape 1024x524288*(2x2^19)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^11)_NN_d
Tuning for shape 1024x524288*(2x2^12)_NN_d
Tuning for shape 1024x524288*(2x2^13)_NN_d
Tuning for shape 1024x524288*(2x2^14)_NN_d
Tuning for shape 1024x524288*(2x2^15)_NN_d
Tuning for shape 1024x524288*(2x2^16)_NN_d
Tuning for shape 1024x524288*(2x2^17)_NN_d
Tuning for shape 1024x524288*(2x2^18)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^11)_NN_d
Tuning for shape 1024x524288*(2x2^12)_NN_d
Tuning for shape 1024x524288*(2x2^13)_NN_d
Tuning for shape 1024x524288*(2x2^14)_NN_d
Tuning for shape 1024x524288*(2x2^15)_NN_d
Tuning for shape 1024x524288*(2x2^16)_NN_d
Tuning for shape 1024x524288*(2x2^17)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^11)_NN_d
Tuning for shape 1024x524288*(2x2^12)_NN_d
Tuning for shape 1024x524288*(2x2^13)_NN_d
Tuning for shape 1024x524288*(2x2^14)_NN_d
Tuning for shape 1024x524288*(2x2^15)_NN_d
Tuning for shape 1024x524288*(2x2^16)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^11)_NN_d
Tuning for shape 1024x524288*(2x2^12)_NN_d
Tuning for shape 1024x524288*(2x2^13)_NN_d
Tuning for shape 1024x524288*(2x2^14)_NN_d
Tuning for shape 1024x524288*(2x2^15)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^11)_NN_d
Tuning for shape 1024x524288*(2x2^12)_NN_d
Tuning for shape 1024x524288*(2x2^13)_NN_d
Tuning for shape 1024x524288*(2x2^14)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^11)_NN_d
Tuning for shape 1024x524288*(2x2^12)_NN_d
Tuning for shape 1024x524288*(2x2^13)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^11)_NN_d
Tuning for shape 1024x524288*(2x2^12)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^11)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^10)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^9)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^8)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^7)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^6)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^5)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^4)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^3)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Tuning for shape 1024x524288*(2x2^2)_NN_d
Tuning for shape 1024x524288*(2x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x2^19  &  1.000 & 1.000 & -1.000 & -1.000
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2] with F_0 [2, 4] x to produce Y[1, 4]
Matmul: 1 x 4 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^1  &  1.000 & 1.000 & 0.000 & 15934.557
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2] with F_0 [2, 4] x to produce Y[4, 4]
Matmul: 4 x 4 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^1  &  1.000 & 1.000 & 0.000 & 4968.792
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2] with F_0 [2, 4] x to produce Y[16, 4]
Matmul: 16 x 4 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^1  &  1.000 & 1.000 & 0.001 & 1194.747
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2] with F_0 [2, 4] x to produce Y[64, 4]
Matmul: 64 x 4 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^1  &  1.000 & 1.000 & 0.003 & 297.930
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2] with F_0 [2, 4] x to produce Y[256, 4]
Matmul: 256 x 4 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^1  &  1.000 & 1.000 & 0.013 & 75.414
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 2] with F_0 [2, 4] x to produce Y[1024, 4]
Matmul: 1024 x 4 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x4^1  &  1.000 & 1.000 & 0.053 & 19.017
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [2, 4] x F_1 [2, 4] x to produce Y[1, 16]
Matmul: 1 x 16 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(2x4^1)_NN_d
Tuning for shape 1x4*(2x4^2)_NN_d
Tuning for shape 1x4*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^2  &  1.000 & 1.000 & 0.000 & 4137.680
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [2, 4] x F_1 [2, 4] x to produce Y[4, 16]
Matmul: 4 x 16 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(2x4^1)_NN_d
Tuning for shape 4x4*(2x4^2)_NN_d
Tuning for shape 4x4*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^2  &  1.000 & 1.000 & 0.001 & 1183.525
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [2, 4] x F_1 [2, 4] x to produce Y[16, 16]
Matmul: 16 x 16 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(2x4^1)_NN_d
Tuning for shape 16x4*(2x4^2)_NN_d
Tuning for shape 16x4*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^2  &  1.000 & 1.000 & 0.003 & 300.926
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [2, 4] x F_1 [2, 4] x to produce Y[64, 16]
Matmul: 64 x 16 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(2x4^1)_NN_d
Tuning for shape 64x4*(2x4^2)_NN_d
Tuning for shape 64x4*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^2  &  1.000 & 1.000 & 0.013 & 74.164
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [2, 4] x F_1 [2, 4] x to produce Y[256, 16]
Matmul: 256 x 16 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(2x4^1)_NN_d
Tuning for shape 256x4*(2x4^2)_NN_d
Tuning for shape 256x4*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^2  &  1.000 & 1.000 & 0.056 & 18.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [2, 4] x F_1 [2, 4] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(2x4^1)_NN_d
Tuning for shape 1024x4*(2x4^2)_NN_d
Tuning for shape 1024x4*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x4^2  &  1.000 & 1.000 & 0.214 & 4.680
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x to produce Y[1, 64]
Matmul: 1 x 64 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32*(2x4^1)_NN_d
Tuning for shape 1x16*(2x4^2)_NN_d
Tuning for shape 1x8*(2x4^3)_NN_d
Tuning for shape 1x16*(2x4^1)_NN_d
Tuning for shape 1x8*(2x4^2)_NN_d
Tuning for shape 1x8*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^3  &  1.000 & 1.000 & 0.001 & 1189.725
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x to produce Y[4, 64]
Matmul: 4 x 64 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32*(2x4^1)_NN_d
Tuning for shape 4x16*(2x4^2)_NN_d
Tuning for shape 4x8*(2x4^3)_NN_d
Tuning for shape 4x16*(2x4^1)_NN_d
Tuning for shape 4x8*(2x4^2)_NN_d
Tuning for shape 4x8*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^3  &  1.000 & 1.000 & 0.003 & 314.115
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x to produce Y[16, 64]
Matmul: 16 x 64 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32*(2x4^1)_NN_d
Tuning for shape 16x16*(2x4^2)_NN_d
Tuning for shape 16x8*(2x4^3)_NN_d
Tuning for shape 16x16*(2x4^1)_NN_d
Tuning for shape 16x8*(2x4^2)_NN_d
Tuning for shape 16x8*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^3  &  1.000 & 1.000 & 0.013 & 77.747
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x to produce Y[64, 64]
Matmul: 64 x 64 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32*(2x4^1)_NN_d
Tuning for shape 64x16*(2x4^2)_NN_d
Tuning for shape 64x8*(2x4^3)_NN_d
Tuning for shape 64x16*(2x4^1)_NN_d
Tuning for shape 64x8*(2x4^2)_NN_d
Tuning for shape 64x8*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^3  &  1.000 & 1.000 & 0.052 & 19.309
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x to produce Y[256, 64]
Matmul: 256 x 64 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32*(2x4^1)_NN_d
Tuning for shape 256x16*(2x4^2)_NN_d
Tuning for shape 256x8*(2x4^3)_NN_d
Tuning for shape 256x16*(2x4^1)_NN_d
Tuning for shape 256x8*(2x4^2)_NN_d
Tuning for shape 256x8*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^3  &  1.000 & 1.000 & 0.206 & 4.864
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32*(2x4^1)_NN_d
Tuning for shape 1024x16*(2x4^2)_NN_d
Tuning for shape 1024x8*(2x4^3)_NN_d
Tuning for shape 1024x16*(2x4^1)_NN_d
Tuning for shape 1024x8*(2x4^2)_NN_d
Tuning for shape 1024x8*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x4^3  &  1.000 & 1.000 & 0.844 & 1.185
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x to produce Y[1, 256]
Matmul: 1 x 256 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(2x4^1)_NN_d
Tuning for shape 1x64*(2x4^2)_NN_d
Tuning for shape 1x32*(2x4^3)_NN_d
Tuning for shape 1x16*(2x4^4)_NN_d
Tuning for shape 1x64*(2x4^1)_NN_d
Tuning for shape 1x32*(2x4^2)_NN_d
Tuning for shape 1x16*(2x4^3)_NN_d
Tuning for shape 1x32*(2x4^1)_NN_d
Tuning for shape 1x16*(2x4^2)_NN_d
Tuning for shape 1x16*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^4  &  1.000 & 1.000 & 0.003 & 320.412
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x to produce Y[4, 256]
Matmul: 4 x 256 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(2x4^1)_NN_d
Tuning for shape 4x64*(2x4^2)_NN_d
Tuning for shape 4x32*(2x4^3)_NN_d
Tuning for shape 4x16*(2x4^4)_NN_d
Tuning for shape 4x64*(2x4^1)_NN_d
Tuning for shape 4x32*(2x4^2)_NN_d
Tuning for shape 4x16*(2x4^3)_NN_d
Tuning for shape 4x32*(2x4^1)_NN_d
Tuning for shape 4x16*(2x4^2)_NN_d
Tuning for shape 4x16*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^4  &  1.000 & 1.000 & 0.011 & 87.332
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x to produce Y[16, 256]
Matmul: 16 x 256 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(2x4^1)_NN_d
Tuning for shape 16x64*(2x4^2)_NN_d
Tuning for shape 16x32*(2x4^3)_NN_d
Tuning for shape 16x16*(2x4^4)_NN_d
Tuning for shape 16x64*(2x4^1)_NN_d
Tuning for shape 16x32*(2x4^2)_NN_d
Tuning for shape 16x16*(2x4^3)_NN_d
Tuning for shape 16x32*(2x4^1)_NN_d
Tuning for shape 16x16*(2x4^2)_NN_d
Tuning for shape 16x16*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^4  &  1.000 & 1.000 & 0.045 & 22.358
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x to produce Y[64, 256]
Matmul: 64 x 256 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(2x4^1)_NN_d
Tuning for shape 64x64*(2x4^2)_NN_d
Tuning for shape 64x32*(2x4^3)_NN_d
Tuning for shape 64x16*(2x4^4)_NN_d
Tuning for shape 64x64*(2x4^1)_NN_d
Tuning for shape 64x32*(2x4^2)_NN_d
Tuning for shape 64x16*(2x4^3)_NN_d
Tuning for shape 64x32*(2x4^1)_NN_d
Tuning for shape 64x16*(2x4^2)_NN_d
Tuning for shape 64x16*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^4  &  1.000 & 1.000 & 0.186 & 5.377
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x to produce Y[256, 256]
Matmul: 256 x 256 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(2x4^1)_NN_d
Tuning for shape 256x64*(2x4^2)_NN_d
Tuning for shape 256x32*(2x4^3)_NN_d
Tuning for shape 256x16*(2x4^4)_NN_d
Tuning for shape 256x64*(2x4^1)_NN_d
Tuning for shape 256x32*(2x4^2)_NN_d
Tuning for shape 256x16*(2x4^3)_NN_d
Tuning for shape 256x32*(2x4^1)_NN_d
Tuning for shape 256x16*(2x4^2)_NN_d
Tuning for shape 256x16*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^4  &  1.000 & 1.000 & 0.756 & 1.323
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x to produce Y[1024, 256]
Matmul: 1024 x 256 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(2x4^1)_NN_d
Tuning for shape 1024x64*(2x4^2)_NN_d
Tuning for shape 1024x32*(2x4^3)_NN_d
Tuning for shape 1024x16*(2x4^4)_NN_d
Tuning for shape 1024x64*(2x4^1)_NN_d
Tuning for shape 1024x32*(2x4^2)_NN_d
Tuning for shape 1024x16*(2x4^3)_NN_d
Tuning for shape 1024x32*(2x4^1)_NN_d
Tuning for shape 1024x16*(2x4^2)_NN_d
Tuning for shape 1024x16*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x4^4  &  1.000 & 1.000 & 2.987 & 0.335
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x to produce Y[1, 1024]
Matmul: 1 x 1024 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(2x4^1)_NN_d
Tuning for shape 1x256*(2x4^2)_NN_d
Tuning for shape 1x128*(2x4^3)_NN_d
Tuning for shape 1x64*(2x4^4)_NN_d
Tuning for shape 1x32*(2x4^5)_NN_d
Tuning for shape 1x256*(2x4^1)_NN_d
Tuning for shape 1x128*(2x4^2)_NN_d
Tuning for shape 1x64*(2x4^3)_NN_d
Tuning for shape 1x32*(2x4^4)_NN_d
Tuning for shape 1x128*(2x4^1)_NN_d
Tuning for shape 1x64*(2x4^2)_NN_d
Tuning for shape 1x32*(2x4^3)_NN_d
Tuning for shape 1x64*(2x4^1)_NN_d
Tuning for shape 1x32*(2x4^2)_NN_d
Tuning for shape 1x32*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^5  &  1.000 & 1.000 & 0.011 & 91.833
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x to produce Y[4, 1024]
Matmul: 4 x 1024 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(2x4^1)_NN_d
Tuning for shape 4x256*(2x4^2)_NN_d
Tuning for shape 4x128*(2x4^3)_NN_d
Tuning for shape 4x64*(2x4^4)_NN_d
Tuning for shape 4x32*(2x4^5)_NN_d
Tuning for shape 4x256*(2x4^1)_NN_d
Tuning for shape 4x128*(2x4^2)_NN_d
Tuning for shape 4x64*(2x4^3)_NN_d
Tuning for shape 4x32*(2x4^4)_NN_d
Tuning for shape 4x128*(2x4^1)_NN_d
Tuning for shape 4x64*(2x4^2)_NN_d
Tuning for shape 4x32*(2x4^3)_NN_d
Tuning for shape 4x64*(2x4^1)_NN_d
Tuning for shape 4x32*(2x4^2)_NN_d
Tuning for shape 4x32*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^5  &  1.000 & 1.000 & 0.042 & 23.732
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x to produce Y[16, 1024]
Matmul: 16 x 1024 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(2x4^1)_NN_d
Tuning for shape 16x256*(2x4^2)_NN_d
Tuning for shape 16x128*(2x4^3)_NN_d
Tuning for shape 16x64*(2x4^4)_NN_d
Tuning for shape 16x32*(2x4^5)_NN_d
Tuning for shape 16x256*(2x4^1)_NN_d
Tuning for shape 16x128*(2x4^2)_NN_d
Tuning for shape 16x64*(2x4^3)_NN_d
Tuning for shape 16x32*(2x4^4)_NN_d
Tuning for shape 16x128*(2x4^1)_NN_d
Tuning for shape 16x64*(2x4^2)_NN_d
Tuning for shape 16x32*(2x4^3)_NN_d
Tuning for shape 16x64*(2x4^1)_NN_d
Tuning for shape 16x32*(2x4^2)_NN_d
Tuning for shape 16x32*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^5  &  1.000 & 1.000 & 0.168 & 5.957
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 32] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x to produce Y[64, 1024]
Matmul: 64 x 1024 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x512*(2x4^1)_NN_d
Tuning for shape 64x256*(2x4^2)_NN_d
Tuning for shape 64x128*(2x4^3)_NN_d
Tuning for shape 64x64*(2x4^4)_NN_d
Tuning for shape 64x32*(2x4^5)_NN_d
Tuning for shape 64x256*(2x4^1)_NN_d
Tuning for shape 64x128*(2x4^2)_NN_d
Tuning for shape 64x64*(2x4^3)_NN_d
Tuning for shape 64x32*(2x4^4)_NN_d
Tuning for shape 64x128*(2x4^1)_NN_d
Tuning for shape 64x64*(2x4^2)_NN_d
Tuning for shape 64x32*(2x4^3)_NN_d
Tuning for shape 64x64*(2x4^1)_NN_d
Tuning for shape 64x32*(2x4^2)_NN_d
Tuning for shape 64x32*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^5  &  1.000 & 1.000 & 0.676 & 1.479
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 32] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x to produce Y[256, 1024]
Matmul: 256 x 1024 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x512*(2x4^1)_NN_d
Tuning for shape 256x256*(2x4^2)_NN_d
Tuning for shape 256x128*(2x4^3)_NN_d
Tuning for shape 256x64*(2x4^4)_NN_d
Tuning for shape 256x32*(2x4^5)_NN_d
Tuning for shape 256x256*(2x4^1)_NN_d
Tuning for shape 256x128*(2x4^2)_NN_d
Tuning for shape 256x64*(2x4^3)_NN_d
Tuning for shape 256x32*(2x4^4)_NN_d
Tuning for shape 256x128*(2x4^1)_NN_d
Tuning for shape 256x64*(2x4^2)_NN_d
Tuning for shape 256x32*(2x4^3)_NN_d
Tuning for shape 256x64*(2x4^1)_NN_d
Tuning for shape 256x32*(2x4^2)_NN_d
Tuning for shape 256x32*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^5  &  1.000 & 1.000 & 2.617 & 0.382
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 5 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 32] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x to produce Y[1024, 1024]
Matmul: 1024 x 1024 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x512*(2x4^1)_NN_d
Tuning for shape 1024x256*(2x4^2)_NN_d
Tuning for shape 1024x128*(2x4^3)_NN_d
Tuning for shape 1024x64*(2x4^4)_NN_d
Tuning for shape 1024x32*(2x4^5)_NN_d
Tuning for shape 1024x256*(2x4^1)_NN_d
Tuning for shape 1024x128*(2x4^2)_NN_d
Tuning for shape 1024x64*(2x4^3)_NN_d
Tuning for shape 1024x32*(2x4^4)_NN_d
Tuning for shape 1024x128*(2x4^1)_NN_d
Tuning for shape 1024x64*(2x4^2)_NN_d
Tuning for shape 1024x32*(2x4^3)_NN_d
Tuning for shape 1024x64*(2x4^1)_NN_d
Tuning for shape 1024x32*(2x4^2)_NN_d
Tuning for shape 1024x32*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x4^5  &  1.000 & 1.000 & 10.755 & 0.093
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2048*(2x4^1)_NN_d
Tuning for shape 1x1024*(2x4^2)_NN_d
Tuning for shape 1x512*(2x4^3)_NN_d
Tuning for shape 1x256*(2x4^4)_NN_d
Tuning for shape 1x128*(2x4^5)_NN_d
Tuning for shape 1x64*(2x4^6)_NN_d
Tuning for shape 1x1024*(2x4^1)_NN_d
Tuning for shape 1x512*(2x4^2)_NN_d
Tuning for shape 1x256*(2x4^3)_NN_d
Tuning for shape 1x128*(2x4^4)_NN_d
Tuning for shape 1x64*(2x4^5)_NN_d
Tuning for shape 1x512*(2x4^1)_NN_d
Tuning for shape 1x256*(2x4^2)_NN_d
Tuning for shape 1x128*(2x4^3)_NN_d
Tuning for shape 1x64*(2x4^4)_NN_d
Tuning for shape 1x256*(2x4^1)_NN_d
Tuning for shape 1x128*(2x4^2)_NN_d
Tuning for shape 1x64*(2x4^3)_NN_d
Tuning for shape 1x128*(2x4^1)_NN_d
Tuning for shape 1x64*(2x4^2)_NN_d
Tuning for shape 1x64*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^6  &  1.000 & 1.000 & 0.040 & 24.764
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2048*(2x4^1)_NN_d
Tuning for shape 4x1024*(2x4^2)_NN_d
Tuning for shape 4x512*(2x4^3)_NN_d
Tuning for shape 4x256*(2x4^4)_NN_d
Tuning for shape 4x128*(2x4^5)_NN_d
Tuning for shape 4x64*(2x4^6)_NN_d
Tuning for shape 4x1024*(2x4^1)_NN_d
Tuning for shape 4x512*(2x4^2)_NN_d
Tuning for shape 4x256*(2x4^3)_NN_d
Tuning for shape 4x128*(2x4^4)_NN_d
Tuning for shape 4x64*(2x4^5)_NN_d
Tuning for shape 4x512*(2x4^1)_NN_d
Tuning for shape 4x256*(2x4^2)_NN_d
Tuning for shape 4x128*(2x4^3)_NN_d
Tuning for shape 4x64*(2x4^4)_NN_d
Tuning for shape 4x256*(2x4^1)_NN_d
Tuning for shape 4x128*(2x4^2)_NN_d
Tuning for shape 4x64*(2x4^3)_NN_d
Tuning for shape 4x128*(2x4^1)_NN_d
Tuning for shape 4x64*(2x4^2)_NN_d
Tuning for shape 4x64*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^6  &  1.000 & 1.000 & 0.142 & 7.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2048*(2x4^1)_NN_d
Tuning for shape 16x1024*(2x4^2)_NN_d
Tuning for shape 16x512*(2x4^3)_NN_d
Tuning for shape 16x256*(2x4^4)_NN_d
Tuning for shape 16x128*(2x4^5)_NN_d
Tuning for shape 16x64*(2x4^6)_NN_d
Tuning for shape 16x1024*(2x4^1)_NN_d
Tuning for shape 16x512*(2x4^2)_NN_d
Tuning for shape 16x256*(2x4^3)_NN_d
Tuning for shape 16x128*(2x4^4)_NN_d
Tuning for shape 16x64*(2x4^5)_NN_d
Tuning for shape 16x512*(2x4^1)_NN_d
Tuning for shape 16x256*(2x4^2)_NN_d
Tuning for shape 16x128*(2x4^3)_NN_d
Tuning for shape 16x64*(2x4^4)_NN_d
Tuning for shape 16x256*(2x4^1)_NN_d
Tuning for shape 16x128*(2x4^2)_NN_d
Tuning for shape 16x64*(2x4^3)_NN_d
Tuning for shape 16x128*(2x4^1)_NN_d
Tuning for shape 16x64*(2x4^2)_NN_d
Tuning for shape 16x64*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^6  &  1.000 & 1.000 & 0.611 & 1.636
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 6 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2048*(2x4^1)_NN_d
Tuning for shape 64x1024*(2x4^2)_NN_d
Tuning for shape 64x512*(2x4^3)_NN_d
Tuning for shape 64x256*(2x4^4)_NN_d
Tuning for shape 64x128*(2x4^5)_NN_d
Tuning for shape 64x64*(2x4^6)_NN_d
Tuning for shape 64x1024*(2x4^1)_NN_d
Tuning for shape 64x512*(2x4^2)_NN_d
Tuning for shape 64x256*(2x4^3)_NN_d
Tuning for shape 64x128*(2x4^4)_NN_d
Tuning for shape 64x64*(2x4^5)_NN_d
Tuning for shape 64x512*(2x4^1)_NN_d
Tuning for shape 64x256*(2x4^2)_NN_d
Tuning for shape 64x128*(2x4^3)_NN_d
Tuning for shape 64x64*(2x4^4)_NN_d
Tuning for shape 64x256*(2x4^1)_NN_d
Tuning for shape 64x128*(2x4^2)_NN_d
Tuning for shape 64x64*(2x4^3)_NN_d
Tuning for shape 64x128*(2x4^1)_NN_d
Tuning for shape 64x64*(2x4^2)_NN_d
Tuning for shape 64x64*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^6  &  1.000 & 1.000 & 2.172 & 0.460
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 6 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2048*(2x4^1)_NN_d
Tuning for shape 256x1024*(2x4^2)_NN_d
Tuning for shape 256x512*(2x4^3)_NN_d
Tuning for shape 256x256*(2x4^4)_NN_d
Tuning for shape 256x128*(2x4^5)_NN_d
Tuning for shape 256x64*(2x4^6)_NN_d
Tuning for shape 256x1024*(2x4^1)_NN_d
Tuning for shape 256x512*(2x4^2)_NN_d
Tuning for shape 256x256*(2x4^3)_NN_d
Tuning for shape 256x128*(2x4^4)_NN_d
Tuning for shape 256x64*(2x4^5)_NN_d
Tuning for shape 256x512*(2x4^1)_NN_d
Tuning for shape 256x256*(2x4^2)_NN_d
Tuning for shape 256x128*(2x4^3)_NN_d
Tuning for shape 256x64*(2x4^4)_NN_d
Tuning for shape 256x256*(2x4^1)_NN_d
Tuning for shape 256x128*(2x4^2)_NN_d
Tuning for shape 256x64*(2x4^3)_NN_d
Tuning for shape 256x128*(2x4^1)_NN_d
Tuning for shape 256x64*(2x4^2)_NN_d
Tuning for shape 256x64*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^6  &  1.000 & 1.000 & 9.928 & 0.101
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 6 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2048*(2x4^1)_NN_d
Tuning for shape 1024x1024*(2x4^2)_NN_d
Tuning for shape 1024x512*(2x4^3)_NN_d
Tuning for shape 1024x256*(2x4^4)_NN_d
Tuning for shape 1024x128*(2x4^5)_NN_d
Tuning for shape 1024x64*(2x4^6)_NN_d
Tuning for shape 1024x1024*(2x4^1)_NN_d
Tuning for shape 1024x512*(2x4^2)_NN_d
Tuning for shape 1024x256*(2x4^3)_NN_d
Tuning for shape 1024x128*(2x4^4)_NN_d
Tuning for shape 1024x64*(2x4^5)_NN_d
Tuning for shape 1024x512*(2x4^1)_NN_d
Tuning for shape 1024x256*(2x4^2)_NN_d
Tuning for shape 1024x128*(2x4^3)_NN_d
Tuning for shape 1024x64*(2x4^4)_NN_d
Tuning for shape 1024x256*(2x4^1)_NN_d
Tuning for shape 1024x128*(2x4^2)_NN_d
Tuning for shape 1024x64*(2x4^3)_NN_d
Tuning for shape 1024x128*(2x4^1)_NN_d
Tuning for shape 1024x64*(2x4^2)_NN_d
Tuning for shape 1024x64*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x4^6  &  1.000 & 1.000 & 31.123 & 0.032
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 128] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x to produce Y[1, 16384]
Matmul: 1 x 16384 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8192*(2x4^1)_NN_d
Tuning for shape 1x4096*(2x4^2)_NN_d
Tuning for shape 1x2048*(2x4^3)_NN_d
Tuning for shape 1x1024*(2x4^4)_NN_d
Tuning for shape 1x512*(2x4^5)_NN_d
Tuning for shape 1x256*(2x4^6)_NN_d
Tuning for shape 1x128*(2x4^7)_NN_d
Tuning for shape 1x4096*(2x4^1)_NN_d
Tuning for shape 1x2048*(2x4^2)_NN_d
Tuning for shape 1x1024*(2x4^3)_NN_d
Tuning for shape 1x512*(2x4^4)_NN_d
Tuning for shape 1x256*(2x4^5)_NN_d
Tuning for shape 1x128*(2x4^6)_NN_d
Tuning for shape 1x2048*(2x4^1)_NN_d
Tuning for shape 1x1024*(2x4^2)_NN_d
Tuning for shape 1x512*(2x4^3)_NN_d
Tuning for shape 1x256*(2x4^4)_NN_d
Tuning for shape 1x128*(2x4^5)_NN_d
Tuning for shape 1x1024*(2x4^1)_NN_d
Tuning for shape 1x512*(2x4^2)_NN_d
Tuning for shape 1x256*(2x4^3)_NN_d
Tuning for shape 1x128*(2x4^4)_NN_d
Tuning for shape 1x512*(2x4^1)_NN_d
Tuning for shape 1x256*(2x4^2)_NN_d
Tuning for shape 1x128*(2x4^3)_NN_d
Tuning for shape 1x256*(2x4^1)_NN_d
Tuning for shape 1x128*(2x4^2)_NN_d
Tuning for shape 1x128*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^7  &  1.000 & 1.000 & 0.143 & 6.992
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 7 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 128] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x to produce Y[4, 16384]
Matmul: 4 x 16384 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8192*(2x4^1)_NN_d
Tuning for shape 4x4096*(2x4^2)_NN_d
Tuning for shape 4x2048*(2x4^3)_NN_d
Tuning for shape 4x1024*(2x4^4)_NN_d
Tuning for shape 4x512*(2x4^5)_NN_d
Tuning for shape 4x256*(2x4^6)_NN_d
Tuning for shape 4x128*(2x4^7)_NN_d
Tuning for shape 4x4096*(2x4^1)_NN_d
Tuning for shape 4x2048*(2x4^2)_NN_d
Tuning for shape 4x1024*(2x4^3)_NN_d
Tuning for shape 4x512*(2x4^4)_NN_d
Tuning for shape 4x256*(2x4^5)_NN_d
Tuning for shape 4x128*(2x4^6)_NN_d
Tuning for shape 4x2048*(2x4^1)_NN_d
Tuning for shape 4x1024*(2x4^2)_NN_d
Tuning for shape 4x512*(2x4^3)_NN_d
Tuning for shape 4x256*(2x4^4)_NN_d
Tuning for shape 4x128*(2x4^5)_NN_d
Tuning for shape 4x1024*(2x4^1)_NN_d
Tuning for shape 4x512*(2x4^2)_NN_d
Tuning for shape 4x256*(2x4^3)_NN_d
Tuning for shape 4x128*(2x4^4)_NN_d
Tuning for shape 4x512*(2x4^1)_NN_d
Tuning for shape 4x256*(2x4^2)_NN_d
Tuning for shape 4x128*(2x4^3)_NN_d
Tuning for shape 4x256*(2x4^1)_NN_d
Tuning for shape 4x128*(2x4^2)_NN_d
Tuning for shape 4x128*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^7  &  1.000 & 1.000 & 0.486 & 2.057
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 7 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 128] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x to produce Y[16, 16384]
Matmul: 16 x 16384 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8192*(2x4^1)_NN_d
Tuning for shape 16x4096*(2x4^2)_NN_d
Tuning for shape 16x2048*(2x4^3)_NN_d
Tuning for shape 16x1024*(2x4^4)_NN_d
Tuning for shape 16x512*(2x4^5)_NN_d
Tuning for shape 16x256*(2x4^6)_NN_d
Tuning for shape 16x128*(2x4^7)_NN_d
Tuning for shape 16x4096*(2x4^1)_NN_d
Tuning for shape 16x2048*(2x4^2)_NN_d
Tuning for shape 16x1024*(2x4^3)_NN_d
Tuning for shape 16x512*(2x4^4)_NN_d
Tuning for shape 16x256*(2x4^5)_NN_d
Tuning for shape 16x128*(2x4^6)_NN_d
Tuning for shape 16x2048*(2x4^1)_NN_d
Tuning for shape 16x1024*(2x4^2)_NN_d
Tuning for shape 16x512*(2x4^3)_NN_d
Tuning for shape 16x256*(2x4^4)_NN_d
Tuning for shape 16x128*(2x4^5)_NN_d
Tuning for shape 16x1024*(2x4^1)_NN_d
Tuning for shape 16x512*(2x4^2)_NN_d
Tuning for shape 16x256*(2x4^3)_NN_d
Tuning for shape 16x128*(2x4^4)_NN_d
Tuning for shape 16x512*(2x4^1)_NN_d
Tuning for shape 16x256*(2x4^2)_NN_d
Tuning for shape 16x128*(2x4^3)_NN_d
Tuning for shape 16x256*(2x4^1)_NN_d
Tuning for shape 16x128*(2x4^2)_NN_d
Tuning for shape 16x128*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^7  &  1.000 & 1.000 & 2.223 & 0.450
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 7 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 128] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x to produce Y[64, 16384]
Matmul: 64 x 16384 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8192*(2x4^1)_NN_d
Tuning for shape 64x4096*(2x4^2)_NN_d
Tuning for shape 64x2048*(2x4^3)_NN_d
Tuning for shape 64x1024*(2x4^4)_NN_d
Tuning for shape 64x512*(2x4^5)_NN_d
Tuning for shape 64x256*(2x4^6)_NN_d
Tuning for shape 64x128*(2x4^7)_NN_d
Tuning for shape 64x4096*(2x4^1)_NN_d
Tuning for shape 64x2048*(2x4^2)_NN_d
Tuning for shape 64x1024*(2x4^3)_NN_d
Tuning for shape 64x512*(2x4^4)_NN_d
Tuning for shape 64x256*(2x4^5)_NN_d
Tuning for shape 64x128*(2x4^6)_NN_d
Tuning for shape 64x2048*(2x4^1)_NN_d
Tuning for shape 64x1024*(2x4^2)_NN_d
Tuning for shape 64x512*(2x4^3)_NN_d
Tuning for shape 64x256*(2x4^4)_NN_d
Tuning for shape 64x128*(2x4^5)_NN_d
Tuning for shape 64x1024*(2x4^1)_NN_d
Tuning for shape 64x512*(2x4^2)_NN_d
Tuning for shape 64x256*(2x4^3)_NN_d
Tuning for shape 64x128*(2x4^4)_NN_d
Tuning for shape 64x512*(2x4^1)_NN_d
Tuning for shape 64x256*(2x4^2)_NN_d
Tuning for shape 64x128*(2x4^3)_NN_d
Tuning for shape 64x256*(2x4^1)_NN_d
Tuning for shape 64x128*(2x4^2)_NN_d
Tuning for shape 64x128*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^7  &  1.000 & 1.000 & 8.778 & 0.114
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 7 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 128] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x to produce Y[256, 16384]
Matmul: 256 x 16384 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8192*(2x4^1)_NN_d
Tuning for shape 256x4096*(2x4^2)_NN_d
Tuning for shape 256x2048*(2x4^3)_NN_d
Tuning for shape 256x1024*(2x4^4)_NN_d
Tuning for shape 256x512*(2x4^5)_NN_d
Tuning for shape 256x256*(2x4^6)_NN_d
Tuning for shape 256x128*(2x4^7)_NN_d
Tuning for shape 256x4096*(2x4^1)_NN_d
Tuning for shape 256x2048*(2x4^2)_NN_d
Tuning for shape 256x1024*(2x4^3)_NN_d
Tuning for shape 256x512*(2x4^4)_NN_d
Tuning for shape 256x256*(2x4^5)_NN_d
Tuning for shape 256x128*(2x4^6)_NN_d
Tuning for shape 256x2048*(2x4^1)_NN_d
Tuning for shape 256x1024*(2x4^2)_NN_d
Tuning for shape 256x512*(2x4^3)_NN_d
Tuning for shape 256x256*(2x4^4)_NN_d
Tuning for shape 256x128*(2x4^5)_NN_d
Tuning for shape 256x1024*(2x4^1)_NN_d
Tuning for shape 256x512*(2x4^2)_NN_d
Tuning for shape 256x256*(2x4^3)_NN_d
Tuning for shape 256x128*(2x4^4)_NN_d
Tuning for shape 256x512*(2x4^1)_NN_d
Tuning for shape 256x256*(2x4^2)_NN_d
Tuning for shape 256x128*(2x4^3)_NN_d
Tuning for shape 256x256*(2x4^1)_NN_d
Tuning for shape 256x128*(2x4^2)_NN_d
Tuning for shape 256x128*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^7  &  1.000 & 1.000 & 30.826 & 0.032
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 7 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 128] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x to produce Y[1024, 16384]
Matmul: 1024 x 16384 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8192*(2x4^1)_NN_d
Tuning for shape 1024x4096*(2x4^2)_NN_d
Tuning for shape 1024x2048*(2x4^3)_NN_d
Tuning for shape 1024x1024*(2x4^4)_NN_d
Tuning for shape 1024x512*(2x4^5)_NN_d
Tuning for shape 1024x256*(2x4^6)_NN_d
Tuning for shape 1024x128*(2x4^7)_NN_d
Tuning for shape 1024x4096*(2x4^1)_NN_d
Tuning for shape 1024x2048*(2x4^2)_NN_d
Tuning for shape 1024x1024*(2x4^3)_NN_d
Tuning for shape 1024x512*(2x4^4)_NN_d
Tuning for shape 1024x256*(2x4^5)_NN_d
Tuning for shape 1024x128*(2x4^6)_NN_d
Tuning for shape 1024x2048*(2x4^1)_NN_d
Tuning for shape 1024x1024*(2x4^2)_NN_d
Tuning for shape 1024x512*(2x4^3)_NN_d
Tuning for shape 1024x256*(2x4^4)_NN_d
Tuning for shape 1024x128*(2x4^5)_NN_d
Tuning for shape 1024x1024*(2x4^1)_NN_d
Tuning for shape 1024x512*(2x4^2)_NN_d
Tuning for shape 1024x256*(2x4^3)_NN_d
Tuning for shape 1024x128*(2x4^4)_NN_d
Tuning for shape 1024x512*(2x4^1)_NN_d
Tuning for shape 1024x256*(2x4^2)_NN_d
Tuning for shape 1024x128*(2x4^3)_NN_d
Tuning for shape 1024x256*(2x4^1)_NN_d
Tuning for shape 1024x128*(2x4^2)_NN_d
Tuning for shape 1024x128*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x4^7  &  1.000 & 1.000 & 32.966 & 0.030
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 8 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x to produce Y[1, 65536]
Matmul: 1 x 65536 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32768*(2x4^1)_NN_d
Tuning for shape 1x16384*(2x4^2)_NN_d
Tuning for shape 1x8192*(2x4^3)_NN_d
Tuning for shape 1x4096*(2x4^4)_NN_d
Tuning for shape 1x2048*(2x4^5)_NN_d
Tuning for shape 1x1024*(2x4^6)_NN_d
Tuning for shape 1x512*(2x4^7)_NN_d
Tuning for shape 1x256*(2x4^8)_NN_d
Tuning for shape 1x16384*(2x4^1)_NN_d
Tuning for shape 1x8192*(2x4^2)_NN_d
Tuning for shape 1x4096*(2x4^3)_NN_d
Tuning for shape 1x2048*(2x4^4)_NN_d
Tuning for shape 1x1024*(2x4^5)_NN_d
Tuning for shape 1x512*(2x4^6)_NN_d
Tuning for shape 1x256*(2x4^7)_NN_d
Tuning for shape 1x8192*(2x4^1)_NN_d
Tuning for shape 1x4096*(2x4^2)_NN_d
Tuning for shape 1x2048*(2x4^3)_NN_d
Tuning for shape 1x1024*(2x4^4)_NN_d
Tuning for shape 1x512*(2x4^5)_NN_d
Tuning for shape 1x256*(2x4^6)_NN_d
Tuning for shape 1x4096*(2x4^1)_NN_d
Tuning for shape 1x2048*(2x4^2)_NN_d
Tuning for shape 1x1024*(2x4^3)_NN_d
Tuning for shape 1x512*(2x4^4)_NN_d
Tuning for shape 1x256*(2x4^5)_NN_d
Tuning for shape 1x2048*(2x4^1)_NN_d
Tuning for shape 1x1024*(2x4^2)_NN_d
Tuning for shape 1x512*(2x4^3)_NN_d
Tuning for shape 1x256*(2x4^4)_NN_d
Tuning for shape 1x1024*(2x4^1)_NN_d
Tuning for shape 1x512*(2x4^2)_NN_d
Tuning for shape 1x256*(2x4^3)_NN_d
Tuning for shape 1x512*(2x4^1)_NN_d
Tuning for shape 1x256*(2x4^2)_NN_d
Tuning for shape 1x256*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^8  &  1.000 & 1.000 & 0.522 & 1.914
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 8 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x to produce Y[4, 65536]
Matmul: 4 x 65536 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32768*(2x4^1)_NN_d
Tuning for shape 4x16384*(2x4^2)_NN_d
Tuning for shape 4x8192*(2x4^3)_NN_d
Tuning for shape 4x4096*(2x4^4)_NN_d
Tuning for shape 4x2048*(2x4^5)_NN_d
Tuning for shape 4x1024*(2x4^6)_NN_d
Tuning for shape 4x512*(2x4^7)_NN_d
Tuning for shape 4x256*(2x4^8)_NN_d
Tuning for shape 4x16384*(2x4^1)_NN_d
Tuning for shape 4x8192*(2x4^2)_NN_d
Tuning for shape 4x4096*(2x4^3)_NN_d
Tuning for shape 4x2048*(2x4^4)_NN_d
Tuning for shape 4x1024*(2x4^5)_NN_d
Tuning for shape 4x512*(2x4^6)_NN_d
Tuning for shape 4x256*(2x4^7)_NN_d
Tuning for shape 4x8192*(2x4^1)_NN_d
Tuning for shape 4x4096*(2x4^2)_NN_d
Tuning for shape 4x2048*(2x4^3)_NN_d
Tuning for shape 4x1024*(2x4^4)_NN_d
Tuning for shape 4x512*(2x4^5)_NN_d
Tuning for shape 4x256*(2x4^6)_NN_d
Tuning for shape 4x4096*(2x4^1)_NN_d
Tuning for shape 4x2048*(2x4^2)_NN_d
Tuning for shape 4x1024*(2x4^3)_NN_d
Tuning for shape 4x512*(2x4^4)_NN_d
Tuning for shape 4x256*(2x4^5)_NN_d
Tuning for shape 4x2048*(2x4^1)_NN_d
Tuning for shape 4x1024*(2x4^2)_NN_d
Tuning for shape 4x512*(2x4^3)_NN_d
Tuning for shape 4x256*(2x4^4)_NN_d
Tuning for shape 4x1024*(2x4^1)_NN_d
Tuning for shape 4x512*(2x4^2)_NN_d
Tuning for shape 4x256*(2x4^3)_NN_d
Tuning for shape 4x512*(2x4^1)_NN_d
Tuning for shape 4x256*(2x4^2)_NN_d
Tuning for shape 4x256*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^8  &  1.000 & 1.000 & 2.032 & 0.492
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 8 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x to produce Y[16, 65536]
Matmul: 16 x 65536 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32768*(2x4^1)_NN_d
Tuning for shape 16x16384*(2x4^2)_NN_d
Tuning for shape 16x8192*(2x4^3)_NN_d
Tuning for shape 16x4096*(2x4^4)_NN_d
Tuning for shape 16x2048*(2x4^5)_NN_d
Tuning for shape 16x1024*(2x4^6)_NN_d
Tuning for shape 16x512*(2x4^7)_NN_d
Tuning for shape 16x256*(2x4^8)_NN_d
Tuning for shape 16x16384*(2x4^1)_NN_d
Tuning for shape 16x8192*(2x4^2)_NN_d
Tuning for shape 16x4096*(2x4^3)_NN_d
Tuning for shape 16x2048*(2x4^4)_NN_d
Tuning for shape 16x1024*(2x4^5)_NN_d
Tuning for shape 16x512*(2x4^6)_NN_d
Tuning for shape 16x256*(2x4^7)_NN_d
Tuning for shape 16x8192*(2x4^1)_NN_d
Tuning for shape 16x4096*(2x4^2)_NN_d
Tuning for shape 16x2048*(2x4^3)_NN_d
Tuning for shape 16x1024*(2x4^4)_NN_d
Tuning for shape 16x512*(2x4^5)_NN_d
Tuning for shape 16x256*(2x4^6)_NN_d
Tuning for shape 16x4096*(2x4^1)_NN_d
Tuning for shape 16x2048*(2x4^2)_NN_d
Tuning for shape 16x1024*(2x4^3)_NN_d
Tuning for shape 16x512*(2x4^4)_NN_d
Tuning for shape 16x256*(2x4^5)_NN_d
Tuning for shape 16x2048*(2x4^1)_NN_d
Tuning for shape 16x1024*(2x4^2)_NN_d
Tuning for shape 16x512*(2x4^3)_NN_d
Tuning for shape 16x256*(2x4^4)_NN_d
Tuning for shape 16x1024*(2x4^1)_NN_d
Tuning for shape 16x512*(2x4^2)_NN_d
Tuning for shape 16x256*(2x4^3)_NN_d
Tuning for shape 16x512*(2x4^1)_NN_d
Tuning for shape 16x256*(2x4^2)_NN_d
Tuning for shape 16x256*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^8  &  1.000 & 1.000 & 8.151 & 0.123
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 8 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x to produce Y[64, 65536]
Matmul: 64 x 65536 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32768*(2x4^1)_NN_d
Tuning for shape 64x16384*(2x4^2)_NN_d
Tuning for shape 64x8192*(2x4^3)_NN_d
Tuning for shape 64x4096*(2x4^4)_NN_d
Tuning for shape 64x2048*(2x4^5)_NN_d
Tuning for shape 64x1024*(2x4^6)_NN_d
Tuning for shape 64x512*(2x4^7)_NN_d
Tuning for shape 64x256*(2x4^8)_NN_d
Tuning for shape 64x16384*(2x4^1)_NN_d
Tuning for shape 64x8192*(2x4^2)_NN_d
Tuning for shape 64x4096*(2x4^3)_NN_d
Tuning for shape 64x2048*(2x4^4)_NN_d
Tuning for shape 64x1024*(2x4^5)_NN_d
Tuning for shape 64x512*(2x4^6)_NN_d
Tuning for shape 64x256*(2x4^7)_NN_d
Tuning for shape 64x8192*(2x4^1)_NN_d
Tuning for shape 64x4096*(2x4^2)_NN_d
Tuning for shape 64x2048*(2x4^3)_NN_d
Tuning for shape 64x1024*(2x4^4)_NN_d
Tuning for shape 64x512*(2x4^5)_NN_d
Tuning for shape 64x256*(2x4^6)_NN_d
Tuning for shape 64x4096*(2x4^1)_NN_d
Tuning for shape 64x2048*(2x4^2)_NN_d
Tuning for shape 64x1024*(2x4^3)_NN_d
Tuning for shape 64x512*(2x4^4)_NN_d
Tuning for shape 64x256*(2x4^5)_NN_d
Tuning for shape 64x2048*(2x4^1)_NN_d
Tuning for shape 64x1024*(2x4^2)_NN_d
Tuning for shape 64x512*(2x4^3)_NN_d
Tuning for shape 64x256*(2x4^4)_NN_d
Tuning for shape 64x1024*(2x4^1)_NN_d
Tuning for shape 64x512*(2x4^2)_NN_d
Tuning for shape 64x256*(2x4^3)_NN_d
Tuning for shape 64x512*(2x4^1)_NN_d
Tuning for shape 64x256*(2x4^2)_NN_d
Tuning for shape 64x256*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^8  &  1.000 & 1.000 & 31.212 & 0.032
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 8 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x to produce Y[256, 65536]
Matmul: 256 x 65536 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32768*(2x4^1)_NN_d
Tuning for shape 256x16384*(2x4^2)_NN_d
Tuning for shape 256x8192*(2x4^3)_NN_d
Tuning for shape 256x4096*(2x4^4)_NN_d
Tuning for shape 256x2048*(2x4^5)_NN_d
Tuning for shape 256x1024*(2x4^6)_NN_d
Tuning for shape 256x512*(2x4^7)_NN_d
Tuning for shape 256x256*(2x4^8)_NN_d
Tuning for shape 256x16384*(2x4^1)_NN_d
Tuning for shape 256x8192*(2x4^2)_NN_d
Tuning for shape 256x4096*(2x4^3)_NN_d
Tuning for shape 256x2048*(2x4^4)_NN_d
Tuning for shape 256x1024*(2x4^5)_NN_d
Tuning for shape 256x512*(2x4^6)_NN_d
Tuning for shape 256x256*(2x4^7)_NN_d
Tuning for shape 256x8192*(2x4^1)_NN_d
Tuning for shape 256x4096*(2x4^2)_NN_d
Tuning for shape 256x2048*(2x4^3)_NN_d
Tuning for shape 256x1024*(2x4^4)_NN_d
Tuning for shape 256x512*(2x4^5)_NN_d
Tuning for shape 256x256*(2x4^6)_NN_d
Tuning for shape 256x4096*(2x4^1)_NN_d
Tuning for shape 256x2048*(2x4^2)_NN_d
Tuning for shape 256x1024*(2x4^3)_NN_d
Tuning for shape 256x512*(2x4^4)_NN_d
Tuning for shape 256x256*(2x4^5)_NN_d
Tuning for shape 256x2048*(2x4^1)_NN_d
Tuning for shape 256x1024*(2x4^2)_NN_d
Tuning for shape 256x512*(2x4^3)_NN_d
Tuning for shape 256x256*(2x4^4)_NN_d
Tuning for shape 256x1024*(2x4^1)_NN_d
Tuning for shape 256x512*(2x4^2)_NN_d
Tuning for shape 256x256*(2x4^3)_NN_d
Tuning for shape 256x512*(2x4^1)_NN_d
Tuning for shape 256x256*(2x4^2)_NN_d
Tuning for shape 256x256*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^8  &  1.000 & 1.000 & 35.424 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 8 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 256] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x to produce Y[1024, 65536]
Matmul: 1024 x 65536 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32768*(2x4^1)_NN_d
Tuning for shape 1024x16384*(2x4^2)_NN_d
Tuning for shape 1024x8192*(2x4^3)_NN_d
Tuning for shape 1024x4096*(2x4^4)_NN_d
Tuning for shape 1024x2048*(2x4^5)_NN_d
Tuning for shape 1024x1024*(2x4^6)_NN_d
Tuning for shape 1024x512*(2x4^7)_NN_d
Tuning for shape 1024x256*(2x4^8)_NN_d
Tuning for shape 1024x16384*(2x4^1)_NN_d
Tuning for shape 1024x8192*(2x4^2)_NN_d
Tuning for shape 1024x4096*(2x4^3)_NN_d
Tuning for shape 1024x2048*(2x4^4)_NN_d
Tuning for shape 1024x1024*(2x4^5)_NN_d
Tuning for shape 1024x512*(2x4^6)_NN_d
Tuning for shape 1024x256*(2x4^7)_NN_d
Tuning for shape 1024x8192*(2x4^1)_NN_d
Tuning for shape 1024x4096*(2x4^2)_NN_d
Tuning for shape 1024x2048*(2x4^3)_NN_d
Tuning for shape 1024x1024*(2x4^4)_NN_d
Tuning for shape 1024x512*(2x4^5)_NN_d
Tuning for shape 1024x256*(2x4^6)_NN_d
Tuning for shape 1024x4096*(2x4^1)_NN_d
Tuning for shape 1024x2048*(2x4^2)_NN_d
Tuning for shape 1024x1024*(2x4^3)_NN_d
Tuning for shape 1024x512*(2x4^4)_NN_d
Tuning for shape 1024x256*(2x4^5)_NN_d
Tuning for shape 1024x2048*(2x4^1)_NN_d
Tuning for shape 1024x1024*(2x4^2)_NN_d
Tuning for shape 1024x512*(2x4^3)_NN_d
Tuning for shape 1024x256*(2x4^4)_NN_d
Tuning for shape 1024x1024*(2x4^1)_NN_d
Tuning for shape 1024x512*(2x4^2)_NN_d
Tuning for shape 1024x256*(2x4^3)_NN_d
Tuning for shape 1024x512*(2x4^1)_NN_d
Tuning for shape 1024x256*(2x4^2)_NN_d
Tuning for shape 1024x256*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x4^8  &  1.000 & 1.000 & 36.386 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 9 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x131072*(2x4^1)_NN_d
Tuning for shape 1x65536*(2x4^2)_NN_d
Tuning for shape 1x32768*(2x4^3)_NN_d
Tuning for shape 1x16384*(2x4^4)_NN_d
Tuning for shape 1x8192*(2x4^5)_NN_d
Tuning for shape 1x4096*(2x4^6)_NN_d
Tuning for shape 1x2048*(2x4^7)_NN_d
Tuning for shape 1x1024*(2x4^8)_NN_d
Tuning for shape 1x512*(2x4^9)_NN_d
Tuning for shape 1x65536*(2x4^1)_NN_d
Tuning for shape 1x32768*(2x4^2)_NN_d
Tuning for shape 1x16384*(2x4^3)_NN_d
Tuning for shape 1x8192*(2x4^4)_NN_d
Tuning for shape 1x4096*(2x4^5)_NN_d
Tuning for shape 1x2048*(2x4^6)_NN_d
Tuning for shape 1x1024*(2x4^7)_NN_d
Tuning for shape 1x512*(2x4^8)_NN_d
Tuning for shape 1x32768*(2x4^1)_NN_d
Tuning for shape 1x16384*(2x4^2)_NN_d
Tuning for shape 1x8192*(2x4^3)_NN_d
Tuning for shape 1x4096*(2x4^4)_NN_d
Tuning for shape 1x2048*(2x4^5)_NN_d
Tuning for shape 1x1024*(2x4^6)_NN_d
Tuning for shape 1x512*(2x4^7)_NN_d
Tuning for shape 1x16384*(2x4^1)_NN_d
Tuning for shape 1x8192*(2x4^2)_NN_d
Tuning for shape 1x4096*(2x4^3)_NN_d
Tuning for shape 1x2048*(2x4^4)_NN_d
Tuning for shape 1x1024*(2x4^5)_NN_d
Tuning for shape 1x512*(2x4^6)_NN_d
Tuning for shape 1x8192*(2x4^1)_NN_d
Tuning for shape 1x4096*(2x4^2)_NN_d
Tuning for shape 1x2048*(2x4^3)_NN_d
Tuning for shape 1x1024*(2x4^4)_NN_d
Tuning for shape 1x512*(2x4^5)_NN_d
Tuning for shape 1x4096*(2x4^1)_NN_d
Tuning for shape 1x2048*(2x4^2)_NN_d
Tuning for shape 1x1024*(2x4^3)_NN_d
Tuning for shape 1x512*(2x4^4)_NN_d
Tuning for shape 1x2048*(2x4^1)_NN_d
Tuning for shape 1x1024*(2x4^2)_NN_d
Tuning for shape 1x512*(2x4^3)_NN_d
Tuning for shape 1x1024*(2x4^1)_NN_d
Tuning for shape 1x512*(2x4^2)_NN_d
Tuning for shape 1x512*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^9  &  1.000 & 1.000 & 1.927 & 0.519
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 9 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x131072*(2x4^1)_NN_d
Tuning for shape 4x65536*(2x4^2)_NN_d
Tuning for shape 4x32768*(2x4^3)_NN_d
Tuning for shape 4x16384*(2x4^4)_NN_d
Tuning for shape 4x8192*(2x4^5)_NN_d
Tuning for shape 4x4096*(2x4^6)_NN_d
Tuning for shape 4x2048*(2x4^7)_NN_d
Tuning for shape 4x1024*(2x4^8)_NN_d
Tuning for shape 4x512*(2x4^9)_NN_d
Tuning for shape 4x65536*(2x4^1)_NN_d
Tuning for shape 4x32768*(2x4^2)_NN_d
Tuning for shape 4x16384*(2x4^3)_NN_d
Tuning for shape 4x8192*(2x4^4)_NN_d
Tuning for shape 4x4096*(2x4^5)_NN_d
Tuning for shape 4x2048*(2x4^6)_NN_d
Tuning for shape 4x1024*(2x4^7)_NN_d
Tuning for shape 4x512*(2x4^8)_NN_d
Tuning for shape 4x32768*(2x4^1)_NN_d
Tuning for shape 4x16384*(2x4^2)_NN_d
Tuning for shape 4x8192*(2x4^3)_NN_d
Tuning for shape 4x4096*(2x4^4)_NN_d
Tuning for shape 4x2048*(2x4^5)_NN_d
Tuning for shape 4x1024*(2x4^6)_NN_d
Tuning for shape 4x512*(2x4^7)_NN_d
Tuning for shape 4x16384*(2x4^1)_NN_d
Tuning for shape 4x8192*(2x4^2)_NN_d
Tuning for shape 4x4096*(2x4^3)_NN_d
Tuning for shape 4x2048*(2x4^4)_NN_d
Tuning for shape 4x1024*(2x4^5)_NN_d
Tuning for shape 4x512*(2x4^6)_NN_d
Tuning for shape 4x8192*(2x4^1)_NN_d
Tuning for shape 4x4096*(2x4^2)_NN_d
Tuning for shape 4x2048*(2x4^3)_NN_d
Tuning for shape 4x1024*(2x4^4)_NN_d
Tuning for shape 4x512*(2x4^5)_NN_d
Tuning for shape 4x4096*(2x4^1)_NN_d
Tuning for shape 4x2048*(2x4^2)_NN_d
Tuning for shape 4x1024*(2x4^3)_NN_d
Tuning for shape 4x512*(2x4^4)_NN_d
Tuning for shape 4x2048*(2x4^1)_NN_d
Tuning for shape 4x1024*(2x4^2)_NN_d
Tuning for shape 4x512*(2x4^3)_NN_d
Tuning for shape 4x1024*(2x4^1)_NN_d
Tuning for shape 4x512*(2x4^2)_NN_d
Tuning for shape 4x512*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^9  &  1.000 & 1.000 & 7.518 & 0.133
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 9 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 512] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x131072*(2x4^1)_NN_d
Tuning for shape 16x65536*(2x4^2)_NN_d
Tuning for shape 16x32768*(2x4^3)_NN_d
Tuning for shape 16x16384*(2x4^4)_NN_d
Tuning for shape 16x8192*(2x4^5)_NN_d
Tuning for shape 16x4096*(2x4^6)_NN_d
Tuning for shape 16x2048*(2x4^7)_NN_d
Tuning for shape 16x1024*(2x4^8)_NN_d
Tuning for shape 16x512*(2x4^9)_NN_d
Tuning for shape 16x65536*(2x4^1)_NN_d
Tuning for shape 16x32768*(2x4^2)_NN_d
Tuning for shape 16x16384*(2x4^3)_NN_d
Tuning for shape 16x8192*(2x4^4)_NN_d
Tuning for shape 16x4096*(2x4^5)_NN_d
Tuning for shape 16x2048*(2x4^6)_NN_d
Tuning for shape 16x1024*(2x4^7)_NN_d
Tuning for shape 16x512*(2x4^8)_NN_d
Tuning for shape 16x32768*(2x4^1)_NN_d
Tuning for shape 16x16384*(2x4^2)_NN_d
Tuning for shape 16x8192*(2x4^3)_NN_d
Tuning for shape 16x4096*(2x4^4)_NN_d
Tuning for shape 16x2048*(2x4^5)_NN_d
Tuning for shape 16x1024*(2x4^6)_NN_d
Tuning for shape 16x512*(2x4^7)_NN_d
Tuning for shape 16x16384*(2x4^1)_NN_d
Tuning for shape 16x8192*(2x4^2)_NN_d
Tuning for shape 16x4096*(2x4^3)_NN_d
Tuning for shape 16x2048*(2x4^4)_NN_d
Tuning for shape 16x1024*(2x4^5)_NN_d
Tuning for shape 16x512*(2x4^6)_NN_d
Tuning for shape 16x8192*(2x4^1)_NN_d
Tuning for shape 16x4096*(2x4^2)_NN_d
Tuning for shape 16x2048*(2x4^3)_NN_d
Tuning for shape 16x1024*(2x4^4)_NN_d
Tuning for shape 16x512*(2x4^5)_NN_d
Tuning for shape 16x4096*(2x4^1)_NN_d
Tuning for shape 16x2048*(2x4^2)_NN_d
Tuning for shape 16x1024*(2x4^3)_NN_d
Tuning for shape 16x512*(2x4^4)_NN_d
Tuning for shape 16x2048*(2x4^1)_NN_d
Tuning for shape 16x1024*(2x4^2)_NN_d
Tuning for shape 16x512*(2x4^3)_NN_d
Tuning for shape 16x1024*(2x4^1)_NN_d
Tuning for shape 16x512*(2x4^2)_NN_d
Tuning for shape 16x512*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^9  &  1.000 & 1.000 & 28.884 & 0.035
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 9 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 512] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x to produce Y[64, 262144]
Matmul: 64 x 262144 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x131072*(2x4^1)_NN_d
Tuning for shape 64x65536*(2x4^2)_NN_d
Tuning for shape 64x32768*(2x4^3)_NN_d
Tuning for shape 64x16384*(2x4^4)_NN_d
Tuning for shape 64x8192*(2x4^5)_NN_d
Tuning for shape 64x4096*(2x4^6)_NN_d
Tuning for shape 64x2048*(2x4^7)_NN_d
Tuning for shape 64x1024*(2x4^8)_NN_d
Tuning for shape 64x512*(2x4^9)_NN_d
Tuning for shape 64x65536*(2x4^1)_NN_d
Tuning for shape 64x32768*(2x4^2)_NN_d
Tuning for shape 64x16384*(2x4^3)_NN_d
Tuning for shape 64x8192*(2x4^4)_NN_d
Tuning for shape 64x4096*(2x4^5)_NN_d
Tuning for shape 64x2048*(2x4^6)_NN_d
Tuning for shape 64x1024*(2x4^7)_NN_d
Tuning for shape 64x512*(2x4^8)_NN_d
Tuning for shape 64x32768*(2x4^1)_NN_d
Tuning for shape 64x16384*(2x4^2)_NN_d
Tuning for shape 64x8192*(2x4^3)_NN_d
Tuning for shape 64x4096*(2x4^4)_NN_d
Tuning for shape 64x2048*(2x4^5)_NN_d
Tuning for shape 64x1024*(2x4^6)_NN_d
Tuning for shape 64x512*(2x4^7)_NN_d
Tuning for shape 64x16384*(2x4^1)_NN_d
Tuning for shape 64x8192*(2x4^2)_NN_d
Tuning for shape 64x4096*(2x4^3)_NN_d
Tuning for shape 64x2048*(2x4^4)_NN_d
Tuning for shape 64x1024*(2x4^5)_NN_d
Tuning for shape 64x512*(2x4^6)_NN_d
Tuning for shape 64x8192*(2x4^1)_NN_d
Tuning for shape 64x4096*(2x4^2)_NN_d
Tuning for shape 64x2048*(2x4^3)_NN_d
Tuning for shape 64x1024*(2x4^4)_NN_d
Tuning for shape 64x512*(2x4^5)_NN_d
Tuning for shape 64x4096*(2x4^1)_NN_d
Tuning for shape 64x2048*(2x4^2)_NN_d
Tuning for shape 64x1024*(2x4^3)_NN_d
Tuning for shape 64x512*(2x4^4)_NN_d
Tuning for shape 64x2048*(2x4^1)_NN_d
Tuning for shape 64x1024*(2x4^2)_NN_d
Tuning for shape 64x512*(2x4^3)_NN_d
Tuning for shape 64x1024*(2x4^1)_NN_d
Tuning for shape 64x512*(2x4^2)_NN_d
Tuning for shape 64x512*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^9  &  1.000 & 1.000 & 35.314 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 9 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 512] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x to produce Y[256, 262144]
Matmul: 256 x 262144 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x131072*(2x4^1)_NN_d
Tuning for shape 256x65536*(2x4^2)_NN_d
Tuning for shape 256x32768*(2x4^3)_NN_d
Tuning for shape 256x16384*(2x4^4)_NN_d
Tuning for shape 256x8192*(2x4^5)_NN_d
Tuning for shape 256x4096*(2x4^6)_NN_d
Tuning for shape 256x2048*(2x4^7)_NN_d
Tuning for shape 256x1024*(2x4^8)_NN_d
Tuning for shape 256x512*(2x4^9)_NN_d
Tuning for shape 256x65536*(2x4^1)_NN_d
Tuning for shape 256x32768*(2x4^2)_NN_d
Tuning for shape 256x16384*(2x4^3)_NN_d
Tuning for shape 256x8192*(2x4^4)_NN_d
Tuning for shape 256x4096*(2x4^5)_NN_d
Tuning for shape 256x2048*(2x4^6)_NN_d
Tuning for shape 256x1024*(2x4^7)_NN_d
Tuning for shape 256x512*(2x4^8)_NN_d
Tuning for shape 256x32768*(2x4^1)_NN_d
Tuning for shape 256x16384*(2x4^2)_NN_d
Tuning for shape 256x8192*(2x4^3)_NN_d
Tuning for shape 256x4096*(2x4^4)_NN_d
Tuning for shape 256x2048*(2x4^5)_NN_d
Tuning for shape 256x1024*(2x4^6)_NN_d
Tuning for shape 256x512*(2x4^7)_NN_d
Tuning for shape 256x16384*(2x4^1)_NN_d
Tuning for shape 256x8192*(2x4^2)_NN_d
Tuning for shape 256x4096*(2x4^3)_NN_d
Tuning for shape 256x2048*(2x4^4)_NN_d
Tuning for shape 256x1024*(2x4^5)_NN_d
Tuning for shape 256x512*(2x4^6)_NN_d
Tuning for shape 256x8192*(2x4^1)_NN_d
Tuning for shape 256x4096*(2x4^2)_NN_d
Tuning for shape 256x2048*(2x4^3)_NN_d
Tuning for shape 256x1024*(2x4^4)_NN_d
Tuning for shape 256x512*(2x4^5)_NN_d
Tuning for shape 256x4096*(2x4^1)_NN_d
Tuning for shape 256x2048*(2x4^2)_NN_d
Tuning for shape 256x1024*(2x4^3)_NN_d
Tuning for shape 256x512*(2x4^4)_NN_d
Tuning for shape 256x2048*(2x4^1)_NN_d
Tuning for shape 256x1024*(2x4^2)_NN_d
Tuning for shape 256x512*(2x4^3)_NN_d
Tuning for shape 256x1024*(2x4^1)_NN_d
Tuning for shape 256x512*(2x4^2)_NN_d
Tuning for shape 256x512*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^9  &  1.000 & 1.000 & 36.371 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 9 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 512] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x to produce Y[1024, 262144]
Matmul: 1024 x 262144 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x131072*(2x4^1)_NN_d
Tuning for shape 1024x65536*(2x4^2)_NN_d
Tuning for shape 1024x32768*(2x4^3)_NN_d
Tuning for shape 1024x16384*(2x4^4)_NN_d
Tuning for shape 1024x8192*(2x4^5)_NN_d
Tuning for shape 1024x4096*(2x4^6)_NN_d
Tuning for shape 1024x2048*(2x4^7)_NN_d
Tuning for shape 1024x1024*(2x4^8)_NN_d
Tuning for shape 1024x512*(2x4^9)_NN_d
Tuning for shape 1024x65536*(2x4^1)_NN_d
Tuning for shape 1024x32768*(2x4^2)_NN_d
Tuning for shape 1024x16384*(2x4^3)_NN_d
Tuning for shape 1024x8192*(2x4^4)_NN_d
Tuning for shape 1024x4096*(2x4^5)_NN_d
Tuning for shape 1024x2048*(2x4^6)_NN_d
Tuning for shape 1024x1024*(2x4^7)_NN_d
Tuning for shape 1024x512*(2x4^8)_NN_d
Tuning for shape 1024x32768*(2x4^1)_NN_d
Tuning for shape 1024x16384*(2x4^2)_NN_d
Tuning for shape 1024x8192*(2x4^3)_NN_d
Tuning for shape 1024x4096*(2x4^4)_NN_d
Tuning for shape 1024x2048*(2x4^5)_NN_d
Tuning for shape 1024x1024*(2x4^6)_NN_d
Tuning for shape 1024x512*(2x4^7)_NN_d
Tuning for shape 1024x16384*(2x4^1)_NN_d
Tuning for shape 1024x8192*(2x4^2)_NN_d
Tuning for shape 1024x4096*(2x4^3)_NN_d
Tuning for shape 1024x2048*(2x4^4)_NN_d
Tuning for shape 1024x1024*(2x4^5)_NN_d
Tuning for shape 1024x512*(2x4^6)_NN_d
Tuning for shape 1024x8192*(2x4^1)_NN_d
Tuning for shape 1024x4096*(2x4^2)_NN_d
Tuning for shape 1024x2048*(2x4^3)_NN_d
Tuning for shape 1024x1024*(2x4^4)_NN_d
Tuning for shape 1024x512*(2x4^5)_NN_d
Tuning for shape 1024x4096*(2x4^1)_NN_d
Tuning for shape 1024x2048*(2x4^2)_NN_d
Tuning for shape 1024x1024*(2x4^3)_NN_d
Tuning for shape 1024x512*(2x4^4)_NN_d
Tuning for shape 1024x2048*(2x4^1)_NN_d
Tuning for shape 1024x1024*(2x4^2)_NN_d
Tuning for shape 1024x512*(2x4^3)_NN_d
Tuning for shape 1024x1024*(2x4^1)_NN_d
Tuning for shape 1024x512*(2x4^2)_NN_d
Tuning for shape 1024x512*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x4^9  &  1.000 & 1.000 & 36.641 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 10 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1024] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x to produce Y[1, 1048576]
Matmul: 1 x 1048576 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x524288*(2x4^1)_NN_d
Tuning for shape 1x262144*(2x4^2)_NN_d
Tuning for shape 1x131072*(2x4^3)_NN_d
Tuning for shape 1x65536*(2x4^4)_NN_d
Tuning for shape 1x32768*(2x4^5)_NN_d
Tuning for shape 1x16384*(2x4^6)_NN_d
Tuning for shape 1x8192*(2x4^7)_NN_d
Tuning for shape 1x4096*(2x4^8)_NN_d
Tuning for shape 1x2048*(2x4^9)_NN_d
Tuning for shape 1x1024*(2x4^10)_NN_d
Tuning for shape 1x262144*(2x4^1)_NN_d
Tuning for shape 1x131072*(2x4^2)_NN_d
Tuning for shape 1x65536*(2x4^3)_NN_d
Tuning for shape 1x32768*(2x4^4)_NN_d
Tuning for shape 1x16384*(2x4^5)_NN_d
Tuning for shape 1x8192*(2x4^6)_NN_d
Tuning for shape 1x4096*(2x4^7)_NN_d
Tuning for shape 1x2048*(2x4^8)_NN_d
Tuning for shape 1x1024*(2x4^9)_NN_d
Tuning for shape 1x131072*(2x4^1)_NN_d
Tuning for shape 1x65536*(2x4^2)_NN_d
Tuning for shape 1x32768*(2x4^3)_NN_d
Tuning for shape 1x16384*(2x4^4)_NN_d
Tuning for shape 1x8192*(2x4^5)_NN_d
Tuning for shape 1x4096*(2x4^6)_NN_d
Tuning for shape 1x2048*(2x4^7)_NN_d
Tuning for shape 1x1024*(2x4^8)_NN_d
Tuning for shape 1x65536*(2x4^1)_NN_d
Tuning for shape 1x32768*(2x4^2)_NN_d
Tuning for shape 1x16384*(2x4^3)_NN_d
Tuning for shape 1x8192*(2x4^4)_NN_d
Tuning for shape 1x4096*(2x4^5)_NN_d
Tuning for shape 1x2048*(2x4^6)_NN_d
Tuning for shape 1x1024*(2x4^7)_NN_d
Tuning for shape 1x32768*(2x4^1)_NN_d
Tuning for shape 1x16384*(2x4^2)_NN_d
Tuning for shape 1x8192*(2x4^3)_NN_d
Tuning for shape 1x4096*(2x4^4)_NN_d
Tuning for shape 1x2048*(2x4^5)_NN_d
Tuning for shape 1x1024*(2x4^6)_NN_d
Tuning for shape 1x16384*(2x4^1)_NN_d
Tuning for shape 1x8192*(2x4^2)_NN_d
Tuning for shape 1x4096*(2x4^3)_NN_d
Tuning for shape 1x2048*(2x4^4)_NN_d
Tuning for shape 1x1024*(2x4^5)_NN_d
Tuning for shape 1x8192*(2x4^1)_NN_d
Tuning for shape 1x4096*(2x4^2)_NN_d
Tuning for shape 1x2048*(2x4^3)_NN_d
Tuning for shape 1x1024*(2x4^4)_NN_d
Tuning for shape 1x4096*(2x4^1)_NN_d
Tuning for shape 1x2048*(2x4^2)_NN_d
Tuning for shape 1x1024*(2x4^3)_NN_d
Tuning for shape 1x2048*(2x4^1)_NN_d
Tuning for shape 1x1024*(2x4^2)_NN_d
Tuning for shape 1x1024*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^10  &  1.000 & 1.000 & 7.150 & 0.140
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 10 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1024] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x to produce Y[4, 1048576]
Matmul: 4 x 1048576 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x524288*(2x4^1)_NN_d
Tuning for shape 4x262144*(2x4^2)_NN_d
Tuning for shape 4x131072*(2x4^3)_NN_d
Tuning for shape 4x65536*(2x4^4)_NN_d
Tuning for shape 4x32768*(2x4^5)_NN_d
Tuning for shape 4x16384*(2x4^6)_NN_d
Tuning for shape 4x8192*(2x4^7)_NN_d
Tuning for shape 4x4096*(2x4^8)_NN_d
Tuning for shape 4x2048*(2x4^9)_NN_d
Tuning for shape 4x1024*(2x4^10)_NN_d
Tuning for shape 4x262144*(2x4^1)_NN_d
Tuning for shape 4x131072*(2x4^2)_NN_d
Tuning for shape 4x65536*(2x4^3)_NN_d
Tuning for shape 4x32768*(2x4^4)_NN_d
Tuning for shape 4x16384*(2x4^5)_NN_d
Tuning for shape 4x8192*(2x4^6)_NN_d
Tuning for shape 4x4096*(2x4^7)_NN_d
Tuning for shape 4x2048*(2x4^8)_NN_d
Tuning for shape 4x1024*(2x4^9)_NN_d
Tuning for shape 4x131072*(2x4^1)_NN_d
Tuning for shape 4x65536*(2x4^2)_NN_d
Tuning for shape 4x32768*(2x4^3)_NN_d
Tuning for shape 4x16384*(2x4^4)_NN_d
Tuning for shape 4x8192*(2x4^5)_NN_d
Tuning for shape 4x4096*(2x4^6)_NN_d
Tuning for shape 4x2048*(2x4^7)_NN_d
Tuning for shape 4x1024*(2x4^8)_NN_d
Tuning for shape 4x65536*(2x4^1)_NN_d
Tuning for shape 4x32768*(2x4^2)_NN_d
Tuning for shape 4x16384*(2x4^3)_NN_d
Tuning for shape 4x8192*(2x4^4)_NN_d
Tuning for shape 4x4096*(2x4^5)_NN_d
Tuning for shape 4x2048*(2x4^6)_NN_d
Tuning for shape 4x1024*(2x4^7)_NN_d
Tuning for shape 4x32768*(2x4^1)_NN_d
Tuning for shape 4x16384*(2x4^2)_NN_d
Tuning for shape 4x8192*(2x4^3)_NN_d
Tuning for shape 4x4096*(2x4^4)_NN_d
Tuning for shape 4x2048*(2x4^5)_NN_d
Tuning for shape 4x1024*(2x4^6)_NN_d
Tuning for shape 4x16384*(2x4^1)_NN_d
Tuning for shape 4x8192*(2x4^2)_NN_d
Tuning for shape 4x4096*(2x4^3)_NN_d
Tuning for shape 4x2048*(2x4^4)_NN_d
Tuning for shape 4x1024*(2x4^5)_NN_d
Tuning for shape 4x8192*(2x4^1)_NN_d
Tuning for shape 4x4096*(2x4^2)_NN_d
Tuning for shape 4x2048*(2x4^3)_NN_d
Tuning for shape 4x1024*(2x4^4)_NN_d
Tuning for shape 4x4096*(2x4^1)_NN_d
Tuning for shape 4x2048*(2x4^2)_NN_d
Tuning for shape 4x1024*(2x4^3)_NN_d
Tuning for shape 4x2048*(2x4^1)_NN_d
Tuning for shape 4x1024*(2x4^2)_NN_d
Tuning for shape 4x1024*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^10  &  1.000 & 1.000 & 27.253 & 0.037
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 10 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1024] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x to produce Y[16, 1048576]
Matmul: 16 x 1048576 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x524288*(2x4^1)_NN_d
Tuning for shape 16x262144*(2x4^2)_NN_d
Tuning for shape 16x131072*(2x4^3)_NN_d
Tuning for shape 16x65536*(2x4^4)_NN_d
Tuning for shape 16x32768*(2x4^5)_NN_d
Tuning for shape 16x16384*(2x4^6)_NN_d
Tuning for shape 16x8192*(2x4^7)_NN_d
Tuning for shape 16x4096*(2x4^8)_NN_d
Tuning for shape 16x2048*(2x4^9)_NN_d
Tuning for shape 16x1024*(2x4^10)_NN_d
Tuning for shape 16x262144*(2x4^1)_NN_d
Tuning for shape 16x131072*(2x4^2)_NN_d
Tuning for shape 16x65536*(2x4^3)_NN_d
Tuning for shape 16x32768*(2x4^4)_NN_d
Tuning for shape 16x16384*(2x4^5)_NN_d
Tuning for shape 16x8192*(2x4^6)_NN_d
Tuning for shape 16x4096*(2x4^7)_NN_d
Tuning for shape 16x2048*(2x4^8)_NN_d
Tuning for shape 16x1024*(2x4^9)_NN_d
Tuning for shape 16x131072*(2x4^1)_NN_d
Tuning for shape 16x65536*(2x4^2)_NN_d
Tuning for shape 16x32768*(2x4^3)_NN_d
Tuning for shape 16x16384*(2x4^4)_NN_d
Tuning for shape 16x8192*(2x4^5)_NN_d
Tuning for shape 16x4096*(2x4^6)_NN_d
Tuning for shape 16x2048*(2x4^7)_NN_d
Tuning for shape 16x1024*(2x4^8)_NN_d
Tuning for shape 16x65536*(2x4^1)_NN_d
Tuning for shape 16x32768*(2x4^2)_NN_d
Tuning for shape 16x16384*(2x4^3)_NN_d
Tuning for shape 16x8192*(2x4^4)_NN_d
Tuning for shape 16x4096*(2x4^5)_NN_d
Tuning for shape 16x2048*(2x4^6)_NN_d
Tuning for shape 16x1024*(2x4^7)_NN_d
Tuning for shape 16x32768*(2x4^1)_NN_d
Tuning for shape 16x16384*(2x4^2)_NN_d
Tuning for shape 16x8192*(2x4^3)_NN_d
Tuning for shape 16x4096*(2x4^4)_NN_d
Tuning for shape 16x2048*(2x4^5)_NN_d
Tuning for shape 16x1024*(2x4^6)_NN_d
Tuning for shape 16x16384*(2x4^1)_NN_d
Tuning for shape 16x8192*(2x4^2)_NN_d
Tuning for shape 16x4096*(2x4^3)_NN_d
Tuning for shape 16x2048*(2x4^4)_NN_d
Tuning for shape 16x1024*(2x4^5)_NN_d
Tuning for shape 16x8192*(2x4^1)_NN_d
Tuning for shape 16x4096*(2x4^2)_NN_d
Tuning for shape 16x2048*(2x4^3)_NN_d
Tuning for shape 16x1024*(2x4^4)_NN_d
Tuning for shape 16x4096*(2x4^1)_NN_d
Tuning for shape 16x2048*(2x4^2)_NN_d
Tuning for shape 16x1024*(2x4^3)_NN_d
Tuning for shape 16x2048*(2x4^1)_NN_d
Tuning for shape 16x1024*(2x4^2)_NN_d
Tuning for shape 16x1024*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^10  &  1.000 & 1.000 & 35.587 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 10 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1024] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x to produce Y[64, 1048576]
Matmul: 64 x 1048576 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x524288*(2x4^1)_NN_d
Tuning for shape 64x262144*(2x4^2)_NN_d
Tuning for shape 64x131072*(2x4^3)_NN_d
Tuning for shape 64x65536*(2x4^4)_NN_d
Tuning for shape 64x32768*(2x4^5)_NN_d
Tuning for shape 64x16384*(2x4^6)_NN_d
Tuning for shape 64x8192*(2x4^7)_NN_d
Tuning for shape 64x4096*(2x4^8)_NN_d
Tuning for shape 64x2048*(2x4^9)_NN_d
Tuning for shape 64x1024*(2x4^10)_NN_d
Tuning for shape 64x262144*(2x4^1)_NN_d
Tuning for shape 64x131072*(2x4^2)_NN_d
Tuning for shape 64x65536*(2x4^3)_NN_d
Tuning for shape 64x32768*(2x4^4)_NN_d
Tuning for shape 64x16384*(2x4^5)_NN_d
Tuning for shape 64x8192*(2x4^6)_NN_d
Tuning for shape 64x4096*(2x4^7)_NN_d
Tuning for shape 64x2048*(2x4^8)_NN_d
Tuning for shape 64x1024*(2x4^9)_NN_d
Tuning for shape 64x131072*(2x4^1)_NN_d
Tuning for shape 64x65536*(2x4^2)_NN_d
Tuning for shape 64x32768*(2x4^3)_NN_d
Tuning for shape 64x16384*(2x4^4)_NN_d
Tuning for shape 64x8192*(2x4^5)_NN_d
Tuning for shape 64x4096*(2x4^6)_NN_d
Tuning for shape 64x2048*(2x4^7)_NN_d
Tuning for shape 64x1024*(2x4^8)_NN_d
Tuning for shape 64x65536*(2x4^1)_NN_d
Tuning for shape 64x32768*(2x4^2)_NN_d
Tuning for shape 64x16384*(2x4^3)_NN_d
Tuning for shape 64x8192*(2x4^4)_NN_d
Tuning for shape 64x4096*(2x4^5)_NN_d
Tuning for shape 64x2048*(2x4^6)_NN_d
Tuning for shape 64x1024*(2x4^7)_NN_d
Tuning for shape 64x32768*(2x4^1)_NN_d
Tuning for shape 64x16384*(2x4^2)_NN_d
Tuning for shape 64x8192*(2x4^3)_NN_d
Tuning for shape 64x4096*(2x4^4)_NN_d
Tuning for shape 64x2048*(2x4^5)_NN_d
Tuning for shape 64x1024*(2x4^6)_NN_d
Tuning for shape 64x16384*(2x4^1)_NN_d
Tuning for shape 64x8192*(2x4^2)_NN_d
Tuning for shape 64x4096*(2x4^3)_NN_d
Tuning for shape 64x2048*(2x4^4)_NN_d
Tuning for shape 64x1024*(2x4^5)_NN_d
Tuning for shape 64x8192*(2x4^1)_NN_d
Tuning for shape 64x4096*(2x4^2)_NN_d
Tuning for shape 64x2048*(2x4^3)_NN_d
Tuning for shape 64x1024*(2x4^4)_NN_d
Tuning for shape 64x4096*(2x4^1)_NN_d
Tuning for shape 64x2048*(2x4^2)_NN_d
Tuning for shape 64x1024*(2x4^3)_NN_d
Tuning for shape 64x2048*(2x4^1)_NN_d
Tuning for shape 64x1024*(2x4^2)_NN_d
Tuning for shape 64x1024*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^10  &  1.000 & 1.000 & 36.354 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 10 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1024] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x to produce Y[256, 1048576]
Matmul: 256 x 1048576 x 1024, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x524288*(2x4^1)_NN_d
Tuning for shape 256x262144*(2x4^2)_NN_d
Tuning for shape 256x131072*(2x4^3)_NN_d
Tuning for shape 256x65536*(2x4^4)_NN_d
Tuning for shape 256x32768*(2x4^5)_NN_d
Tuning for shape 256x16384*(2x4^6)_NN_d
Tuning for shape 256x8192*(2x4^7)_NN_d
Tuning for shape 256x4096*(2x4^8)_NN_d
Tuning for shape 256x2048*(2x4^9)_NN_d
Tuning for shape 256x1024*(2x4^10)_NN_d
Tuning for shape 256x262144*(2x4^1)_NN_d
Tuning for shape 256x131072*(2x4^2)_NN_d
Tuning for shape 256x65536*(2x4^3)_NN_d
Tuning for shape 256x32768*(2x4^4)_NN_d
Tuning for shape 256x16384*(2x4^5)_NN_d
Tuning for shape 256x8192*(2x4^6)_NN_d
Tuning for shape 256x4096*(2x4^7)_NN_d
Tuning for shape 256x2048*(2x4^8)_NN_d
Tuning for shape 256x1024*(2x4^9)_NN_d
Tuning for shape 256x131072*(2x4^1)_NN_d
Tuning for shape 256x65536*(2x4^2)_NN_d
Tuning for shape 256x32768*(2x4^3)_NN_d
Tuning for shape 256x16384*(2x4^4)_NN_d
Tuning for shape 256x8192*(2x4^5)_NN_d
Tuning for shape 256x4096*(2x4^6)_NN_d
Tuning for shape 256x2048*(2x4^7)_NN_d
Tuning for shape 256x1024*(2x4^8)_NN_d
Tuning for shape 256x65536*(2x4^1)_NN_d
Tuning for shape 256x32768*(2x4^2)_NN_d
Tuning for shape 256x16384*(2x4^3)_NN_d
Tuning for shape 256x8192*(2x4^4)_NN_d
Tuning for shape 256x4096*(2x4^5)_NN_d
Tuning for shape 256x2048*(2x4^6)_NN_d
Tuning for shape 256x1024*(2x4^7)_NN_d
Tuning for shape 256x32768*(2x4^1)_NN_d
Tuning for shape 256x16384*(2x4^2)_NN_d
Tuning for shape 256x8192*(2x4^3)_NN_d
Tuning for shape 256x4096*(2x4^4)_NN_d
Tuning for shape 256x2048*(2x4^5)_NN_d
Tuning for shape 256x1024*(2x4^6)_NN_d
Tuning for shape 256x16384*(2x4^1)_NN_d
Tuning for shape 256x8192*(2x4^2)_NN_d
Tuning for shape 256x4096*(2x4^3)_NN_d
Tuning for shape 256x2048*(2x4^4)_NN_d
Tuning for shape 256x1024*(2x4^5)_NN_d
Tuning for shape 256x8192*(2x4^1)_NN_d
Tuning for shape 256x4096*(2x4^2)_NN_d
Tuning for shape 256x2048*(2x4^3)_NN_d
Tuning for shape 256x1024*(2x4^4)_NN_d
Tuning for shape 256x4096*(2x4^1)_NN_d
Tuning for shape 256x2048*(2x4^2)_NN_d
Tuning for shape 256x1024*(2x4^3)_NN_d
Tuning for shape 256x2048*(2x4^1)_NN_d
Tuning for shape 256x1024*(2x4^2)_NN_d
Tuning for shape 256x1024*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x4^10  &  1.000 & 1.000 & 36.625 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 11 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2048] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x to produce Y[1, 4194304]
Matmul: 1 x 4194304 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2097152*(2x4^1)_NN_d
Tuning for shape 1x1048576*(2x4^2)_NN_d
Tuning for shape 1x524288*(2x4^3)_NN_d
Tuning for shape 1x262144*(2x4^4)_NN_d
Tuning for shape 1x131072*(2x4^5)_NN_d
Tuning for shape 1x65536*(2x4^6)_NN_d
Tuning for shape 1x32768*(2x4^7)_NN_d
Tuning for shape 1x16384*(2x4^8)_NN_d
Tuning for shape 1x8192*(2x4^9)_NN_d
Tuning for shape 1x4096*(2x4^10)_NN_d
Tuning for shape 1x2048*(2x4^11)_NN_d
Tuning for shape 1x1048576*(2x4^1)_NN_d
Tuning for shape 1x524288*(2x4^2)_NN_d
Tuning for shape 1x262144*(2x4^3)_NN_d
Tuning for shape 1x131072*(2x4^4)_NN_d
Tuning for shape 1x65536*(2x4^5)_NN_d
Tuning for shape 1x32768*(2x4^6)_NN_d
Tuning for shape 1x16384*(2x4^7)_NN_d
Tuning for shape 1x8192*(2x4^8)_NN_d
Tuning for shape 1x4096*(2x4^9)_NN_d
Tuning for shape 1x2048*(2x4^10)_NN_d
Tuning for shape 1x524288*(2x4^1)_NN_d
Tuning for shape 1x262144*(2x4^2)_NN_d
Tuning for shape 1x131072*(2x4^3)_NN_d
Tuning for shape 1x65536*(2x4^4)_NN_d
Tuning for shape 1x32768*(2x4^5)_NN_d
Tuning for shape 1x16384*(2x4^6)_NN_d
Tuning for shape 1x8192*(2x4^7)_NN_d
Tuning for shape 1x4096*(2x4^8)_NN_d
Tuning for shape 1x2048*(2x4^9)_NN_d
Tuning for shape 1x262144*(2x4^1)_NN_d
Tuning for shape 1x131072*(2x4^2)_NN_d
Tuning for shape 1x65536*(2x4^3)_NN_d
Tuning for shape 1x32768*(2x4^4)_NN_d
Tuning for shape 1x16384*(2x4^5)_NN_d
Tuning for shape 1x8192*(2x4^6)_NN_d
Tuning for shape 1x4096*(2x4^7)_NN_d
Tuning for shape 1x2048*(2x4^8)_NN_d
Tuning for shape 1x131072*(2x4^1)_NN_d
Tuning for shape 1x65536*(2x4^2)_NN_d
Tuning for shape 1x32768*(2x4^3)_NN_d
Tuning for shape 1x16384*(2x4^4)_NN_d
Tuning for shape 1x8192*(2x4^5)_NN_d
Tuning for shape 1x4096*(2x4^6)_NN_d
Tuning for shape 1x2048*(2x4^7)_NN_d
Tuning for shape 1x65536*(2x4^1)_NN_d
Tuning for shape 1x32768*(2x4^2)_NN_d
Tuning for shape 1x16384*(2x4^3)_NN_d
Tuning for shape 1x8192*(2x4^4)_NN_d
Tuning for shape 1x4096*(2x4^5)_NN_d
Tuning for shape 1x2048*(2x4^6)_NN_d
Tuning for shape 1x32768*(2x4^1)_NN_d
Tuning for shape 1x16384*(2x4^2)_NN_d
Tuning for shape 1x8192*(2x4^3)_NN_d
Tuning for shape 1x4096*(2x4^4)_NN_d
Tuning for shape 1x2048*(2x4^5)_NN_d
Tuning for shape 1x16384*(2x4^1)_NN_d
Tuning for shape 1x8192*(2x4^2)_NN_d
Tuning for shape 1x4096*(2x4^3)_NN_d
Tuning for shape 1x2048*(2x4^4)_NN_d
Tuning for shape 1x8192*(2x4^1)_NN_d
Tuning for shape 1x4096*(2x4^2)_NN_d
Tuning for shape 1x2048*(2x4^3)_NN_d
Tuning for shape 1x4096*(2x4^1)_NN_d
Tuning for shape 1x2048*(2x4^2)_NN_d
Tuning for shape 1x2048*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^11  &  1.000 & 1.000 & 25.510 & 0.039
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 11 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2048] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x to produce Y[4, 4194304]
Matmul: 4 x 4194304 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2097152*(2x4^1)_NN_d
Tuning for shape 4x1048576*(2x4^2)_NN_d
Tuning for shape 4x524288*(2x4^3)_NN_d
Tuning for shape 4x262144*(2x4^4)_NN_d
Tuning for shape 4x131072*(2x4^5)_NN_d
Tuning for shape 4x65536*(2x4^6)_NN_d
Tuning for shape 4x32768*(2x4^7)_NN_d
Tuning for shape 4x16384*(2x4^8)_NN_d
Tuning for shape 4x8192*(2x4^9)_NN_d
Tuning for shape 4x4096*(2x4^10)_NN_d
Tuning for shape 4x2048*(2x4^11)_NN_d
Tuning for shape 4x1048576*(2x4^1)_NN_d
Tuning for shape 4x524288*(2x4^2)_NN_d
Tuning for shape 4x262144*(2x4^3)_NN_d
Tuning for shape 4x131072*(2x4^4)_NN_d
Tuning for shape 4x65536*(2x4^5)_NN_d
Tuning for shape 4x32768*(2x4^6)_NN_d
Tuning for shape 4x16384*(2x4^7)_NN_d
Tuning for shape 4x8192*(2x4^8)_NN_d
Tuning for shape 4x4096*(2x4^9)_NN_d
Tuning for shape 4x2048*(2x4^10)_NN_d
Tuning for shape 4x524288*(2x4^1)_NN_d
Tuning for shape 4x262144*(2x4^2)_NN_d
Tuning for shape 4x131072*(2x4^3)_NN_d
Tuning for shape 4x65536*(2x4^4)_NN_d
Tuning for shape 4x32768*(2x4^5)_NN_d
Tuning for shape 4x16384*(2x4^6)_NN_d
Tuning for shape 4x8192*(2x4^7)_NN_d
Tuning for shape 4x4096*(2x4^8)_NN_d
Tuning for shape 4x2048*(2x4^9)_NN_d
Tuning for shape 4x262144*(2x4^1)_NN_d
Tuning for shape 4x131072*(2x4^2)_NN_d
Tuning for shape 4x65536*(2x4^3)_NN_d
Tuning for shape 4x32768*(2x4^4)_NN_d
Tuning for shape 4x16384*(2x4^5)_NN_d
Tuning for shape 4x8192*(2x4^6)_NN_d
Tuning for shape 4x4096*(2x4^7)_NN_d
Tuning for shape 4x2048*(2x4^8)_NN_d
Tuning for shape 4x131072*(2x4^1)_NN_d
Tuning for shape 4x65536*(2x4^2)_NN_d
Tuning for shape 4x32768*(2x4^3)_NN_d
Tuning for shape 4x16384*(2x4^4)_NN_d
Tuning for shape 4x8192*(2x4^5)_NN_d
Tuning for shape 4x4096*(2x4^6)_NN_d
Tuning for shape 4x2048*(2x4^7)_NN_d
Tuning for shape 4x65536*(2x4^1)_NN_d
Tuning for shape 4x32768*(2x4^2)_NN_d
Tuning for shape 4x16384*(2x4^3)_NN_d
Tuning for shape 4x8192*(2x4^4)_NN_d
Tuning for shape 4x4096*(2x4^5)_NN_d
Tuning for shape 4x2048*(2x4^6)_NN_d
Tuning for shape 4x32768*(2x4^1)_NN_d
Tuning for shape 4x16384*(2x4^2)_NN_d
Tuning for shape 4x8192*(2x4^3)_NN_d
Tuning for shape 4x4096*(2x4^4)_NN_d
Tuning for shape 4x2048*(2x4^5)_NN_d
Tuning for shape 4x16384*(2x4^1)_NN_d
Tuning for shape 4x8192*(2x4^2)_NN_d
Tuning for shape 4x4096*(2x4^3)_NN_d
Tuning for shape 4x2048*(2x4^4)_NN_d
Tuning for shape 4x8192*(2x4^1)_NN_d
Tuning for shape 4x4096*(2x4^2)_NN_d
Tuning for shape 4x2048*(2x4^3)_NN_d
Tuning for shape 4x4096*(2x4^1)_NN_d
Tuning for shape 4x2048*(2x4^2)_NN_d
Tuning for shape 4x2048*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^11  &  1.000 & 1.000 & 35.450 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 11 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2048] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x to produce Y[16, 4194304]
Matmul: 16 x 4194304 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2097152*(2x4^1)_NN_d
Tuning for shape 16x1048576*(2x4^2)_NN_d
Tuning for shape 16x524288*(2x4^3)_NN_d
Tuning for shape 16x262144*(2x4^4)_NN_d
Tuning for shape 16x131072*(2x4^5)_NN_d
Tuning for shape 16x65536*(2x4^6)_NN_d
Tuning for shape 16x32768*(2x4^7)_NN_d
Tuning for shape 16x16384*(2x4^8)_NN_d
Tuning for shape 16x8192*(2x4^9)_NN_d
Tuning for shape 16x4096*(2x4^10)_NN_d
Tuning for shape 16x2048*(2x4^11)_NN_d
Tuning for shape 16x1048576*(2x4^1)_NN_d
Tuning for shape 16x524288*(2x4^2)_NN_d
Tuning for shape 16x262144*(2x4^3)_NN_d
Tuning for shape 16x131072*(2x4^4)_NN_d
Tuning for shape 16x65536*(2x4^5)_NN_d
Tuning for shape 16x32768*(2x4^6)_NN_d
Tuning for shape 16x16384*(2x4^7)_NN_d
Tuning for shape 16x8192*(2x4^8)_NN_d
Tuning for shape 16x4096*(2x4^9)_NN_d
Tuning for shape 16x2048*(2x4^10)_NN_d
Tuning for shape 16x524288*(2x4^1)_NN_d
Tuning for shape 16x262144*(2x4^2)_NN_d
Tuning for shape 16x131072*(2x4^3)_NN_d
Tuning for shape 16x65536*(2x4^4)_NN_d
Tuning for shape 16x32768*(2x4^5)_NN_d
Tuning for shape 16x16384*(2x4^6)_NN_d
Tuning for shape 16x8192*(2x4^7)_NN_d
Tuning for shape 16x4096*(2x4^8)_NN_d
Tuning for shape 16x2048*(2x4^9)_NN_d
Tuning for shape 16x262144*(2x4^1)_NN_d
Tuning for shape 16x131072*(2x4^2)_NN_d
Tuning for shape 16x65536*(2x4^3)_NN_d
Tuning for shape 16x32768*(2x4^4)_NN_d
Tuning for shape 16x16384*(2x4^5)_NN_d
Tuning for shape 16x8192*(2x4^6)_NN_d
Tuning for shape 16x4096*(2x4^7)_NN_d
Tuning for shape 16x2048*(2x4^8)_NN_d
Tuning for shape 16x131072*(2x4^1)_NN_d
Tuning for shape 16x65536*(2x4^2)_NN_d
Tuning for shape 16x32768*(2x4^3)_NN_d
Tuning for shape 16x16384*(2x4^4)_NN_d
Tuning for shape 16x8192*(2x4^5)_NN_d
Tuning for shape 16x4096*(2x4^6)_NN_d
Tuning for shape 16x2048*(2x4^7)_NN_d
Tuning for shape 16x65536*(2x4^1)_NN_d
Tuning for shape 16x32768*(2x4^2)_NN_d
Tuning for shape 16x16384*(2x4^3)_NN_d
Tuning for shape 16x8192*(2x4^4)_NN_d
Tuning for shape 16x4096*(2x4^5)_NN_d
Tuning for shape 16x2048*(2x4^6)_NN_d
Tuning for shape 16x32768*(2x4^1)_NN_d
Tuning for shape 16x16384*(2x4^2)_NN_d
Tuning for shape 16x8192*(2x4^3)_NN_d
Tuning for shape 16x4096*(2x4^4)_NN_d
Tuning for shape 16x2048*(2x4^5)_NN_d
Tuning for shape 16x16384*(2x4^1)_NN_d
Tuning for shape 16x8192*(2x4^2)_NN_d
Tuning for shape 16x4096*(2x4^3)_NN_d
Tuning for shape 16x2048*(2x4^4)_NN_d
Tuning for shape 16x8192*(2x4^1)_NN_d
Tuning for shape 16x4096*(2x4^2)_NN_d
Tuning for shape 16x2048*(2x4^3)_NN_d
Tuning for shape 16x4096*(2x4^1)_NN_d
Tuning for shape 16x2048*(2x4^2)_NN_d
Tuning for shape 16x2048*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^11  &  1.000 & 1.000 & 36.420 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 11 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2048] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x to produce Y[64, 4194304]
Matmul: 64 x 4194304 x 2048, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2097152*(2x4^1)_NN_d
Tuning for shape 64x1048576*(2x4^2)_NN_d
Tuning for shape 64x524288*(2x4^3)_NN_d
Tuning for shape 64x262144*(2x4^4)_NN_d
Tuning for shape 64x131072*(2x4^5)_NN_d
Tuning for shape 64x65536*(2x4^6)_NN_d
Tuning for shape 64x32768*(2x4^7)_NN_d
Tuning for shape 64x16384*(2x4^8)_NN_d
Tuning for shape 64x8192*(2x4^9)_NN_d
Tuning for shape 64x4096*(2x4^10)_NN_d
Tuning for shape 64x2048*(2x4^11)_NN_d
Tuning for shape 64x1048576*(2x4^1)_NN_d
Tuning for shape 64x524288*(2x4^2)_NN_d
Tuning for shape 64x262144*(2x4^3)_NN_d
Tuning for shape 64x131072*(2x4^4)_NN_d
Tuning for shape 64x65536*(2x4^5)_NN_d
Tuning for shape 64x32768*(2x4^6)_NN_d
Tuning for shape 64x16384*(2x4^7)_NN_d
Tuning for shape 64x8192*(2x4^8)_NN_d
Tuning for shape 64x4096*(2x4^9)_NN_d
Tuning for shape 64x2048*(2x4^10)_NN_d
Tuning for shape 64x524288*(2x4^1)_NN_d
Tuning for shape 64x262144*(2x4^2)_NN_d
Tuning for shape 64x131072*(2x4^3)_NN_d
Tuning for shape 64x65536*(2x4^4)_NN_d
Tuning for shape 64x32768*(2x4^5)_NN_d
Tuning for shape 64x16384*(2x4^6)_NN_d
Tuning for shape 64x8192*(2x4^7)_NN_d
Tuning for shape 64x4096*(2x4^8)_NN_d
Tuning for shape 64x2048*(2x4^9)_NN_d
Tuning for shape 64x262144*(2x4^1)_NN_d
Tuning for shape 64x131072*(2x4^2)_NN_d
Tuning for shape 64x65536*(2x4^3)_NN_d
Tuning for shape 64x32768*(2x4^4)_NN_d
Tuning for shape 64x16384*(2x4^5)_NN_d
Tuning for shape 64x8192*(2x4^6)_NN_d
Tuning for shape 64x4096*(2x4^7)_NN_d
Tuning for shape 64x2048*(2x4^8)_NN_d
Tuning for shape 64x131072*(2x4^1)_NN_d
Tuning for shape 64x65536*(2x4^2)_NN_d
Tuning for shape 64x32768*(2x4^3)_NN_d
Tuning for shape 64x16384*(2x4^4)_NN_d
Tuning for shape 64x8192*(2x4^5)_NN_d
Tuning for shape 64x4096*(2x4^6)_NN_d
Tuning for shape 64x2048*(2x4^7)_NN_d
Tuning for shape 64x65536*(2x4^1)_NN_d
Tuning for shape 64x32768*(2x4^2)_NN_d
Tuning for shape 64x16384*(2x4^3)_NN_d
Tuning for shape 64x8192*(2x4^4)_NN_d
Tuning for shape 64x4096*(2x4^5)_NN_d
Tuning for shape 64x2048*(2x4^6)_NN_d
Tuning for shape 64x32768*(2x4^1)_NN_d
Tuning for shape 64x16384*(2x4^2)_NN_d
Tuning for shape 64x8192*(2x4^3)_NN_d
Tuning for shape 64x4096*(2x4^4)_NN_d
Tuning for shape 64x2048*(2x4^5)_NN_d
Tuning for shape 64x16384*(2x4^1)_NN_d
Tuning for shape 64x8192*(2x4^2)_NN_d
Tuning for shape 64x4096*(2x4^3)_NN_d
Tuning for shape 64x2048*(2x4^4)_NN_d
Tuning for shape 64x8192*(2x4^1)_NN_d
Tuning for shape 64x4096*(2x4^2)_NN_d
Tuning for shape 64x2048*(2x4^3)_NN_d
Tuning for shape 64x4096*(2x4^1)_NN_d
Tuning for shape 64x2048*(2x4^2)_NN_d
Tuning for shape 64x2048*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x4^11  &  1.000 & 1.000 & 36.617 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 12 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x F_11 [2, 4] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 4096, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8388608*(2x4^1)_NN_d
Tuning for shape 1x4194304*(2x4^2)_NN_d
Tuning for shape 1x2097152*(2x4^3)_NN_d
Tuning for shape 1x1048576*(2x4^4)_NN_d
Tuning for shape 1x524288*(2x4^5)_NN_d
Tuning for shape 1x262144*(2x4^6)_NN_d
Tuning for shape 1x131072*(2x4^7)_NN_d
Tuning for shape 1x65536*(2x4^8)_NN_d
Tuning for shape 1x32768*(2x4^9)_NN_d
Tuning for shape 1x16384*(2x4^10)_NN_d
Tuning for shape 1x8192*(2x4^11)_NN_d
Tuning for shape 1x4096*(2x4^12)_NN_d
Tuning for shape 1x4194304*(2x4^1)_NN_d
Tuning for shape 1x2097152*(2x4^2)_NN_d
Tuning for shape 1x1048576*(2x4^3)_NN_d
Tuning for shape 1x524288*(2x4^4)_NN_d
Tuning for shape 1x262144*(2x4^5)_NN_d
Tuning for shape 1x131072*(2x4^6)_NN_d
Tuning for shape 1x65536*(2x4^7)_NN_d
Tuning for shape 1x32768*(2x4^8)_NN_d
Tuning for shape 1x16384*(2x4^9)_NN_d
Tuning for shape 1x8192*(2x4^10)_NN_d
Tuning for shape 1x4096*(2x4^11)_NN_d
Tuning for shape 1x2097152*(2x4^1)_NN_d
Tuning for shape 1x1048576*(2x4^2)_NN_d
Tuning for shape 1x524288*(2x4^3)_NN_d
Tuning for shape 1x262144*(2x4^4)_NN_d
Tuning for shape 1x131072*(2x4^5)_NN_d
Tuning for shape 1x65536*(2x4^6)_NN_d
Tuning for shape 1x32768*(2x4^7)_NN_d
Tuning for shape 1x16384*(2x4^8)_NN_d
Tuning for shape 1x8192*(2x4^9)_NN_d
Tuning for shape 1x4096*(2x4^10)_NN_d
Tuning for shape 1x1048576*(2x4^1)_NN_d
Tuning for shape 1x524288*(2x4^2)_NN_d
Tuning for shape 1x262144*(2x4^3)_NN_d
Tuning for shape 1x131072*(2x4^4)_NN_d
Tuning for shape 1x65536*(2x4^5)_NN_d
Tuning for shape 1x32768*(2x4^6)_NN_d
Tuning for shape 1x16384*(2x4^7)_NN_d
Tuning for shape 1x8192*(2x4^8)_NN_d
Tuning for shape 1x4096*(2x4^9)_NN_d
Tuning for shape 1x524288*(2x4^1)_NN_d
Tuning for shape 1x262144*(2x4^2)_NN_d
Tuning for shape 1x131072*(2x4^3)_NN_d
Tuning for shape 1x65536*(2x4^4)_NN_d
Tuning for shape 1x32768*(2x4^5)_NN_d
Tuning for shape 1x16384*(2x4^6)_NN_d
Tuning for shape 1x8192*(2x4^7)_NN_d
Tuning for shape 1x4096*(2x4^8)_NN_d
Tuning for shape 1x262144*(2x4^1)_NN_d
Tuning for shape 1x131072*(2x4^2)_NN_d
Tuning for shape 1x65536*(2x4^3)_NN_d
Tuning for shape 1x32768*(2x4^4)_NN_d
Tuning for shape 1x16384*(2x4^5)_NN_d
Tuning for shape 1x8192*(2x4^6)_NN_d
Tuning for shape 1x4096*(2x4^7)_NN_d
Tuning for shape 1x131072*(2x4^1)_NN_d
Tuning for shape 1x65536*(2x4^2)_NN_d
Tuning for shape 1x32768*(2x4^3)_NN_d
Tuning for shape 1x16384*(2x4^4)_NN_d
Tuning for shape 1x8192*(2x4^5)_NN_d
Tuning for shape 1x4096*(2x4^6)_NN_d
Tuning for shape 1x65536*(2x4^1)_NN_d
Tuning for shape 1x32768*(2x4^2)_NN_d
Tuning for shape 1x16384*(2x4^3)_NN_d
Tuning for shape 1x8192*(2x4^4)_NN_d
Tuning for shape 1x4096*(2x4^5)_NN_d
Tuning for shape 1x32768*(2x4^1)_NN_d
Tuning for shape 1x16384*(2x4^2)_NN_d
Tuning for shape 1x8192*(2x4^3)_NN_d
Tuning for shape 1x4096*(2x4^4)_NN_d
Tuning for shape 1x16384*(2x4^1)_NN_d
Tuning for shape 1x8192*(2x4^2)_NN_d
Tuning for shape 1x4096*(2x4^3)_NN_d
Tuning for shape 1x8192*(2x4^1)_NN_d
Tuning for shape 1x4096*(2x4^2)_NN_d
Tuning for shape 1x4096*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^12  &  1.000 & 1.000 & 35.131 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 12 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x F_11 [2, 4] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 4096, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8388608*(2x4^1)_NN_d
Tuning for shape 4x4194304*(2x4^2)_NN_d
Tuning for shape 4x2097152*(2x4^3)_NN_d
Tuning for shape 4x1048576*(2x4^4)_NN_d
Tuning for shape 4x524288*(2x4^5)_NN_d
Tuning for shape 4x262144*(2x4^6)_NN_d
Tuning for shape 4x131072*(2x4^7)_NN_d
Tuning for shape 4x65536*(2x4^8)_NN_d
Tuning for shape 4x32768*(2x4^9)_NN_d
Tuning for shape 4x16384*(2x4^10)_NN_d
Tuning for shape 4x8192*(2x4^11)_NN_d
Tuning for shape 4x4096*(2x4^12)_NN_d
Tuning for shape 4x4194304*(2x4^1)_NN_d
Tuning for shape 4x2097152*(2x4^2)_NN_d
Tuning for shape 4x1048576*(2x4^3)_NN_d
Tuning for shape 4x524288*(2x4^4)_NN_d
Tuning for shape 4x262144*(2x4^5)_NN_d
Tuning for shape 4x131072*(2x4^6)_NN_d
Tuning for shape 4x65536*(2x4^7)_NN_d
Tuning for shape 4x32768*(2x4^8)_NN_d
Tuning for shape 4x16384*(2x4^9)_NN_d
Tuning for shape 4x8192*(2x4^10)_NN_d
Tuning for shape 4x4096*(2x4^11)_NN_d
Tuning for shape 4x2097152*(2x4^1)_NN_d
Tuning for shape 4x1048576*(2x4^2)_NN_d
Tuning for shape 4x524288*(2x4^3)_NN_d
Tuning for shape 4x262144*(2x4^4)_NN_d
Tuning for shape 4x131072*(2x4^5)_NN_d
Tuning for shape 4x65536*(2x4^6)_NN_d
Tuning for shape 4x32768*(2x4^7)_NN_d
Tuning for shape 4x16384*(2x4^8)_NN_d
Tuning for shape 4x8192*(2x4^9)_NN_d
Tuning for shape 4x4096*(2x4^10)_NN_d
Tuning for shape 4x1048576*(2x4^1)_NN_d
Tuning for shape 4x524288*(2x4^2)_NN_d
Tuning for shape 4x262144*(2x4^3)_NN_d
Tuning for shape 4x131072*(2x4^4)_NN_d
Tuning for shape 4x65536*(2x4^5)_NN_d
Tuning for shape 4x32768*(2x4^6)_NN_d
Tuning for shape 4x16384*(2x4^7)_NN_d
Tuning for shape 4x8192*(2x4^8)_NN_d
Tuning for shape 4x4096*(2x4^9)_NN_d
Tuning for shape 4x524288*(2x4^1)_NN_d
Tuning for shape 4x262144*(2x4^2)_NN_d
Tuning for shape 4x131072*(2x4^3)_NN_d
Tuning for shape 4x65536*(2x4^4)_NN_d
Tuning for shape 4x32768*(2x4^5)_NN_d
Tuning for shape 4x16384*(2x4^6)_NN_d
Tuning for shape 4x8192*(2x4^7)_NN_d
Tuning for shape 4x4096*(2x4^8)_NN_d
Tuning for shape 4x262144*(2x4^1)_NN_d
Tuning for shape 4x131072*(2x4^2)_NN_d
Tuning for shape 4x65536*(2x4^3)_NN_d
Tuning for shape 4x32768*(2x4^4)_NN_d
Tuning for shape 4x16384*(2x4^5)_NN_d
Tuning for shape 4x8192*(2x4^6)_NN_d
Tuning for shape 4x4096*(2x4^7)_NN_d
Tuning for shape 4x131072*(2x4^1)_NN_d
Tuning for shape 4x65536*(2x4^2)_NN_d
Tuning for shape 4x32768*(2x4^3)_NN_d
Tuning for shape 4x16384*(2x4^4)_NN_d
Tuning for shape 4x8192*(2x4^5)_NN_d
Tuning for shape 4x4096*(2x4^6)_NN_d
Tuning for shape 4x65536*(2x4^1)_NN_d
Tuning for shape 4x32768*(2x4^2)_NN_d
Tuning for shape 4x16384*(2x4^3)_NN_d
Tuning for shape 4x8192*(2x4^4)_NN_d
Tuning for shape 4x4096*(2x4^5)_NN_d
Tuning for shape 4x32768*(2x4^1)_NN_d
Tuning for shape 4x16384*(2x4^2)_NN_d
Tuning for shape 4x8192*(2x4^3)_NN_d
Tuning for shape 4x4096*(2x4^4)_NN_d
Tuning for shape 4x16384*(2x4^1)_NN_d
Tuning for shape 4x8192*(2x4^2)_NN_d
Tuning for shape 4x4096*(2x4^3)_NN_d
Tuning for shape 4x8192*(2x4^1)_NN_d
Tuning for shape 4x4096*(2x4^2)_NN_d
Tuning for shape 4x4096*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^12  &  1.000 & 1.000 & 36.343 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 12 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x F_11 [2, 4] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 4096, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8388608*(2x4^1)_NN_d
Tuning for shape 16x4194304*(2x4^2)_NN_d
Tuning for shape 16x2097152*(2x4^3)_NN_d
Tuning for shape 16x1048576*(2x4^4)_NN_d
Tuning for shape 16x524288*(2x4^5)_NN_d
Tuning for shape 16x262144*(2x4^6)_NN_d
Tuning for shape 16x131072*(2x4^7)_NN_d
Tuning for shape 16x65536*(2x4^8)_NN_d
Tuning for shape 16x32768*(2x4^9)_NN_d
Tuning for shape 16x16384*(2x4^10)_NN_d
Tuning for shape 16x8192*(2x4^11)_NN_d
Tuning for shape 16x4096*(2x4^12)_NN_d
Tuning for shape 16x4194304*(2x4^1)_NN_d
Tuning for shape 16x2097152*(2x4^2)_NN_d
Tuning for shape 16x1048576*(2x4^3)_NN_d
Tuning for shape 16x524288*(2x4^4)_NN_d
Tuning for shape 16x262144*(2x4^5)_NN_d
Tuning for shape 16x131072*(2x4^6)_NN_d
Tuning for shape 16x65536*(2x4^7)_NN_d
Tuning for shape 16x32768*(2x4^8)_NN_d
Tuning for shape 16x16384*(2x4^9)_NN_d
Tuning for shape 16x8192*(2x4^10)_NN_d
Tuning for shape 16x4096*(2x4^11)_NN_d
Tuning for shape 16x2097152*(2x4^1)_NN_d
Tuning for shape 16x1048576*(2x4^2)_NN_d
Tuning for shape 16x524288*(2x4^3)_NN_d
Tuning for shape 16x262144*(2x4^4)_NN_d
Tuning for shape 16x131072*(2x4^5)_NN_d
Tuning for shape 16x65536*(2x4^6)_NN_d
Tuning for shape 16x32768*(2x4^7)_NN_d
Tuning for shape 16x16384*(2x4^8)_NN_d
Tuning for shape 16x8192*(2x4^9)_NN_d
Tuning for shape 16x4096*(2x4^10)_NN_d
Tuning for shape 16x1048576*(2x4^1)_NN_d
Tuning for shape 16x524288*(2x4^2)_NN_d
Tuning for shape 16x262144*(2x4^3)_NN_d
Tuning for shape 16x131072*(2x4^4)_NN_d
Tuning for shape 16x65536*(2x4^5)_NN_d
Tuning for shape 16x32768*(2x4^6)_NN_d
Tuning for shape 16x16384*(2x4^7)_NN_d
Tuning for shape 16x8192*(2x4^8)_NN_d
Tuning for shape 16x4096*(2x4^9)_NN_d
Tuning for shape 16x524288*(2x4^1)_NN_d
Tuning for shape 16x262144*(2x4^2)_NN_d
Tuning for shape 16x131072*(2x4^3)_NN_d
Tuning for shape 16x65536*(2x4^4)_NN_d
Tuning for shape 16x32768*(2x4^5)_NN_d
Tuning for shape 16x16384*(2x4^6)_NN_d
Tuning for shape 16x8192*(2x4^7)_NN_d
Tuning for shape 16x4096*(2x4^8)_NN_d
Tuning for shape 16x262144*(2x4^1)_NN_d
Tuning for shape 16x131072*(2x4^2)_NN_d
Tuning for shape 16x65536*(2x4^3)_NN_d
Tuning for shape 16x32768*(2x4^4)_NN_d
Tuning for shape 16x16384*(2x4^5)_NN_d
Tuning for shape 16x8192*(2x4^6)_NN_d
Tuning for shape 16x4096*(2x4^7)_NN_d
Tuning for shape 16x131072*(2x4^1)_NN_d
Tuning for shape 16x65536*(2x4^2)_NN_d
Tuning for shape 16x32768*(2x4^3)_NN_d
Tuning for shape 16x16384*(2x4^4)_NN_d
Tuning for shape 16x8192*(2x4^5)_NN_d
Tuning for shape 16x4096*(2x4^6)_NN_d
Tuning for shape 16x65536*(2x4^1)_NN_d
Tuning for shape 16x32768*(2x4^2)_NN_d
Tuning for shape 16x16384*(2x4^3)_NN_d
Tuning for shape 16x8192*(2x4^4)_NN_d
Tuning for shape 16x4096*(2x4^5)_NN_d
Tuning for shape 16x32768*(2x4^1)_NN_d
Tuning for shape 16x16384*(2x4^2)_NN_d
Tuning for shape 16x8192*(2x4^3)_NN_d
Tuning for shape 16x4096*(2x4^4)_NN_d
Tuning for shape 16x16384*(2x4^1)_NN_d
Tuning for shape 16x8192*(2x4^2)_NN_d
Tuning for shape 16x4096*(2x4^3)_NN_d
Tuning for shape 16x8192*(2x4^1)_NN_d
Tuning for shape 16x4096*(2x4^2)_NN_d
Tuning for shape 16x4096*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x4^12  &  1.000 & 1.000 & 36.743 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 13 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8192] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x F_11 [2, 4] x F_12 [2, 4] x to produce Y[1, 67108864]
Matmul: 1 x 67108864 x 8192, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x33554432*(2x4^1)_NN_d
Tuning for shape 1x16777216*(2x4^2)_NN_d
Tuning for shape 1x8388608*(2x4^3)_NN_d
Tuning for shape 1x4194304*(2x4^4)_NN_d
Tuning for shape 1x2097152*(2x4^5)_NN_d
Tuning for shape 1x1048576*(2x4^6)_NN_d
Tuning for shape 1x524288*(2x4^7)_NN_d
Tuning for shape 1x262144*(2x4^8)_NN_d
Tuning for shape 1x131072*(2x4^9)_NN_d
Tuning for shape 1x65536*(2x4^10)_NN_d
Tuning for shape 1x32768*(2x4^11)_NN_d
Tuning for shape 1x16384*(2x4^12)_NN_d
Tuning for shape 1x8192*(2x4^13)_NN_d
Tuning for shape 1x16777216*(2x4^1)_NN_d
Tuning for shape 1x8388608*(2x4^2)_NN_d
Tuning for shape 1x4194304*(2x4^3)_NN_d
Tuning for shape 1x2097152*(2x4^4)_NN_d
Tuning for shape 1x1048576*(2x4^5)_NN_d
Tuning for shape 1x524288*(2x4^6)_NN_d
Tuning for shape 1x262144*(2x4^7)_NN_d
Tuning for shape 1x131072*(2x4^8)_NN_d
Tuning for shape 1x65536*(2x4^9)_NN_d
Tuning for shape 1x32768*(2x4^10)_NN_d
Tuning for shape 1x16384*(2x4^11)_NN_d
Tuning for shape 1x8192*(2x4^12)_NN_d
Tuning for shape 1x8388608*(2x4^1)_NN_d
Tuning for shape 1x4194304*(2x4^2)_NN_d
Tuning for shape 1x2097152*(2x4^3)_NN_d
Tuning for shape 1x1048576*(2x4^4)_NN_d
Tuning for shape 1x524288*(2x4^5)_NN_d
Tuning for shape 1x262144*(2x4^6)_NN_d
Tuning for shape 1x131072*(2x4^7)_NN_d
Tuning for shape 1x65536*(2x4^8)_NN_d
Tuning for shape 1x32768*(2x4^9)_NN_d
Tuning for shape 1x16384*(2x4^10)_NN_d
Tuning for shape 1x8192*(2x4^11)_NN_d
Tuning for shape 1x4194304*(2x4^1)_NN_d
Tuning for shape 1x2097152*(2x4^2)_NN_d
Tuning for shape 1x1048576*(2x4^3)_NN_d
Tuning for shape 1x524288*(2x4^4)_NN_d
Tuning for shape 1x262144*(2x4^5)_NN_d
Tuning for shape 1x131072*(2x4^6)_NN_d
Tuning for shape 1x65536*(2x4^7)_NN_d
Tuning for shape 1x32768*(2x4^8)_NN_d
Tuning for shape 1x16384*(2x4^9)_NN_d
Tuning for shape 1x8192*(2x4^10)_NN_d
Tuning for shape 1x2097152*(2x4^1)_NN_d
Tuning for shape 1x1048576*(2x4^2)_NN_d
Tuning for shape 1x524288*(2x4^3)_NN_d
Tuning for shape 1x262144*(2x4^4)_NN_d
Tuning for shape 1x131072*(2x4^5)_NN_d
Tuning for shape 1x65536*(2x4^6)_NN_d
Tuning for shape 1x32768*(2x4^7)_NN_d
Tuning for shape 1x16384*(2x4^8)_NN_d
Tuning for shape 1x8192*(2x4^9)_NN_d
Tuning for shape 1x1048576*(2x4^1)_NN_d
Tuning for shape 1x524288*(2x4^2)_NN_d
Tuning for shape 1x262144*(2x4^3)_NN_d
Tuning for shape 1x131072*(2x4^4)_NN_d
Tuning for shape 1x65536*(2x4^5)_NN_d
Tuning for shape 1x32768*(2x4^6)_NN_d
Tuning for shape 1x16384*(2x4^7)_NN_d
Tuning for shape 1x8192*(2x4^8)_NN_d
Tuning for shape 1x524288*(2x4^1)_NN_d
Tuning for shape 1x262144*(2x4^2)_NN_d
Tuning for shape 1x131072*(2x4^3)_NN_d
Tuning for shape 1x65536*(2x4^4)_NN_d
Tuning for shape 1x32768*(2x4^5)_NN_d
Tuning for shape 1x16384*(2x4^6)_NN_d
Tuning for shape 1x8192*(2x4^7)_NN_d
Tuning for shape 1x262144*(2x4^1)_NN_d
Tuning for shape 1x131072*(2x4^2)_NN_d
Tuning for shape 1x65536*(2x4^3)_NN_d
Tuning for shape 1x32768*(2x4^4)_NN_d
Tuning for shape 1x16384*(2x4^5)_NN_d
Tuning for shape 1x8192*(2x4^6)_NN_d
Tuning for shape 1x131072*(2x4^1)_NN_d
Tuning for shape 1x65536*(2x4^2)_NN_d
Tuning for shape 1x32768*(2x4^3)_NN_d
Tuning for shape 1x16384*(2x4^4)_NN_d
Tuning for shape 1x8192*(2x4^5)_NN_d
Tuning for shape 1x65536*(2x4^1)_NN_d
Tuning for shape 1x32768*(2x4^2)_NN_d
Tuning for shape 1x16384*(2x4^3)_NN_d
Tuning for shape 1x8192*(2x4^4)_NN_d
Tuning for shape 1x32768*(2x4^1)_NN_d
Tuning for shape 1x16384*(2x4^2)_NN_d
Tuning for shape 1x8192*(2x4^3)_NN_d
Tuning for shape 1x16384*(2x4^1)_NN_d
Tuning for shape 1x8192*(2x4^2)_NN_d
Tuning for shape 1x8192*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^13  &  1.000 & 1.000 & 36.348 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 13 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8192] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x F_11 [2, 4] x F_12 [2, 4] x to produce Y[4, 67108864]
Matmul: 4 x 67108864 x 8192, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x33554432*(2x4^1)_NN_d
Tuning for shape 4x16777216*(2x4^2)_NN_d
Tuning for shape 4x8388608*(2x4^3)_NN_d
Tuning for shape 4x4194304*(2x4^4)_NN_d
Tuning for shape 4x2097152*(2x4^5)_NN_d
Tuning for shape 4x1048576*(2x4^6)_NN_d
Tuning for shape 4x524288*(2x4^7)_NN_d
Tuning for shape 4x262144*(2x4^8)_NN_d
Tuning for shape 4x131072*(2x4^9)_NN_d
Tuning for shape 4x65536*(2x4^10)_NN_d
Tuning for shape 4x32768*(2x4^11)_NN_d
Tuning for shape 4x16384*(2x4^12)_NN_d
Tuning for shape 4x8192*(2x4^13)_NN_d
Tuning for shape 4x16777216*(2x4^1)_NN_d
Tuning for shape 4x8388608*(2x4^2)_NN_d
Tuning for shape 4x4194304*(2x4^3)_NN_d
Tuning for shape 4x2097152*(2x4^4)_NN_d
Tuning for shape 4x1048576*(2x4^5)_NN_d
Tuning for shape 4x524288*(2x4^6)_NN_d
Tuning for shape 4x262144*(2x4^7)_NN_d
Tuning for shape 4x131072*(2x4^8)_NN_d
Tuning for shape 4x65536*(2x4^9)_NN_d
Tuning for shape 4x32768*(2x4^10)_NN_d
Tuning for shape 4x16384*(2x4^11)_NN_d
Tuning for shape 4x8192*(2x4^12)_NN_d
Tuning for shape 4x8388608*(2x4^1)_NN_d
Tuning for shape 4x4194304*(2x4^2)_NN_d
Tuning for shape 4x2097152*(2x4^3)_NN_d
Tuning for shape 4x1048576*(2x4^4)_NN_d
Tuning for shape 4x524288*(2x4^5)_NN_d
Tuning for shape 4x262144*(2x4^6)_NN_d
Tuning for shape 4x131072*(2x4^7)_NN_d
Tuning for shape 4x65536*(2x4^8)_NN_d
Tuning for shape 4x32768*(2x4^9)_NN_d
Tuning for shape 4x16384*(2x4^10)_NN_d
Tuning for shape 4x8192*(2x4^11)_NN_d
Tuning for shape 4x4194304*(2x4^1)_NN_d
Tuning for shape 4x2097152*(2x4^2)_NN_d
Tuning for shape 4x1048576*(2x4^3)_NN_d
Tuning for shape 4x524288*(2x4^4)_NN_d
Tuning for shape 4x262144*(2x4^5)_NN_d
Tuning for shape 4x131072*(2x4^6)_NN_d
Tuning for shape 4x65536*(2x4^7)_NN_d
Tuning for shape 4x32768*(2x4^8)_NN_d
Tuning for shape 4x16384*(2x4^9)_NN_d
Tuning for shape 4x8192*(2x4^10)_NN_d
Tuning for shape 4x2097152*(2x4^1)_NN_d
Tuning for shape 4x1048576*(2x4^2)_NN_d
Tuning for shape 4x524288*(2x4^3)_NN_d
Tuning for shape 4x262144*(2x4^4)_NN_d
Tuning for shape 4x131072*(2x4^5)_NN_d
Tuning for shape 4x65536*(2x4^6)_NN_d
Tuning for shape 4x32768*(2x4^7)_NN_d
Tuning for shape 4x16384*(2x4^8)_NN_d
Tuning for shape 4x8192*(2x4^9)_NN_d
Tuning for shape 4x1048576*(2x4^1)_NN_d
Tuning for shape 4x524288*(2x4^2)_NN_d
Tuning for shape 4x262144*(2x4^3)_NN_d
Tuning for shape 4x131072*(2x4^4)_NN_d
Tuning for shape 4x65536*(2x4^5)_NN_d
Tuning for shape 4x32768*(2x4^6)_NN_d
Tuning for shape 4x16384*(2x4^7)_NN_d
Tuning for shape 4x8192*(2x4^8)_NN_d
Tuning for shape 4x524288*(2x4^1)_NN_d
Tuning for shape 4x262144*(2x4^2)_NN_d
Tuning for shape 4x131072*(2x4^3)_NN_d
Tuning for shape 4x65536*(2x4^4)_NN_d
Tuning for shape 4x32768*(2x4^5)_NN_d
Tuning for shape 4x16384*(2x4^6)_NN_d
Tuning for shape 4x8192*(2x4^7)_NN_d
Tuning for shape 4x262144*(2x4^1)_NN_d
Tuning for shape 4x131072*(2x4^2)_NN_d
Tuning for shape 4x65536*(2x4^3)_NN_d
Tuning for shape 4x32768*(2x4^4)_NN_d
Tuning for shape 4x16384*(2x4^5)_NN_d
Tuning for shape 4x8192*(2x4^6)_NN_d
Tuning for shape 4x131072*(2x4^1)_NN_d
Tuning for shape 4x65536*(2x4^2)_NN_d
Tuning for shape 4x32768*(2x4^3)_NN_d
Tuning for shape 4x16384*(2x4^4)_NN_d
Tuning for shape 4x8192*(2x4^5)_NN_d
Tuning for shape 4x65536*(2x4^1)_NN_d
Tuning for shape 4x32768*(2x4^2)_NN_d
Tuning for shape 4x16384*(2x4^3)_NN_d
Tuning for shape 4x8192*(2x4^4)_NN_d
Tuning for shape 4x32768*(2x4^1)_NN_d
Tuning for shape 4x16384*(2x4^2)_NN_d
Tuning for shape 4x8192*(2x4^3)_NN_d
Tuning for shape 4x16384*(2x4^1)_NN_d
Tuning for shape 4x8192*(2x4^2)_NN_d
Tuning for shape 4x8192*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x4^13  &  1.000 & 1.000 & 36.652 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 14 -p 2 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16384] with F_0 [2, 4] x F_1 [2, 4] x F_2 [2, 4] x F_3 [2, 4] x F_4 [2, 4] x F_5 [2, 4] x F_6 [2, 4] x F_7 [2, 4] x F_8 [2, 4] x F_9 [2, 4] x F_10 [2, 4] x F_11 [2, 4] x F_12 [2, 4] x F_13 [2, 4] x to produce Y[1, 268435456]
Matmul: 1 x 268435456 x 16384, Num KP Factors: 14
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x134217728*(2x4^1)_NN_d
Tuning for shape 1x67108864*(2x4^2)_NN_d
Tuning for shape 1x33554432*(2x4^3)_NN_d
Tuning for shape 1x16777216*(2x4^4)_NN_d
Tuning for shape 1x8388608*(2x4^5)_NN_d
Tuning for shape 1x4194304*(2x4^6)_NN_d
Tuning for shape 1x2097152*(2x4^7)_NN_d
Tuning for shape 1x1048576*(2x4^8)_NN_d
Tuning for shape 1x524288*(2x4^9)_NN_d
Tuning for shape 1x262144*(2x4^10)_NN_d
Tuning for shape 1x131072*(2x4^11)_NN_d
Tuning for shape 1x65536*(2x4^12)_NN_d
Tuning for shape 1x32768*(2x4^13)_NN_d
Tuning for shape 1x16384*(2x4^14)_NN_d
Tuning for shape 1x67108864*(2x4^1)_NN_d
Tuning for shape 1x33554432*(2x4^2)_NN_d
Tuning for shape 1x16777216*(2x4^3)_NN_d
Tuning for shape 1x8388608*(2x4^4)_NN_d
Tuning for shape 1x4194304*(2x4^5)_NN_d
Tuning for shape 1x2097152*(2x4^6)_NN_d
Tuning for shape 1x1048576*(2x4^7)_NN_d
Tuning for shape 1x524288*(2x4^8)_NN_d
Tuning for shape 1x262144*(2x4^9)_NN_d
Tuning for shape 1x131072*(2x4^10)_NN_d
Tuning for shape 1x65536*(2x4^11)_NN_d
Tuning for shape 1x32768*(2x4^12)_NN_d
Tuning for shape 1x16384*(2x4^13)_NN_d
Tuning for shape 1x33554432*(2x4^1)_NN_d
Tuning for shape 1x16777216*(2x4^2)_NN_d
Tuning for shape 1x8388608*(2x4^3)_NN_d
Tuning for shape 1x4194304*(2x4^4)_NN_d
Tuning for shape 1x2097152*(2x4^5)_NN_d
Tuning for shape 1x1048576*(2x4^6)_NN_d
Tuning for shape 1x524288*(2x4^7)_NN_d
Tuning for shape 1x262144*(2x4^8)_NN_d
Tuning for shape 1x131072*(2x4^9)_NN_d
Tuning for shape 1x65536*(2x4^10)_NN_d
Tuning for shape 1x32768*(2x4^11)_NN_d
Tuning for shape 1x16384*(2x4^12)_NN_d
Tuning for shape 1x16777216*(2x4^1)_NN_d
Tuning for shape 1x8388608*(2x4^2)_NN_d
Tuning for shape 1x4194304*(2x4^3)_NN_d
Tuning for shape 1x2097152*(2x4^4)_NN_d
Tuning for shape 1x1048576*(2x4^5)_NN_d
Tuning for shape 1x524288*(2x4^6)_NN_d
Tuning for shape 1x262144*(2x4^7)_NN_d
Tuning for shape 1x131072*(2x4^8)_NN_d
Tuning for shape 1x65536*(2x4^9)_NN_d
Tuning for shape 1x32768*(2x4^10)_NN_d
Tuning for shape 1x16384*(2x4^11)_NN_d
Tuning for shape 1x8388608*(2x4^1)_NN_d
Tuning for shape 1x4194304*(2x4^2)_NN_d
Tuning for shape 1x2097152*(2x4^3)_NN_d
Tuning for shape 1x1048576*(2x4^4)_NN_d
Tuning for shape 1x524288*(2x4^5)_NN_d
Tuning for shape 1x262144*(2x4^6)_NN_d
Tuning for shape 1x131072*(2x4^7)_NN_d
Tuning for shape 1x65536*(2x4^8)_NN_d
Tuning for shape 1x32768*(2x4^9)_NN_d
Tuning for shape 1x16384*(2x4^10)_NN_d
Tuning for shape 1x4194304*(2x4^1)_NN_d
Tuning for shape 1x2097152*(2x4^2)_NN_d
Tuning for shape 1x1048576*(2x4^3)_NN_d
Tuning for shape 1x524288*(2x4^4)_NN_d
Tuning for shape 1x262144*(2x4^5)_NN_d
Tuning for shape 1x131072*(2x4^6)_NN_d
Tuning for shape 1x65536*(2x4^7)_NN_d
Tuning for shape 1x32768*(2x4^8)_NN_d
Tuning for shape 1x16384*(2x4^9)_NN_d
Tuning for shape 1x2097152*(2x4^1)_NN_d
Tuning for shape 1x1048576*(2x4^2)_NN_d
Tuning for shape 1x524288*(2x4^3)_NN_d
Tuning for shape 1x262144*(2x4^4)_NN_d
Tuning for shape 1x131072*(2x4^5)_NN_d
Tuning for shape 1x65536*(2x4^6)_NN_d
Tuning for shape 1x32768*(2x4^7)_NN_d
Tuning for shape 1x16384*(2x4^8)_NN_d
Tuning for shape 1x1048576*(2x4^1)_NN_d
Tuning for shape 1x524288*(2x4^2)_NN_d
Tuning for shape 1x262144*(2x4^3)_NN_d
Tuning for shape 1x131072*(2x4^4)_NN_d
Tuning for shape 1x65536*(2x4^5)_NN_d
Tuning for shape 1x32768*(2x4^6)_NN_d
Tuning for shape 1x16384*(2x4^7)_NN_d
Tuning for shape 1x524288*(2x4^1)_NN_d
Tuning for shape 1x262144*(2x4^2)_NN_d
Tuning for shape 1x131072*(2x4^3)_NN_d
Tuning for shape 1x65536*(2x4^4)_NN_d
Tuning for shape 1x32768*(2x4^5)_NN_d
Tuning for shape 1x16384*(2x4^6)_NN_d
Tuning for shape 1x262144*(2x4^1)_NN_d
Tuning for shape 1x131072*(2x4^2)_NN_d
Tuning for shape 1x65536*(2x4^3)_NN_d
Tuning for shape 1x32768*(2x4^4)_NN_d
Tuning for shape 1x16384*(2x4^5)_NN_d
Tuning for shape 1x131072*(2x4^1)_NN_d
Tuning for shape 1x65536*(2x4^2)_NN_d
Tuning for shape 1x32768*(2x4^3)_NN_d
Tuning for shape 1x16384*(2x4^4)_NN_d
Tuning for shape 1x65536*(2x4^1)_NN_d
Tuning for shape 1x32768*(2x4^2)_NN_d
Tuning for shape 1x16384*(2x4^3)_NN_d
Tuning for shape 1x32768*(2x4^1)_NN_d
Tuning for shape 1x16384*(2x4^2)_NN_d
Tuning for shape 1x16384*(2x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x4^14  &  1.000 & 1.000 & 36.681 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2] with F_0 [2, 8] x to produce Y[1, 8]
Matmul: 1 x 8 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x8^1  &  1.000 & 1.000 & 0.000 & 8374.825
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2] with F_0 [2, 8] x to produce Y[4, 8]
Matmul: 4 x 8 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x8^1  &  1.000 & 1.000 & 0.000 & 2462.231
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2] with F_0 [2, 8] x to produce Y[16, 8]
Matmul: 16 x 8 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x8^1  &  1.000 & 1.000 & 0.002 & 642.193
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2] with F_0 [2, 8] x to produce Y[64, 8]
Matmul: 64 x 8 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x8^1  &  1.000 & 1.000 & 0.005 & 182.306
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2] with F_0 [2, 8] x to produce Y[256, 8]
Matmul: 256 x 8 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x8^1  &  1.000 & 1.000 & 0.027 & 37.566
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 2] with F_0 [2, 8] x to produce Y[1024, 8]
Matmul: 1024 x 8 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x8^1  &  1.000 & 1.000 & 0.105 & 9.504
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [2, 8] x F_1 [2, 8] x to produce Y[1, 64]
Matmul: 1 x 64 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16*(2x8^1)_NN_d
Tuning for shape 1x4*(2x8^2)_NN_d
Tuning for shape 1x4*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x8^2  &  1.000 & 1.000 & 0.001 & 1310.296
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [2, 8] x F_1 [2, 8] x to produce Y[4, 64]
Matmul: 4 x 64 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16*(2x8^1)_NN_d
Tuning for shape 4x4*(2x8^2)_NN_d
Tuning for shape 4x4*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x8^2  &  1.000 & 1.000 & 0.003 & 384.850
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [2, 8] x F_1 [2, 8] x to produce Y[16, 64]
Matmul: 16 x 64 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16*(2x8^1)_NN_d
Tuning for shape 16x4*(2x8^2)_NN_d
Tuning for shape 16x4*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x8^2  &  1.000 & 1.000 & 0.011 & 88.450
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [2, 8] x F_1 [2, 8] x to produce Y[64, 64]
Matmul: 64 x 64 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16*(2x8^1)_NN_d
Tuning for shape 64x4*(2x8^2)_NN_d
Tuning for shape 64x4*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x8^2  &  1.000 & 1.000 & 0.045 & 22.422
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [2, 8] x F_1 [2, 8] x to produce Y[256, 64]
Matmul: 256 x 64 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16*(2x8^1)_NN_d
Tuning for shape 256x4*(2x8^2)_NN_d
Tuning for shape 256x4*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x8^2  &  1.000 & 1.000 & 0.181 & 5.526
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [2, 8] x F_1 [2, 8] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16*(2x8^1)_NN_d
Tuning for shape 1024x4*(2x8^2)_NN_d
Tuning for shape 1024x4*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x8^2  &  1.000 & 1.000 & 0.720 & 1.389
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x to produce Y[1, 512]
Matmul: 1 x 512 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(2x8^1)_NN_d
Tuning for shape 1x32*(2x8^2)_NN_d
Tuning for shape 1x8*(2x8^3)_NN_d
Tuning for shape 1x32*(2x8^1)_NN_d
Tuning for shape 1x8*(2x8^2)_NN_d
Tuning for shape 1x8*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x8^3  &  1.000 & 1.000 & 0.005 & 197.241
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x to produce Y[4, 512]
Matmul: 4 x 512 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(2x8^1)_NN_d
Tuning for shape 4x32*(2x8^2)_NN_d
Tuning for shape 4x8*(2x8^3)_NN_d
Tuning for shape 4x32*(2x8^1)_NN_d
Tuning for shape 4x8*(2x8^2)_NN_d
Tuning for shape 4x8*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x8^3  &  1.000 & 1.000 & 0.019 & 52.417
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x to produce Y[16, 512]
Matmul: 16 x 512 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(2x8^1)_NN_d
Tuning for shape 16x32*(2x8^2)_NN_d
Tuning for shape 16x8*(2x8^3)_NN_d
Tuning for shape 16x32*(2x8^1)_NN_d
Tuning for shape 16x8*(2x8^2)_NN_d
Tuning for shape 16x8*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x8^3  &  1.000 & 1.000 & 0.079 & 12.669
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x to produce Y[64, 512]
Matmul: 64 x 512 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(2x8^1)_NN_d
Tuning for shape 64x32*(2x8^2)_NN_d
Tuning for shape 64x8*(2x8^3)_NN_d
Tuning for shape 64x32*(2x8^1)_NN_d
Tuning for shape 64x8*(2x8^2)_NN_d
Tuning for shape 64x8*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x8^3  &  1.000 & 1.000 & 0.263 & 3.797
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x to produce Y[256, 512]
Matmul: 256 x 512 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(2x8^1)_NN_d
Tuning for shape 256x32*(2x8^2)_NN_d
Tuning for shape 256x8*(2x8^3)_NN_d
Tuning for shape 256x32*(2x8^1)_NN_d
Tuning for shape 256x8*(2x8^2)_NN_d
Tuning for shape 256x8*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x8^3  &  1.000 & 1.000 & 1.193 & 0.838
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x to produce Y[1024, 512]
Matmul: 1024 x 512 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(2x8^1)_NN_d
Tuning for shape 1024x32*(2x8^2)_NN_d
Tuning for shape 1024x8*(2x8^3)_NN_d
Tuning for shape 1024x32*(2x8^1)_NN_d
Tuning for shape 1024x8*(2x8^2)_NN_d
Tuning for shape 1024x8*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x8^3  &  1.000 & 1.000 & 4.946 & 0.202
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(2x8^1)_NN_d
Tuning for shape 1x256*(2x8^2)_NN_d
Tuning for shape 1x64*(2x8^3)_NN_d
Tuning for shape 1x16*(2x8^4)_NN_d
Tuning for shape 1x256*(2x8^1)_NN_d
Tuning for shape 1x64*(2x8^2)_NN_d
Tuning for shape 1x16*(2x8^3)_NN_d
Tuning for shape 1x64*(2x8^1)_NN_d
Tuning for shape 1x16*(2x8^2)_NN_d
Tuning for shape 1x16*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x8^4  &  1.000 & 1.000 & 0.035 & 28.234
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1024*(2x8^1)_NN_d
Tuning for shape 4x256*(2x8^2)_NN_d
Tuning for shape 4x64*(2x8^3)_NN_d
Tuning for shape 4x16*(2x8^4)_NN_d
Tuning for shape 4x256*(2x8^1)_NN_d
Tuning for shape 4x64*(2x8^2)_NN_d
Tuning for shape 4x16*(2x8^3)_NN_d
Tuning for shape 4x64*(2x8^1)_NN_d
Tuning for shape 4x16*(2x8^2)_NN_d
Tuning for shape 4x16*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x8^4  &  1.000 & 1.000 & 0.131 & 7.659
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1024*(2x8^1)_NN_d
Tuning for shape 16x256*(2x8^2)_NN_d
Tuning for shape 16x64*(2x8^3)_NN_d
Tuning for shape 16x16*(2x8^4)_NN_d
Tuning for shape 16x256*(2x8^1)_NN_d
Tuning for shape 16x64*(2x8^2)_NN_d
Tuning for shape 16x16*(2x8^3)_NN_d
Tuning for shape 16x64*(2x8^1)_NN_d
Tuning for shape 16x16*(2x8^2)_NN_d
Tuning for shape 16x16*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x8^4  &  1.000 & 1.000 & 0.385 & 2.596
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1024*(2x8^1)_NN_d
Tuning for shape 64x256*(2x8^2)_NN_d
Tuning for shape 64x64*(2x8^3)_NN_d
Tuning for shape 64x16*(2x8^4)_NN_d
Tuning for shape 64x256*(2x8^1)_NN_d
Tuning for shape 64x64*(2x8^2)_NN_d
Tuning for shape 64x16*(2x8^3)_NN_d
Tuning for shape 64x64*(2x8^1)_NN_d
Tuning for shape 64x16*(2x8^2)_NN_d
Tuning for shape 64x16*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x8^4  &  1.000 & 1.000 & 2.032 & 0.492
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1024*(2x8^1)_NN_d
Tuning for shape 256x256*(2x8^2)_NN_d
Tuning for shape 256x64*(2x8^3)_NN_d
Tuning for shape 256x16*(2x8^4)_NN_d
Tuning for shape 256x256*(2x8^1)_NN_d
Tuning for shape 256x64*(2x8^2)_NN_d
Tuning for shape 256x16*(2x8^3)_NN_d
Tuning for shape 256x64*(2x8^1)_NN_d
Tuning for shape 256x16*(2x8^2)_NN_d
Tuning for shape 256x16*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x8^4  &  1.000 & 1.000 & 8.327 & 0.120
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x1024*(2x8^1)_NN_d
Tuning for shape 1024x256*(2x8^2)_NN_d
Tuning for shape 1024x64*(2x8^3)_NN_d
Tuning for shape 1024x16*(2x8^4)_NN_d
Tuning for shape 1024x256*(2x8^1)_NN_d
Tuning for shape 1024x64*(2x8^2)_NN_d
Tuning for shape 1024x16*(2x8^3)_NN_d
Tuning for shape 1024x64*(2x8^1)_NN_d
Tuning for shape 1024x16*(2x8^2)_NN_d
Tuning for shape 1024x16*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x8^4  &  1.000 & 1.000 & 32.768 & 0.031
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x to produce Y[1, 32768]
Matmul: 1 x 32768 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8192*(2x8^1)_NN_d
Tuning for shape 1x2048*(2x8^2)_NN_d
Tuning for shape 1x512*(2x8^3)_NN_d
Tuning for shape 1x128*(2x8^4)_NN_d
Tuning for shape 1x32*(2x8^5)_NN_d
Tuning for shape 1x2048*(2x8^1)_NN_d
Tuning for shape 1x512*(2x8^2)_NN_d
Tuning for shape 1x128*(2x8^3)_NN_d
Tuning for shape 1x32*(2x8^4)_NN_d
Tuning for shape 1x512*(2x8^1)_NN_d
Tuning for shape 1x128*(2x8^2)_NN_d
Tuning for shape 1x32*(2x8^3)_NN_d
Tuning for shape 1x128*(2x8^1)_NN_d
Tuning for shape 1x32*(2x8^2)_NN_d
Tuning for shape 1x32*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x8^5  &  1.000 & 1.000 & 0.236 & 4.235
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x to produce Y[4, 32768]
Matmul: 4 x 32768 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8192*(2x8^1)_NN_d
Tuning for shape 4x2048*(2x8^2)_NN_d
Tuning for shape 4x512*(2x8^3)_NN_d
Tuning for shape 4x128*(2x8^4)_NN_d
Tuning for shape 4x32*(2x8^5)_NN_d
Tuning for shape 4x2048*(2x8^1)_NN_d
Tuning for shape 4x512*(2x8^2)_NN_d
Tuning for shape 4x128*(2x8^3)_NN_d
Tuning for shape 4x32*(2x8^4)_NN_d
Tuning for shape 4x512*(2x8^1)_NN_d
Tuning for shape 4x128*(2x8^2)_NN_d
Tuning for shape 4x32*(2x8^3)_NN_d
Tuning for shape 4x128*(2x8^1)_NN_d
Tuning for shape 4x32*(2x8^2)_NN_d
Tuning for shape 4x32*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x8^5  &  1.000 & 1.000 & 0.904 & 1.106
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x to produce Y[16, 32768]
Matmul: 16 x 32768 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8192*(2x8^1)_NN_d
Tuning for shape 16x2048*(2x8^2)_NN_d
Tuning for shape 16x512*(2x8^3)_NN_d
Tuning for shape 16x128*(2x8^4)_NN_d
Tuning for shape 16x32*(2x8^5)_NN_d
Tuning for shape 16x2048*(2x8^1)_NN_d
Tuning for shape 16x512*(2x8^2)_NN_d
Tuning for shape 16x128*(2x8^3)_NN_d
Tuning for shape 16x32*(2x8^4)_NN_d
Tuning for shape 16x512*(2x8^1)_NN_d
Tuning for shape 16x128*(2x8^2)_NN_d
Tuning for shape 16x32*(2x8^3)_NN_d
Tuning for shape 16x128*(2x8^1)_NN_d
Tuning for shape 16x32*(2x8^2)_NN_d
Tuning for shape 16x32*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x8^5  &  1.000 & 1.000 & 3.577 & 0.280
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 32] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x to produce Y[64, 32768]
Matmul: 64 x 32768 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8192*(2x8^1)_NN_d
Tuning for shape 64x2048*(2x8^2)_NN_d
Tuning for shape 64x512*(2x8^3)_NN_d
Tuning for shape 64x128*(2x8^4)_NN_d
Tuning for shape 64x32*(2x8^5)_NN_d
Tuning for shape 64x2048*(2x8^1)_NN_d
Tuning for shape 64x512*(2x8^2)_NN_d
Tuning for shape 64x128*(2x8^3)_NN_d
Tuning for shape 64x32*(2x8^4)_NN_d
Tuning for shape 64x512*(2x8^1)_NN_d
Tuning for shape 64x128*(2x8^2)_NN_d
Tuning for shape 64x32*(2x8^3)_NN_d
Tuning for shape 64x128*(2x8^1)_NN_d
Tuning for shape 64x32*(2x8^2)_NN_d
Tuning for shape 64x32*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x8^5  &  1.000 & 1.000 & 14.533 & 0.069
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 32] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x to produce Y[256, 32768]
Matmul: 256 x 32768 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8192*(2x8^1)_NN_d
Tuning for shape 256x2048*(2x8^2)_NN_d
Tuning for shape 256x512*(2x8^3)_NN_d
Tuning for shape 256x128*(2x8^4)_NN_d
Tuning for shape 256x32*(2x8^5)_NN_d
Tuning for shape 256x2048*(2x8^1)_NN_d
Tuning for shape 256x512*(2x8^2)_NN_d
Tuning for shape 256x128*(2x8^3)_NN_d
Tuning for shape 256x32*(2x8^4)_NN_d
Tuning for shape 256x512*(2x8^1)_NN_d
Tuning for shape 256x128*(2x8^2)_NN_d
Tuning for shape 256x32*(2x8^3)_NN_d
Tuning for shape 256x128*(2x8^1)_NN_d
Tuning for shape 256x32*(2x8^2)_NN_d
Tuning for shape 256x32*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x8^5  &  1.000 & 1.000 & 52.341 & 0.019
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 5 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 32] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x to produce Y[1024, 32768]
Matmul: 1024 x 32768 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8192*(2x8^1)_NN_d
Tuning for shape 1024x2048*(2x8^2)_NN_d
Tuning for shape 1024x512*(2x8^3)_NN_d
Tuning for shape 1024x128*(2x8^4)_NN_d
Tuning for shape 1024x32*(2x8^5)_NN_d
Tuning for shape 1024x2048*(2x8^1)_NN_d
Tuning for shape 1024x512*(2x8^2)_NN_d
Tuning for shape 1024x128*(2x8^3)_NN_d
Tuning for shape 1024x32*(2x8^4)_NN_d
Tuning for shape 1024x512*(2x8^1)_NN_d
Tuning for shape 1024x128*(2x8^2)_NN_d
Tuning for shape 1024x32*(2x8^3)_NN_d
Tuning for shape 1024x128*(2x8^1)_NN_d
Tuning for shape 1024x32*(2x8^2)_NN_d
Tuning for shape 1024x32*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x8^5  &  1.000 & 1.000 & 56.921 & 0.018
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x65536*(2x8^1)_NN_d
Tuning for shape 1x16384*(2x8^2)_NN_d
Tuning for shape 1x4096*(2x8^3)_NN_d
Tuning for shape 1x1024*(2x8^4)_NN_d
Tuning for shape 1x256*(2x8^5)_NN_d
Tuning for shape 1x64*(2x8^6)_NN_d
Tuning for shape 1x16384*(2x8^1)_NN_d
Tuning for shape 1x4096*(2x8^2)_NN_d
Tuning for shape 1x1024*(2x8^3)_NN_d
Tuning for shape 1x256*(2x8^4)_NN_d
Tuning for shape 1x64*(2x8^5)_NN_d
Tuning for shape 1x4096*(2x8^1)_NN_d
Tuning for shape 1x1024*(2x8^2)_NN_d
Tuning for shape 1x256*(2x8^3)_NN_d
Tuning for shape 1x64*(2x8^4)_NN_d
Tuning for shape 1x1024*(2x8^1)_NN_d
Tuning for shape 1x256*(2x8^2)_NN_d
Tuning for shape 1x64*(2x8^3)_NN_d
Tuning for shape 1x256*(2x8^1)_NN_d
Tuning for shape 1x64*(2x8^2)_NN_d
Tuning for shape 1x64*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x8^6  &  1.000 & 1.000 & 1.687 & 0.593
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x65536*(2x8^1)_NN_d
Tuning for shape 4x16384*(2x8^2)_NN_d
Tuning for shape 4x4096*(2x8^3)_NN_d
Tuning for shape 4x1024*(2x8^4)_NN_d
Tuning for shape 4x256*(2x8^5)_NN_d
Tuning for shape 4x64*(2x8^6)_NN_d
Tuning for shape 4x16384*(2x8^1)_NN_d
Tuning for shape 4x4096*(2x8^2)_NN_d
Tuning for shape 4x1024*(2x8^3)_NN_d
Tuning for shape 4x256*(2x8^4)_NN_d
Tuning for shape 4x64*(2x8^5)_NN_d
Tuning for shape 4x4096*(2x8^1)_NN_d
Tuning for shape 4x1024*(2x8^2)_NN_d
Tuning for shape 4x256*(2x8^3)_NN_d
Tuning for shape 4x64*(2x8^4)_NN_d
Tuning for shape 4x1024*(2x8^1)_NN_d
Tuning for shape 4x256*(2x8^2)_NN_d
Tuning for shape 4x64*(2x8^3)_NN_d
Tuning for shape 4x256*(2x8^1)_NN_d
Tuning for shape 4x64*(2x8^2)_NN_d
Tuning for shape 4x64*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x8^6  &  1.000 & 1.000 & 6.542 & 0.153
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x65536*(2x8^1)_NN_d
Tuning for shape 16x16384*(2x8^2)_NN_d
Tuning for shape 16x4096*(2x8^3)_NN_d
Tuning for shape 16x1024*(2x8^4)_NN_d
Tuning for shape 16x256*(2x8^5)_NN_d
Tuning for shape 16x64*(2x8^6)_NN_d
Tuning for shape 16x16384*(2x8^1)_NN_d
Tuning for shape 16x4096*(2x8^2)_NN_d
Tuning for shape 16x1024*(2x8^3)_NN_d
Tuning for shape 16x256*(2x8^4)_NN_d
Tuning for shape 16x64*(2x8^5)_NN_d
Tuning for shape 16x4096*(2x8^1)_NN_d
Tuning for shape 16x1024*(2x8^2)_NN_d
Tuning for shape 16x256*(2x8^3)_NN_d
Tuning for shape 16x64*(2x8^4)_NN_d
Tuning for shape 16x1024*(2x8^1)_NN_d
Tuning for shape 16x256*(2x8^2)_NN_d
Tuning for shape 16x64*(2x8^3)_NN_d
Tuning for shape 16x256*(2x8^1)_NN_d
Tuning for shape 16x64*(2x8^2)_NN_d
Tuning for shape 16x64*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x8^6  &  1.000 & 1.000 & 25.816 & 0.039
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 6 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x to produce Y[64, 262144]
Matmul: 64 x 262144 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x65536*(2x8^1)_NN_d
Tuning for shape 64x16384*(2x8^2)_NN_d
Tuning for shape 64x4096*(2x8^3)_NN_d
Tuning for shape 64x1024*(2x8^4)_NN_d
Tuning for shape 64x256*(2x8^5)_NN_d
Tuning for shape 64x64*(2x8^6)_NN_d
Tuning for shape 64x16384*(2x8^1)_NN_d
Tuning for shape 64x4096*(2x8^2)_NN_d
Tuning for shape 64x1024*(2x8^3)_NN_d
Tuning for shape 64x256*(2x8^4)_NN_d
Tuning for shape 64x64*(2x8^5)_NN_d
Tuning for shape 64x4096*(2x8^1)_NN_d
Tuning for shape 64x1024*(2x8^2)_NN_d
Tuning for shape 64x256*(2x8^3)_NN_d
Tuning for shape 64x64*(2x8^4)_NN_d
Tuning for shape 64x1024*(2x8^1)_NN_d
Tuning for shape 64x256*(2x8^2)_NN_d
Tuning for shape 64x64*(2x8^3)_NN_d
Tuning for shape 64x256*(2x8^1)_NN_d
Tuning for shape 64x64*(2x8^2)_NN_d
Tuning for shape 64x64*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x8^6  &  1.000 & 1.000 & 56.954 & 0.018
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 6 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x to produce Y[256, 262144]
Matmul: 256 x 262144 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x65536*(2x8^1)_NN_d
Tuning for shape 256x16384*(2x8^2)_NN_d
Tuning for shape 256x4096*(2x8^3)_NN_d
Tuning for shape 256x1024*(2x8^4)_NN_d
Tuning for shape 256x256*(2x8^5)_NN_d
Tuning for shape 256x64*(2x8^6)_NN_d
Tuning for shape 256x16384*(2x8^1)_NN_d
Tuning for shape 256x4096*(2x8^2)_NN_d
Tuning for shape 256x1024*(2x8^3)_NN_d
Tuning for shape 256x256*(2x8^4)_NN_d
Tuning for shape 256x64*(2x8^5)_NN_d
Tuning for shape 256x4096*(2x8^1)_NN_d
Tuning for shape 256x1024*(2x8^2)_NN_d
Tuning for shape 256x256*(2x8^3)_NN_d
Tuning for shape 256x64*(2x8^4)_NN_d
Tuning for shape 256x1024*(2x8^1)_NN_d
Tuning for shape 256x256*(2x8^2)_NN_d
Tuning for shape 256x64*(2x8^3)_NN_d
Tuning for shape 256x256*(2x8^1)_NN_d
Tuning for shape 256x64*(2x8^2)_NN_d
Tuning for shape 256x64*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x8^6  &  1.000 & 1.000 & 60.995 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 6 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x to produce Y[1024, 262144]
Matmul: 1024 x 262144 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x65536*(2x8^1)_NN_d
Tuning for shape 1024x16384*(2x8^2)_NN_d
Tuning for shape 1024x4096*(2x8^3)_NN_d
Tuning for shape 1024x1024*(2x8^4)_NN_d
Tuning for shape 1024x256*(2x8^5)_NN_d
Tuning for shape 1024x64*(2x8^6)_NN_d
Tuning for shape 1024x16384*(2x8^1)_NN_d
Tuning for shape 1024x4096*(2x8^2)_NN_d
Tuning for shape 1024x1024*(2x8^3)_NN_d
Tuning for shape 1024x256*(2x8^4)_NN_d
Tuning for shape 1024x64*(2x8^5)_NN_d
Tuning for shape 1024x4096*(2x8^1)_NN_d
Tuning for shape 1024x1024*(2x8^2)_NN_d
Tuning for shape 1024x256*(2x8^3)_NN_d
Tuning for shape 1024x64*(2x8^4)_NN_d
Tuning for shape 1024x1024*(2x8^1)_NN_d
Tuning for shape 1024x256*(2x8^2)_NN_d
Tuning for shape 1024x64*(2x8^3)_NN_d
Tuning for shape 1024x256*(2x8^1)_NN_d
Tuning for shape 1024x64*(2x8^2)_NN_d
Tuning for shape 1024x64*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x8^6  &  1.000 & 1.000 & 61.712 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 128] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x to produce Y[1, 2097152]
Matmul: 1 x 2097152 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x524288*(2x8^1)_NN_d
Tuning for shape 1x131072*(2x8^2)_NN_d
Tuning for shape 1x32768*(2x8^3)_NN_d
Tuning for shape 1x8192*(2x8^4)_NN_d
Tuning for shape 1x2048*(2x8^5)_NN_d
Tuning for shape 1x512*(2x8^6)_NN_d
Tuning for shape 1x128*(2x8^7)_NN_d
Tuning for shape 1x131072*(2x8^1)_NN_d
Tuning for shape 1x32768*(2x8^2)_NN_d
Tuning for shape 1x8192*(2x8^3)_NN_d
Tuning for shape 1x2048*(2x8^4)_NN_d
Tuning for shape 1x512*(2x8^5)_NN_d
Tuning for shape 1x128*(2x8^6)_NN_d
Tuning for shape 1x32768*(2x8^1)_NN_d
Tuning for shape 1x8192*(2x8^2)_NN_d
Tuning for shape 1x2048*(2x8^3)_NN_d
Tuning for shape 1x512*(2x8^4)_NN_d
Tuning for shape 1x128*(2x8^5)_NN_d
Tuning for shape 1x8192*(2x8^1)_NN_d
Tuning for shape 1x2048*(2x8^2)_NN_d
Tuning for shape 1x512*(2x8^3)_NN_d
Tuning for shape 1x128*(2x8^4)_NN_d
Tuning for shape 1x2048*(2x8^1)_NN_d
Tuning for shape 1x512*(2x8^2)_NN_d
Tuning for shape 1x128*(2x8^3)_NN_d
Tuning for shape 1x512*(2x8^1)_NN_d
Tuning for shape 1x128*(2x8^2)_NN_d
Tuning for shape 1x128*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x8^7  &  1.000 & 1.000 & 12.084 & 0.083
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 7 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 128] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x to produce Y[4, 2097152]
Matmul: 4 x 2097152 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x524288*(2x8^1)_NN_d
Tuning for shape 4x131072*(2x8^2)_NN_d
Tuning for shape 4x32768*(2x8^3)_NN_d
Tuning for shape 4x8192*(2x8^4)_NN_d
Tuning for shape 4x2048*(2x8^5)_NN_d
Tuning for shape 4x512*(2x8^6)_NN_d
Tuning for shape 4x128*(2x8^7)_NN_d
Tuning for shape 4x131072*(2x8^1)_NN_d
Tuning for shape 4x32768*(2x8^2)_NN_d
Tuning for shape 4x8192*(2x8^3)_NN_d
Tuning for shape 4x2048*(2x8^4)_NN_d
Tuning for shape 4x512*(2x8^5)_NN_d
Tuning for shape 4x128*(2x8^6)_NN_d
Tuning for shape 4x32768*(2x8^1)_NN_d
Tuning for shape 4x8192*(2x8^2)_NN_d
Tuning for shape 4x2048*(2x8^3)_NN_d
Tuning for shape 4x512*(2x8^4)_NN_d
Tuning for shape 4x128*(2x8^5)_NN_d
Tuning for shape 4x8192*(2x8^1)_NN_d
Tuning for shape 4x2048*(2x8^2)_NN_d
Tuning for shape 4x512*(2x8^3)_NN_d
Tuning for shape 4x128*(2x8^4)_NN_d
Tuning for shape 4x2048*(2x8^1)_NN_d
Tuning for shape 4x512*(2x8^2)_NN_d
Tuning for shape 4x128*(2x8^3)_NN_d
Tuning for shape 4x512*(2x8^1)_NN_d
Tuning for shape 4x128*(2x8^2)_NN_d
Tuning for shape 4x128*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x8^7  &  1.000 & 1.000 & 45.520 & 0.022
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 7 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 128] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x to produce Y[16, 2097152]
Matmul: 16 x 2097152 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x524288*(2x8^1)_NN_d
Tuning for shape 16x131072*(2x8^2)_NN_d
Tuning for shape 16x32768*(2x8^3)_NN_d
Tuning for shape 16x8192*(2x8^4)_NN_d
Tuning for shape 16x2048*(2x8^5)_NN_d
Tuning for shape 16x512*(2x8^6)_NN_d
Tuning for shape 16x128*(2x8^7)_NN_d
Tuning for shape 16x131072*(2x8^1)_NN_d
Tuning for shape 16x32768*(2x8^2)_NN_d
Tuning for shape 16x8192*(2x8^3)_NN_d
Tuning for shape 16x2048*(2x8^4)_NN_d
Tuning for shape 16x512*(2x8^5)_NN_d
Tuning for shape 16x128*(2x8^6)_NN_d
Tuning for shape 16x32768*(2x8^1)_NN_d
Tuning for shape 16x8192*(2x8^2)_NN_d
Tuning for shape 16x2048*(2x8^3)_NN_d
Tuning for shape 16x512*(2x8^4)_NN_d
Tuning for shape 16x128*(2x8^5)_NN_d
Tuning for shape 16x8192*(2x8^1)_NN_d
Tuning for shape 16x2048*(2x8^2)_NN_d
Tuning for shape 16x512*(2x8^3)_NN_d
Tuning for shape 16x128*(2x8^4)_NN_d
Tuning for shape 16x2048*(2x8^1)_NN_d
Tuning for shape 16x512*(2x8^2)_NN_d
Tuning for shape 16x128*(2x8^3)_NN_d
Tuning for shape 16x512*(2x8^1)_NN_d
Tuning for shape 16x128*(2x8^2)_NN_d
Tuning for shape 16x128*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x8^7  &  1.000 & 1.000 & 60.220 & 0.017
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 7 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 128] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x to produce Y[64, 2097152]
Matmul: 64 x 2097152 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x524288*(2x8^1)_NN_d
Tuning for shape 64x131072*(2x8^2)_NN_d
Tuning for shape 64x32768*(2x8^3)_NN_d
Tuning for shape 64x8192*(2x8^4)_NN_d
Tuning for shape 64x2048*(2x8^5)_NN_d
Tuning for shape 64x512*(2x8^6)_NN_d
Tuning for shape 64x128*(2x8^7)_NN_d
Tuning for shape 64x131072*(2x8^1)_NN_d
Tuning for shape 64x32768*(2x8^2)_NN_d
Tuning for shape 64x8192*(2x8^3)_NN_d
Tuning for shape 64x2048*(2x8^4)_NN_d
Tuning for shape 64x512*(2x8^5)_NN_d
Tuning for shape 64x128*(2x8^6)_NN_d
Tuning for shape 64x32768*(2x8^1)_NN_d
Tuning for shape 64x8192*(2x8^2)_NN_d
Tuning for shape 64x2048*(2x8^3)_NN_d
Tuning for shape 64x512*(2x8^4)_NN_d
Tuning for shape 64x128*(2x8^5)_NN_d
Tuning for shape 64x8192*(2x8^1)_NN_d
Tuning for shape 64x2048*(2x8^2)_NN_d
Tuning for shape 64x512*(2x8^3)_NN_d
Tuning for shape 64x128*(2x8^4)_NN_d
Tuning for shape 64x2048*(2x8^1)_NN_d
Tuning for shape 64x512*(2x8^2)_NN_d
Tuning for shape 64x128*(2x8^3)_NN_d
Tuning for shape 64x512*(2x8^1)_NN_d
Tuning for shape 64x128*(2x8^2)_NN_d
Tuning for shape 64x128*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x8^7  &  1.000 & 1.000 & 61.062 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 7 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 128] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x to produce Y[256, 2097152]
Matmul: 256 x 2097152 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x524288*(2x8^1)_NN_d
Tuning for shape 256x131072*(2x8^2)_NN_d
Tuning for shape 256x32768*(2x8^3)_NN_d
Tuning for shape 256x8192*(2x8^4)_NN_d
Tuning for shape 256x2048*(2x8^5)_NN_d
Tuning for shape 256x512*(2x8^6)_NN_d
Tuning for shape 256x128*(2x8^7)_NN_d
Tuning for shape 256x131072*(2x8^1)_NN_d
Tuning for shape 256x32768*(2x8^2)_NN_d
Tuning for shape 256x8192*(2x8^3)_NN_d
Tuning for shape 256x2048*(2x8^4)_NN_d
Tuning for shape 256x512*(2x8^5)_NN_d
Tuning for shape 256x128*(2x8^6)_NN_d
Tuning for shape 256x32768*(2x8^1)_NN_d
Tuning for shape 256x8192*(2x8^2)_NN_d
Tuning for shape 256x2048*(2x8^3)_NN_d
Tuning for shape 256x512*(2x8^4)_NN_d
Tuning for shape 256x128*(2x8^5)_NN_d
Tuning for shape 256x8192*(2x8^1)_NN_d
Tuning for shape 256x2048*(2x8^2)_NN_d
Tuning for shape 256x512*(2x8^3)_NN_d
Tuning for shape 256x128*(2x8^4)_NN_d
Tuning for shape 256x2048*(2x8^1)_NN_d
Tuning for shape 256x512*(2x8^2)_NN_d
Tuning for shape 256x128*(2x8^3)_NN_d
Tuning for shape 256x512*(2x8^1)_NN_d
Tuning for shape 256x128*(2x8^2)_NN_d
Tuning for shape 256x128*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x8^7  &  1.000 & 1.000 & 61.267 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 8 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x F_7 [2, 8] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4194304*(2x8^1)_NN_d
Tuning for shape 1x1048576*(2x8^2)_NN_d
Tuning for shape 1x262144*(2x8^3)_NN_d
Tuning for shape 1x65536*(2x8^4)_NN_d
Tuning for shape 1x16384*(2x8^5)_NN_d
Tuning for shape 1x4096*(2x8^6)_NN_d
Tuning for shape 1x1024*(2x8^7)_NN_d
Tuning for shape 1x256*(2x8^8)_NN_d
Tuning for shape 1x1048576*(2x8^1)_NN_d
Tuning for shape 1x262144*(2x8^2)_NN_d
Tuning for shape 1x65536*(2x8^3)_NN_d
Tuning for shape 1x16384*(2x8^4)_NN_d
Tuning for shape 1x4096*(2x8^5)_NN_d
Tuning for shape 1x1024*(2x8^6)_NN_d
Tuning for shape 1x256*(2x8^7)_NN_d
Tuning for shape 1x262144*(2x8^1)_NN_d
Tuning for shape 1x65536*(2x8^2)_NN_d
Tuning for shape 1x16384*(2x8^3)_NN_d
Tuning for shape 1x4096*(2x8^4)_NN_d
Tuning for shape 1x1024*(2x8^5)_NN_d
Tuning for shape 1x256*(2x8^6)_NN_d
Tuning for shape 1x65536*(2x8^1)_NN_d
Tuning for shape 1x16384*(2x8^2)_NN_d
Tuning for shape 1x4096*(2x8^3)_NN_d
Tuning for shape 1x1024*(2x8^4)_NN_d
Tuning for shape 1x256*(2x8^5)_NN_d
Tuning for shape 1x16384*(2x8^1)_NN_d
Tuning for shape 1x4096*(2x8^2)_NN_d
Tuning for shape 1x1024*(2x8^3)_NN_d
Tuning for shape 1x256*(2x8^4)_NN_d
Tuning for shape 1x4096*(2x8^1)_NN_d
Tuning for shape 1x1024*(2x8^2)_NN_d
Tuning for shape 1x256*(2x8^3)_NN_d
Tuning for shape 1x1024*(2x8^1)_NN_d
Tuning for shape 1x256*(2x8^2)_NN_d
Tuning for shape 1x256*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x8^8  &  1.000 & 1.000 & 57.761 & 0.017
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 8 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x F_7 [2, 8] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4194304*(2x8^1)_NN_d
Tuning for shape 4x1048576*(2x8^2)_NN_d
Tuning for shape 4x262144*(2x8^3)_NN_d
Tuning for shape 4x65536*(2x8^4)_NN_d
Tuning for shape 4x16384*(2x8^5)_NN_d
Tuning for shape 4x4096*(2x8^6)_NN_d
Tuning for shape 4x1024*(2x8^7)_NN_d
Tuning for shape 4x256*(2x8^8)_NN_d
Tuning for shape 4x1048576*(2x8^1)_NN_d
Tuning for shape 4x262144*(2x8^2)_NN_d
Tuning for shape 4x65536*(2x8^3)_NN_d
Tuning for shape 4x16384*(2x8^4)_NN_d
Tuning for shape 4x4096*(2x8^5)_NN_d
Tuning for shape 4x1024*(2x8^6)_NN_d
Tuning for shape 4x256*(2x8^7)_NN_d
Tuning for shape 4x262144*(2x8^1)_NN_d
Tuning for shape 4x65536*(2x8^2)_NN_d
Tuning for shape 4x16384*(2x8^3)_NN_d
Tuning for shape 4x4096*(2x8^4)_NN_d
Tuning for shape 4x1024*(2x8^5)_NN_d
Tuning for shape 4x256*(2x8^6)_NN_d
Tuning for shape 4x65536*(2x8^1)_NN_d
Tuning for shape 4x16384*(2x8^2)_NN_d
Tuning for shape 4x4096*(2x8^3)_NN_d
Tuning for shape 4x1024*(2x8^4)_NN_d
Tuning for shape 4x256*(2x8^5)_NN_d
Tuning for shape 4x16384*(2x8^1)_NN_d
Tuning for shape 4x4096*(2x8^2)_NN_d
Tuning for shape 4x1024*(2x8^3)_NN_d
Tuning for shape 4x256*(2x8^4)_NN_d
Tuning for shape 4x4096*(2x8^1)_NN_d
Tuning for shape 4x1024*(2x8^2)_NN_d
Tuning for shape 4x256*(2x8^3)_NN_d
Tuning for shape 4x1024*(2x8^1)_NN_d
Tuning for shape 4x256*(2x8^2)_NN_d
Tuning for shape 4x256*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x8^8  &  1.000 & 1.000 & 60.993 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 8 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x F_7 [2, 8] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 256, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4194304*(2x8^1)_NN_d
Tuning for shape 16x1048576*(2x8^2)_NN_d
Tuning for shape 16x262144*(2x8^3)_NN_d
Tuning for shape 16x65536*(2x8^4)_NN_d
Tuning for shape 16x16384*(2x8^5)_NN_d
Tuning for shape 16x4096*(2x8^6)_NN_d
Tuning for shape 16x1024*(2x8^7)_NN_d
Tuning for shape 16x256*(2x8^8)_NN_d
Tuning for shape 16x1048576*(2x8^1)_NN_d
Tuning for shape 16x262144*(2x8^2)_NN_d
Tuning for shape 16x65536*(2x8^3)_NN_d
Tuning for shape 16x16384*(2x8^4)_NN_d
Tuning for shape 16x4096*(2x8^5)_NN_d
Tuning for shape 16x1024*(2x8^6)_NN_d
Tuning for shape 16x256*(2x8^7)_NN_d
Tuning for shape 16x262144*(2x8^1)_NN_d
Tuning for shape 16x65536*(2x8^2)_NN_d
Tuning for shape 16x16384*(2x8^3)_NN_d
Tuning for shape 16x4096*(2x8^4)_NN_d
Tuning for shape 16x1024*(2x8^5)_NN_d
Tuning for shape 16x256*(2x8^6)_NN_d
Tuning for shape 16x65536*(2x8^1)_NN_d
Tuning for shape 16x16384*(2x8^2)_NN_d
Tuning for shape 16x4096*(2x8^3)_NN_d
Tuning for shape 16x1024*(2x8^4)_NN_d
Tuning for shape 16x256*(2x8^5)_NN_d
Tuning for shape 16x16384*(2x8^1)_NN_d
Tuning for shape 16x4096*(2x8^2)_NN_d
Tuning for shape 16x1024*(2x8^3)_NN_d
Tuning for shape 16x256*(2x8^4)_NN_d
Tuning for shape 16x4096*(2x8^1)_NN_d
Tuning for shape 16x1024*(2x8^2)_NN_d
Tuning for shape 16x256*(2x8^3)_NN_d
Tuning for shape 16x1024*(2x8^1)_NN_d
Tuning for shape 16x256*(2x8^2)_NN_d
Tuning for shape 16x256*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x8^8  &  1.000 & 1.000 & 61.906 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 9 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x F_7 [2, 8] x F_8 [2, 8] x to produce Y[1, 134217728]
Matmul: 1 x 134217728 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x33554432*(2x8^1)_NN_d
Tuning for shape 1x8388608*(2x8^2)_NN_d
Tuning for shape 1x2097152*(2x8^3)_NN_d
Tuning for shape 1x524288*(2x8^4)_NN_d
Tuning for shape 1x131072*(2x8^5)_NN_d
Tuning for shape 1x32768*(2x8^6)_NN_d
Tuning for shape 1x8192*(2x8^7)_NN_d
Tuning for shape 1x2048*(2x8^8)_NN_d
Tuning for shape 1x512*(2x8^9)_NN_d
Tuning for shape 1x8388608*(2x8^1)_NN_d
Tuning for shape 1x2097152*(2x8^2)_NN_d
Tuning for shape 1x524288*(2x8^3)_NN_d
Tuning for shape 1x131072*(2x8^4)_NN_d
Tuning for shape 1x32768*(2x8^5)_NN_d
Tuning for shape 1x8192*(2x8^6)_NN_d
Tuning for shape 1x2048*(2x8^7)_NN_d
Tuning for shape 1x512*(2x8^8)_NN_d
Tuning for shape 1x2097152*(2x8^1)_NN_d
Tuning for shape 1x524288*(2x8^2)_NN_d
Tuning for shape 1x131072*(2x8^3)_NN_d
Tuning for shape 1x32768*(2x8^4)_NN_d
Tuning for shape 1x8192*(2x8^5)_NN_d
Tuning for shape 1x2048*(2x8^6)_NN_d
Tuning for shape 1x512*(2x8^7)_NN_d
Tuning for shape 1x524288*(2x8^1)_NN_d
Tuning for shape 1x131072*(2x8^2)_NN_d
Tuning for shape 1x32768*(2x8^3)_NN_d
Tuning for shape 1x8192*(2x8^4)_NN_d
Tuning for shape 1x2048*(2x8^5)_NN_d
Tuning for shape 1x512*(2x8^6)_NN_d
Tuning for shape 1x131072*(2x8^1)_NN_d
Tuning for shape 1x32768*(2x8^2)_NN_d
Tuning for shape 1x8192*(2x8^3)_NN_d
Tuning for shape 1x2048*(2x8^4)_NN_d
Tuning for shape 1x512*(2x8^5)_NN_d
Tuning for shape 1x32768*(2x8^1)_NN_d
Tuning for shape 1x8192*(2x8^2)_NN_d
Tuning for shape 1x2048*(2x8^3)_NN_d
Tuning for shape 1x512*(2x8^4)_NN_d
Tuning for shape 1x8192*(2x8^1)_NN_d
Tuning for shape 1x2048*(2x8^2)_NN_d
Tuning for shape 1x512*(2x8^3)_NN_d
Tuning for shape 1x2048*(2x8^1)_NN_d
Tuning for shape 1x512*(2x8^2)_NN_d
Tuning for shape 1x512*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x8^9  &  1.000 & 1.000 & 61.341 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 9 -p 2 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [2, 8] x F_1 [2, 8] x F_2 [2, 8] x F_3 [2, 8] x F_4 [2, 8] x F_5 [2, 8] x F_6 [2, 8] x F_7 [2, 8] x F_8 [2, 8] x to produce Y[4, 134217728]
Matmul: 4 x 134217728 x 512, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x33554432*(2x8^1)_NN_d
Tuning for shape 4x8388608*(2x8^2)_NN_d
Tuning for shape 4x2097152*(2x8^3)_NN_d
Tuning for shape 4x524288*(2x8^4)_NN_d
Tuning for shape 4x131072*(2x8^5)_NN_d
Tuning for shape 4x32768*(2x8^6)_NN_d
Tuning for shape 4x8192*(2x8^7)_NN_d
Tuning for shape 4x2048*(2x8^8)_NN_d
Tuning for shape 4x512*(2x8^9)_NN_d
Tuning for shape 4x8388608*(2x8^1)_NN_d
Tuning for shape 4x2097152*(2x8^2)_NN_d
Tuning for shape 4x524288*(2x8^3)_NN_d
Tuning for shape 4x131072*(2x8^4)_NN_d
Tuning for shape 4x32768*(2x8^5)_NN_d
Tuning for shape 4x8192*(2x8^6)_NN_d
Tuning for shape 4x2048*(2x8^7)_NN_d
Tuning for shape 4x512*(2x8^8)_NN_d
Tuning for shape 4x2097152*(2x8^1)_NN_d
Tuning for shape 4x524288*(2x8^2)_NN_d
Tuning for shape 4x131072*(2x8^3)_NN_d
Tuning for shape 4x32768*(2x8^4)_NN_d
Tuning for shape 4x8192*(2x8^5)_NN_d
Tuning for shape 4x2048*(2x8^6)_NN_d
Tuning for shape 4x512*(2x8^7)_NN_d
Tuning for shape 4x524288*(2x8^1)_NN_d
Tuning for shape 4x131072*(2x8^2)_NN_d
Tuning for shape 4x32768*(2x8^3)_NN_d
Tuning for shape 4x8192*(2x8^4)_NN_d
Tuning for shape 4x2048*(2x8^5)_NN_d
Tuning for shape 4x512*(2x8^6)_NN_d
Tuning for shape 4x131072*(2x8^1)_NN_d
Tuning for shape 4x32768*(2x8^2)_NN_d
Tuning for shape 4x8192*(2x8^3)_NN_d
Tuning for shape 4x2048*(2x8^4)_NN_d
Tuning for shape 4x512*(2x8^5)_NN_d
Tuning for shape 4x32768*(2x8^1)_NN_d
Tuning for shape 4x8192*(2x8^2)_NN_d
Tuning for shape 4x2048*(2x8^3)_NN_d
Tuning for shape 4x512*(2x8^4)_NN_d
Tuning for shape 4x8192*(2x8^1)_NN_d
Tuning for shape 4x2048*(2x8^2)_NN_d
Tuning for shape 4x512*(2x8^3)_NN_d
Tuning for shape 4x2048*(2x8^1)_NN_d
Tuning for shape 4x512*(2x8^2)_NN_d
Tuning for shape 4x512*(2x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x8^9  &  1.000 & 1.000 & 61.850 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2] with F_0 [2, 16] x to produce Y[1, 16]
Matmul: 1 x 16 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x16^1  &  1.000 & 1.000 & 0.000 & 4277.378
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2] with F_0 [2, 16] x to produce Y[4, 16]
Matmul: 4 x 16 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x16^1  &  1.000 & 1.000 & 0.001 & 1233.444
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2] with F_0 [2, 16] x to produce Y[16, 16]
Matmul: 16 x 16 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x16^1  &  1.000 & 1.000 & 0.003 & 313.949
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2] with F_0 [2, 16] x to produce Y[64, 16]
Matmul: 64 x 16 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x16^1  &  1.000 & 1.000 & 0.013 & 78.944
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2] with F_0 [2, 16] x to produce Y[256, 16]
Matmul: 256 x 16 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x16^1  &  1.000 & 1.000 & 0.052 & 19.129
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 2] with F_0 [2, 16] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x16^1  &  1.000 & 1.000 & 0.208 & 4.806
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [2, 16] x F_1 [2, 16] x to produce Y[1, 256]
Matmul: 1 x 256 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32*(2x16^1)_NN_d
Tuning for shape 1x4*(2x16^2)_NN_d
Tuning for shape 1x4*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x16^2  &  1.000 & 1.000 & 0.003 & 363.537
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [2, 16] x F_1 [2, 16] x to produce Y[4, 256]
Matmul: 4 x 256 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32*(2x16^1)_NN_d
Tuning for shape 4x4*(2x16^2)_NN_d
Tuning for shape 4x4*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x16^2  &  1.000 & 1.000 & 0.010 & 97.970
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [2, 16] x F_1 [2, 16] x to produce Y[16, 256]
Matmul: 16 x 256 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32*(2x16^1)_NN_d
Tuning for shape 16x4*(2x16^2)_NN_d
Tuning for shape 16x4*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x16^2  &  1.000 & 1.000 & 0.040 & 25.210
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [2, 16] x F_1 [2, 16] x to produce Y[64, 256]
Matmul: 64 x 256 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32*(2x16^1)_NN_d
Tuning for shape 64x4*(2x16^2)_NN_d
Tuning for shape 64x4*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x16^2  &  1.000 & 1.000 & 0.161 & 6.192
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [2, 16] x F_1 [2, 16] x to produce Y[256, 256]
Matmul: 256 x 256 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32*(2x16^1)_NN_d
Tuning for shape 256x4*(2x16^2)_NN_d
Tuning for shape 256x4*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x16^2  &  1.000 & 1.000 & 0.636 & 1.572
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [2, 16] x F_1 [2, 16] x to produce Y[1024, 256]
Matmul: 1024 x 256 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32*(2x16^1)_NN_d
Tuning for shape 1024x4*(2x16^2)_NN_d
Tuning for shape 1024x4*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x16^2  &  1.000 & 1.000 & 2.597 & 0.385
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(2x16^1)_NN_d
Tuning for shape 1x64*(2x16^2)_NN_d
Tuning for shape 1x8*(2x16^3)_NN_d
Tuning for shape 1x64*(2x16^1)_NN_d
Tuning for shape 1x8*(2x16^2)_NN_d
Tuning for shape 1x8*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x16^3  &  1.000 & 1.000 & 0.037 & 27.360
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(2x16^1)_NN_d
Tuning for shape 4x64*(2x16^2)_NN_d
Tuning for shape 4x8*(2x16^3)_NN_d
Tuning for shape 4x64*(2x16^1)_NN_d
Tuning for shape 4x8*(2x16^2)_NN_d
Tuning for shape 4x8*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x16^3  &  1.000 & 1.000 & 0.135 & 7.430
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(2x16^1)_NN_d
Tuning for shape 16x64*(2x16^2)_NN_d
Tuning for shape 16x8*(2x16^3)_NN_d
Tuning for shape 16x64*(2x16^1)_NN_d
Tuning for shape 16x8*(2x16^2)_NN_d
Tuning for shape 16x8*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x16^3  &  1.000 & 1.000 & 0.529 & 1.889
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x512*(2x16^1)_NN_d
Tuning for shape 64x64*(2x16^2)_NN_d
Tuning for shape 64x8*(2x16^3)_NN_d
Tuning for shape 64x64*(2x16^1)_NN_d
Tuning for shape 64x8*(2x16^2)_NN_d
Tuning for shape 64x8*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x16^3  &  1.000 & 1.000 & 2.116 & 0.473
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x512*(2x16^1)_NN_d
Tuning for shape 256x64*(2x16^2)_NN_d
Tuning for shape 256x8*(2x16^3)_NN_d
Tuning for shape 256x64*(2x16^1)_NN_d
Tuning for shape 256x8*(2x16^2)_NN_d
Tuning for shape 256x8*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x16^3  &  1.000 & 1.000 & 7.946 & 0.126
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x512*(2x16^1)_NN_d
Tuning for shape 1024x64*(2x16^2)_NN_d
Tuning for shape 1024x8*(2x16^3)_NN_d
Tuning for shape 1024x64*(2x16^1)_NN_d
Tuning for shape 1024x8*(2x16^2)_NN_d
Tuning for shape 1024x8*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x16^3  &  1.000 & 1.000 & 32.259 & 0.031
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x to produce Y[1, 65536]
Matmul: 1 x 65536 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8192*(2x16^1)_NN_d
Tuning for shape 1x1024*(2x16^2)_NN_d
Tuning for shape 1x128*(2x16^3)_NN_d
Tuning for shape 1x16*(2x16^4)_NN_d
Tuning for shape 1x1024*(2x16^1)_NN_d
Tuning for shape 1x128*(2x16^2)_NN_d
Tuning for shape 1x16*(2x16^3)_NN_d
Tuning for shape 1x128*(2x16^1)_NN_d
Tuning for shape 1x16*(2x16^2)_NN_d
Tuning for shape 1x16*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x16^4  &  1.000 & 1.000 & 0.482 & 2.073
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x to produce Y[4, 65536]
Matmul: 4 x 65536 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8192*(2x16^1)_NN_d
Tuning for shape 4x1024*(2x16^2)_NN_d
Tuning for shape 4x128*(2x16^3)_NN_d
Tuning for shape 4x16*(2x16^4)_NN_d
Tuning for shape 4x1024*(2x16^1)_NN_d
Tuning for shape 4x128*(2x16^2)_NN_d
Tuning for shape 4x16*(2x16^3)_NN_d
Tuning for shape 4x128*(2x16^1)_NN_d
Tuning for shape 4x16*(2x16^2)_NN_d
Tuning for shape 4x16*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x16^4  &  1.000 & 1.000 & 1.834 & 0.545
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x to produce Y[16, 65536]
Matmul: 16 x 65536 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8192*(2x16^1)_NN_d
Tuning for shape 16x1024*(2x16^2)_NN_d
Tuning for shape 16x128*(2x16^3)_NN_d
Tuning for shape 16x16*(2x16^4)_NN_d
Tuning for shape 16x1024*(2x16^1)_NN_d
Tuning for shape 16x128*(2x16^2)_NN_d
Tuning for shape 16x16*(2x16^3)_NN_d
Tuning for shape 16x128*(2x16^1)_NN_d
Tuning for shape 16x16*(2x16^2)_NN_d
Tuning for shape 16x16*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x16^4  &  1.000 & 1.000 & 6.601 & 0.152
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x to produce Y[64, 65536]
Matmul: 64 x 65536 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8192*(2x16^1)_NN_d
Tuning for shape 64x1024*(2x16^2)_NN_d
Tuning for shape 64x128*(2x16^3)_NN_d
Tuning for shape 64x16*(2x16^4)_NN_d
Tuning for shape 64x1024*(2x16^1)_NN_d
Tuning for shape 64x128*(2x16^2)_NN_d
Tuning for shape 64x16*(2x16^3)_NN_d
Tuning for shape 64x128*(2x16^1)_NN_d
Tuning for shape 64x16*(2x16^2)_NN_d
Tuning for shape 64x16*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x16^4  &  1.000 & 1.000 & 27.905 & 0.036
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x to produce Y[256, 65536]
Matmul: 256 x 65536 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8192*(2x16^1)_NN_d
Tuning for shape 256x1024*(2x16^2)_NN_d
Tuning for shape 256x128*(2x16^3)_NN_d
Tuning for shape 256x16*(2x16^4)_NN_d
Tuning for shape 256x1024*(2x16^1)_NN_d
Tuning for shape 256x128*(2x16^2)_NN_d
Tuning for shape 256x16*(2x16^3)_NN_d
Tuning for shape 256x128*(2x16^1)_NN_d
Tuning for shape 256x16*(2x16^2)_NN_d
Tuning for shape 256x16*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x16^4  &  1.000 & 1.000 & 63.199 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x to produce Y[1024, 65536]
Matmul: 1024 x 65536 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8192*(2x16^1)_NN_d
Tuning for shape 1024x1024*(2x16^2)_NN_d
Tuning for shape 1024x128*(2x16^3)_NN_d
Tuning for shape 1024x16*(2x16^4)_NN_d
Tuning for shape 1024x1024*(2x16^1)_NN_d
Tuning for shape 1024x128*(2x16^2)_NN_d
Tuning for shape 1024x16*(2x16^3)_NN_d
Tuning for shape 1024x128*(2x16^1)_NN_d
Tuning for shape 1024x16*(2x16^2)_NN_d
Tuning for shape 1024x16*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x16^4  &  1.000 & 1.000 & 66.092 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x F_4 [2, 16] x to produce Y[1, 1048576]
Matmul: 1 x 1048576 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x131072*(2x16^1)_NN_d
Tuning for shape 1x16384*(2x16^2)_NN_d
Tuning for shape 1x2048*(2x16^3)_NN_d
Tuning for shape 1x256*(2x16^4)_NN_d
Tuning for shape 1x32*(2x16^5)_NN_d
Tuning for shape 1x16384*(2x16^1)_NN_d
Tuning for shape 1x2048*(2x16^2)_NN_d
Tuning for shape 1x256*(2x16^3)_NN_d
Tuning for shape 1x32*(2x16^4)_NN_d
Tuning for shape 1x2048*(2x16^1)_NN_d
Tuning for shape 1x256*(2x16^2)_NN_d
Tuning for shape 1x32*(2x16^3)_NN_d
Tuning for shape 1x256*(2x16^1)_NN_d
Tuning for shape 1x32*(2x16^2)_NN_d
Tuning for shape 1x32*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x16^5  &  1.000 & 1.000 & 5.459 & 0.183
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x F_4 [2, 16] x to produce Y[4, 1048576]
Matmul: 4 x 1048576 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x131072*(2x16^1)_NN_d
Tuning for shape 4x16384*(2x16^2)_NN_d
Tuning for shape 4x2048*(2x16^3)_NN_d
Tuning for shape 4x256*(2x16^4)_NN_d
Tuning for shape 4x32*(2x16^5)_NN_d
Tuning for shape 4x16384*(2x16^1)_NN_d
Tuning for shape 4x2048*(2x16^2)_NN_d
Tuning for shape 4x256*(2x16^3)_NN_d
Tuning for shape 4x32*(2x16^4)_NN_d
Tuning for shape 4x2048*(2x16^1)_NN_d
Tuning for shape 4x256*(2x16^2)_NN_d
Tuning for shape 4x32*(2x16^3)_NN_d
Tuning for shape 4x256*(2x16^1)_NN_d
Tuning for shape 4x32*(2x16^2)_NN_d
Tuning for shape 4x32*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x16^5  &  1.000 & 1.000 & 24.519 & 0.041
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x F_4 [2, 16] x to produce Y[16, 1048576]
Matmul: 16 x 1048576 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x131072*(2x16^1)_NN_d
Tuning for shape 16x16384*(2x16^2)_NN_d
Tuning for shape 16x2048*(2x16^3)_NN_d
Tuning for shape 16x256*(2x16^4)_NN_d
Tuning for shape 16x32*(2x16^5)_NN_d
Tuning for shape 16x16384*(2x16^1)_NN_d
Tuning for shape 16x2048*(2x16^2)_NN_d
Tuning for shape 16x256*(2x16^3)_NN_d
Tuning for shape 16x32*(2x16^4)_NN_d
Tuning for shape 16x2048*(2x16^1)_NN_d
Tuning for shape 16x256*(2x16^2)_NN_d
Tuning for shape 16x32*(2x16^3)_NN_d
Tuning for shape 16x256*(2x16^1)_NN_d
Tuning for shape 16x32*(2x16^2)_NN_d
Tuning for shape 16x32*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x16^5  &  1.000 & 1.000 & 64.239 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 32] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x F_4 [2, 16] x to produce Y[64, 1048576]
Matmul: 64 x 1048576 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x131072*(2x16^1)_NN_d
Tuning for shape 64x16384*(2x16^2)_NN_d
Tuning for shape 64x2048*(2x16^3)_NN_d
Tuning for shape 64x256*(2x16^4)_NN_d
Tuning for shape 64x32*(2x16^5)_NN_d
Tuning for shape 64x16384*(2x16^1)_NN_d
Tuning for shape 64x2048*(2x16^2)_NN_d
Tuning for shape 64x256*(2x16^3)_NN_d
Tuning for shape 64x32*(2x16^4)_NN_d
Tuning for shape 64x2048*(2x16^1)_NN_d
Tuning for shape 64x256*(2x16^2)_NN_d
Tuning for shape 64x32*(2x16^3)_NN_d
Tuning for shape 64x256*(2x16^1)_NN_d
Tuning for shape 64x32*(2x16^2)_NN_d
Tuning for shape 64x32*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x16^5  &  1.000 & 1.000 & 67.284 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 32] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x F_4 [2, 16] x to produce Y[256, 1048576]
Matmul: 256 x 1048576 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x131072*(2x16^1)_NN_d
Tuning for shape 256x16384*(2x16^2)_NN_d
Tuning for shape 256x2048*(2x16^3)_NN_d
Tuning for shape 256x256*(2x16^4)_NN_d
Tuning for shape 256x32*(2x16^5)_NN_d
Tuning for shape 256x16384*(2x16^1)_NN_d
Tuning for shape 256x2048*(2x16^2)_NN_d
Tuning for shape 256x256*(2x16^3)_NN_d
Tuning for shape 256x32*(2x16^4)_NN_d
Tuning for shape 256x2048*(2x16^1)_NN_d
Tuning for shape 256x256*(2x16^2)_NN_d
Tuning for shape 256x32*(2x16^3)_NN_d
Tuning for shape 256x256*(2x16^1)_NN_d
Tuning for shape 256x32*(2x16^2)_NN_d
Tuning for shape 256x32*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x16^5  &  1.000 & 1.000 & 66.875 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x F_4 [2, 16] x F_5 [2, 16] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2097152*(2x16^1)_NN_d
Tuning for shape 1x262144*(2x16^2)_NN_d
Tuning for shape 1x32768*(2x16^3)_NN_d
Tuning for shape 1x4096*(2x16^4)_NN_d
Tuning for shape 1x512*(2x16^5)_NN_d
Tuning for shape 1x64*(2x16^6)_NN_d
Tuning for shape 1x262144*(2x16^1)_NN_d
Tuning for shape 1x32768*(2x16^2)_NN_d
Tuning for shape 1x4096*(2x16^3)_NN_d
Tuning for shape 1x512*(2x16^4)_NN_d
Tuning for shape 1x64*(2x16^5)_NN_d
Tuning for shape 1x32768*(2x16^1)_NN_d
Tuning for shape 1x4096*(2x16^2)_NN_d
Tuning for shape 1x512*(2x16^3)_NN_d
Tuning for shape 1x64*(2x16^4)_NN_d
Tuning for shape 1x4096*(2x16^1)_NN_d
Tuning for shape 1x512*(2x16^2)_NN_d
Tuning for shape 1x64*(2x16^3)_NN_d
Tuning for shape 1x512*(2x16^1)_NN_d
Tuning for shape 1x64*(2x16^2)_NN_d
Tuning for shape 1x64*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x16^6  &  1.000 & 1.000 & 63.943 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x F_4 [2, 16] x F_5 [2, 16] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2097152*(2x16^1)_NN_d
Tuning for shape 4x262144*(2x16^2)_NN_d
Tuning for shape 4x32768*(2x16^3)_NN_d
Tuning for shape 4x4096*(2x16^4)_NN_d
Tuning for shape 4x512*(2x16^5)_NN_d
Tuning for shape 4x64*(2x16^6)_NN_d
Tuning for shape 4x262144*(2x16^1)_NN_d
Tuning for shape 4x32768*(2x16^2)_NN_d
Tuning for shape 4x4096*(2x16^3)_NN_d
Tuning for shape 4x512*(2x16^4)_NN_d
Tuning for shape 4x64*(2x16^5)_NN_d
Tuning for shape 4x32768*(2x16^1)_NN_d
Tuning for shape 4x4096*(2x16^2)_NN_d
Tuning for shape 4x512*(2x16^3)_NN_d
Tuning for shape 4x64*(2x16^4)_NN_d
Tuning for shape 4x4096*(2x16^1)_NN_d
Tuning for shape 4x512*(2x16^2)_NN_d
Tuning for shape 4x64*(2x16^3)_NN_d
Tuning for shape 4x512*(2x16^1)_NN_d
Tuning for shape 4x64*(2x16^2)_NN_d
Tuning for shape 4x64*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x16^6  &  1.000 & 1.000 & 67.185 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x F_4 [2, 16] x F_5 [2, 16] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 64, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2097152*(2x16^1)_NN_d
Tuning for shape 16x262144*(2x16^2)_NN_d
Tuning for shape 16x32768*(2x16^3)_NN_d
Tuning for shape 16x4096*(2x16^4)_NN_d
Tuning for shape 16x512*(2x16^5)_NN_d
Tuning for shape 16x64*(2x16^6)_NN_d
Tuning for shape 16x262144*(2x16^1)_NN_d
Tuning for shape 16x32768*(2x16^2)_NN_d
Tuning for shape 16x4096*(2x16^3)_NN_d
Tuning for shape 16x512*(2x16^4)_NN_d
Tuning for shape 16x64*(2x16^5)_NN_d
Tuning for shape 16x32768*(2x16^1)_NN_d
Tuning for shape 16x4096*(2x16^2)_NN_d
Tuning for shape 16x512*(2x16^3)_NN_d
Tuning for shape 16x64*(2x16^4)_NN_d
Tuning for shape 16x4096*(2x16^1)_NN_d
Tuning for shape 16x512*(2x16^2)_NN_d
Tuning for shape 16x64*(2x16^3)_NN_d
Tuning for shape 16x512*(2x16^1)_NN_d
Tuning for shape 16x64*(2x16^2)_NN_d
Tuning for shape 16x64*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x16^6  &  1.000 & 1.000 & 66.487 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 2 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 128] with F_0 [2, 16] x F_1 [2, 16] x F_2 [2, 16] x F_3 [2, 16] x F_4 [2, 16] x F_5 [2, 16] x F_6 [2, 16] x to produce Y[1, 268435456]
Matmul: 1 x 268435456 x 128, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x33554432*(2x16^1)_NN_d
Tuning for shape 1x4194304*(2x16^2)_NN_d
Tuning for shape 1x524288*(2x16^3)_NN_d
Tuning for shape 1x65536*(2x16^4)_NN_d
Tuning for shape 1x8192*(2x16^5)_NN_d
Tuning for shape 1x1024*(2x16^6)_NN_d
Tuning for shape 1x128*(2x16^7)_NN_d
Tuning for shape 1x4194304*(2x16^1)_NN_d
Tuning for shape 1x524288*(2x16^2)_NN_d
Tuning for shape 1x65536*(2x16^3)_NN_d
Tuning for shape 1x8192*(2x16^4)_NN_d
Tuning for shape 1x1024*(2x16^5)_NN_d
Tuning for shape 1x128*(2x16^6)_NN_d
Tuning for shape 1x524288*(2x16^1)_NN_d
Tuning for shape 1x65536*(2x16^2)_NN_d
Tuning for shape 1x8192*(2x16^3)_NN_d
Tuning for shape 1x1024*(2x16^4)_NN_d
Tuning for shape 1x128*(2x16^5)_NN_d
Tuning for shape 1x65536*(2x16^1)_NN_d
Tuning for shape 1x8192*(2x16^2)_NN_d
Tuning for shape 1x1024*(2x16^3)_NN_d
Tuning for shape 1x128*(2x16^4)_NN_d
Tuning for shape 1x8192*(2x16^1)_NN_d
Tuning for shape 1x1024*(2x16^2)_NN_d
Tuning for shape 1x128*(2x16^3)_NN_d
Tuning for shape 1x1024*(2x16^1)_NN_d
Tuning for shape 1x128*(2x16^2)_NN_d
Tuning for shape 1x128*(2x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x16^7  &  1.000 & 1.000 & 66.229 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2] with F_0 [2, 32] x to produce Y[1, 32]
Matmul: 1 x 32 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x32^1  &  1.000 & 1.000 & 0.000 & 2136.454
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2] with F_0 [2, 32] x to produce Y[4, 32]
Matmul: 4 x 32 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x32^1  &  1.000 & 1.000 & 0.002 & 597.700
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2] with F_0 [2, 32] x to produce Y[16, 32]
Matmul: 16 x 32 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x32^1  &  1.000 & 1.000 & 0.007 & 149.664
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2] with F_0 [2, 32] x to produce Y[64, 32]
Matmul: 64 x 32 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x32^1  &  1.000 & 1.000 & 0.026 & 37.751
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2] with F_0 [2, 32] x to produce Y[256, 32]
Matmul: 256 x 32 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x32^1  &  1.000 & 1.000 & 0.107 & 9.362
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 2] with F_0 [2, 32] x to produce Y[1024, 32]
Matmul: 1024 x 32 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x32^1  &  1.000 & 1.000 & 0.424 & 2.357
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [2, 32] x F_1 [2, 32] x to produce Y[1, 1024]
Matmul: 1 x 1024 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x64*(2x32^1)_NN_d
Tuning for shape 1x4*(2x32^2)_NN_d
Tuning for shape 1x4*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x32^2  &  1.000 & 1.000 & 0.011 & 94.910
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [2, 32] x F_1 [2, 32] x to produce Y[4, 1024]
Matmul: 4 x 1024 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x64*(2x32^1)_NN_d
Tuning for shape 4x4*(2x32^2)_NN_d
Tuning for shape 4x4*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x32^2  &  1.000 & 1.000 & 0.039 & 25.327
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [2, 32] x F_1 [2, 32] x to produce Y[16, 1024]
Matmul: 16 x 1024 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x64*(2x32^1)_NN_d
Tuning for shape 16x4*(2x32^2)_NN_d
Tuning for shape 16x4*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x32^2  &  1.000 & 1.000 & 0.154 & 6.492
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [2, 32] x F_1 [2, 32] x to produce Y[64, 1024]
Matmul: 64 x 1024 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x64*(2x32^1)_NN_d
Tuning for shape 64x4*(2x32^2)_NN_d
Tuning for shape 64x4*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x32^2  &  1.000 & 1.000 & 0.620 & 1.614
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [2, 32] x F_1 [2, 32] x to produce Y[256, 1024]
Matmul: 256 x 1024 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x64*(2x32^1)_NN_d
Tuning for shape 256x4*(2x32^2)_NN_d
Tuning for shape 256x4*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x32^2  &  1.000 & 1.000 & 2.319 & 0.431
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [2, 32] x F_1 [2, 32] x to produce Y[1024, 1024]
Matmul: 1024 x 1024 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x64*(2x32^1)_NN_d
Tuning for shape 1024x4*(2x32^2)_NN_d
Tuning for shape 1024x4*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x32^2  &  1.000 & 1.000 & 9.455 & 0.106
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x to produce Y[1, 32768]
Matmul: 1 x 32768 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2048*(2x32^1)_NN_d
Tuning for shape 1x128*(2x32^2)_NN_d
Tuning for shape 1x8*(2x32^3)_NN_d
Tuning for shape 1x128*(2x32^1)_NN_d
Tuning for shape 1x8*(2x32^2)_NN_d
Tuning for shape 1x8*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x32^3  &  1.000 & 1.000 & 0.270 & 3.706
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x to produce Y[4, 32768]
Matmul: 4 x 32768 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2048*(2x32^1)_NN_d
Tuning for shape 4x128*(2x32^2)_NN_d
Tuning for shape 4x8*(2x32^3)_NN_d
Tuning for shape 4x128*(2x32^1)_NN_d
Tuning for shape 4x8*(2x32^2)_NN_d
Tuning for shape 4x8*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x32^3  &  1.000 & 1.000 & 0.707 & 1.414
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x to produce Y[16, 32768]
Matmul: 16 x 32768 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2048*(2x32^1)_NN_d
Tuning for shape 16x128*(2x32^2)_NN_d
Tuning for shape 16x8*(2x32^3)_NN_d
Tuning for shape 16x128*(2x32^1)_NN_d
Tuning for shape 16x8*(2x32^2)_NN_d
Tuning for shape 16x8*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x32^3  &  1.000 & 1.000 & 3.937 & 0.254
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x to produce Y[64, 32768]
Matmul: 64 x 32768 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2048*(2x32^1)_NN_d
Tuning for shape 64x128*(2x32^2)_NN_d
Tuning for shape 64x8*(2x32^3)_NN_d
Tuning for shape 64x128*(2x32^1)_NN_d
Tuning for shape 64x8*(2x32^2)_NN_d
Tuning for shape 64x8*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x32^3  &  1.000 & 1.000 & 15.612 & 0.064
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x to produce Y[256, 32768]
Matmul: 256 x 32768 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2048*(2x32^1)_NN_d
Tuning for shape 256x128*(2x32^2)_NN_d
Tuning for shape 256x8*(2x32^3)_NN_d
Tuning for shape 256x128*(2x32^1)_NN_d
Tuning for shape 256x8*(2x32^2)_NN_d
Tuning for shape 256x8*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x32^3  &  1.000 & 1.000 & 63.526 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x to produce Y[1024, 32768]
Matmul: 1024 x 32768 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2048*(2x32^1)_NN_d
Tuning for shape 1024x128*(2x32^2)_NN_d
Tuning for shape 1024x8*(2x32^3)_NN_d
Tuning for shape 1024x128*(2x32^1)_NN_d
Tuning for shape 1024x8*(2x32^2)_NN_d
Tuning for shape 1024x8*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x32^3  &  1.000 & 1.000 & 120.227 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x F_3 [2, 32] x to produce Y[1, 1048576]
Matmul: 1 x 1048576 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x65536*(2x32^1)_NN_d
Tuning for shape 1x4096*(2x32^2)_NN_d
Tuning for shape 1x256*(2x32^3)_NN_d
Tuning for shape 1x16*(2x32^4)_NN_d
Tuning for shape 1x4096*(2x32^1)_NN_d
Tuning for shape 1x256*(2x32^2)_NN_d
Tuning for shape 1x16*(2x32^3)_NN_d
Tuning for shape 1x256*(2x32^1)_NN_d
Tuning for shape 1x16*(2x32^2)_NN_d
Tuning for shape 1x16*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x32^4  &  1.000 & 1.000 & 7.032 & 0.142
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x F_3 [2, 32] x to produce Y[4, 1048576]
Matmul: 4 x 1048576 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x65536*(2x32^1)_NN_d
Tuning for shape 4x4096*(2x32^2)_NN_d
Tuning for shape 4x256*(2x32^3)_NN_d
Tuning for shape 4x16*(2x32^4)_NN_d
Tuning for shape 4x4096*(2x32^1)_NN_d
Tuning for shape 4x256*(2x32^2)_NN_d
Tuning for shape 4x16*(2x32^3)_NN_d
Tuning for shape 4x256*(2x32^1)_NN_d
Tuning for shape 4x16*(2x32^2)_NN_d
Tuning for shape 4x16*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x32^4  &  1.000 & 1.000 & 26.922 & 0.037
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x F_3 [2, 32] x to produce Y[16, 1048576]
Matmul: 16 x 1048576 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x65536*(2x32^1)_NN_d
Tuning for shape 16x4096*(2x32^2)_NN_d
Tuning for shape 16x256*(2x32^3)_NN_d
Tuning for shape 16x16*(2x32^4)_NN_d
Tuning for shape 16x4096*(2x32^1)_NN_d
Tuning for shape 16x256*(2x32^2)_NN_d
Tuning for shape 16x16*(2x32^3)_NN_d
Tuning for shape 16x256*(2x32^1)_NN_d
Tuning for shape 16x16*(2x32^2)_NN_d
Tuning for shape 16x16*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x32^4  &  1.000 & 1.000 & 104.012 & 0.010
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x F_3 [2, 32] x to produce Y[64, 1048576]
Matmul: 64 x 1048576 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x65536*(2x32^1)_NN_d
Tuning for shape 64x4096*(2x32^2)_NN_d
Tuning for shape 64x256*(2x32^3)_NN_d
Tuning for shape 64x16*(2x32^4)_NN_d
Tuning for shape 64x4096*(2x32^1)_NN_d
Tuning for shape 64x256*(2x32^2)_NN_d
Tuning for shape 64x16*(2x32^3)_NN_d
Tuning for shape 64x256*(2x32^1)_NN_d
Tuning for shape 64x16*(2x32^2)_NN_d
Tuning for shape 64x16*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x32^4  &  1.000 & 1.000 & 123.143 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x F_3 [2, 32] x to produce Y[256, 1048576]
Matmul: 256 x 1048576 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x65536*(2x32^1)_NN_d
Tuning for shape 256x4096*(2x32^2)_NN_d
Tuning for shape 256x256*(2x32^3)_NN_d
Tuning for shape 256x16*(2x32^4)_NN_d
Tuning for shape 256x4096*(2x32^1)_NN_d
Tuning for shape 256x256*(2x32^2)_NN_d
Tuning for shape 256x16*(2x32^3)_NN_d
Tuning for shape 256x256*(2x32^1)_NN_d
Tuning for shape 256x16*(2x32^2)_NN_d
Tuning for shape 256x16*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x32^4  &  1.000 & 1.000 & 125.833 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x F_3 [2, 32] x F_4 [2, 32] x to produce Y[1, 33554432]
Matmul: 1 x 33554432 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2097152*(2x32^1)_NN_d
Tuning for shape 1x131072*(2x32^2)_NN_d
Tuning for shape 1x8192*(2x32^3)_NN_d
Tuning for shape 1x512*(2x32^4)_NN_d
Tuning for shape 1x32*(2x32^5)_NN_d
Tuning for shape 1x131072*(2x32^1)_NN_d
Tuning for shape 1x8192*(2x32^2)_NN_d
Tuning for shape 1x512*(2x32^3)_NN_d
Tuning for shape 1x32*(2x32^4)_NN_d
Tuning for shape 1x8192*(2x32^1)_NN_d
Tuning for shape 1x512*(2x32^2)_NN_d
Tuning for shape 1x32*(2x32^3)_NN_d
Tuning for shape 1x512*(2x32^1)_NN_d
Tuning for shape 1x32*(2x32^2)_NN_d
Tuning for shape 1x32*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x32^5  &  1.000 & 1.000 & 100.017 & 0.010
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x F_3 [2, 32] x F_4 [2, 32] x to produce Y[4, 33554432]
Matmul: 4 x 33554432 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2097152*(2x32^1)_NN_d
Tuning for shape 4x131072*(2x32^2)_NN_d
Tuning for shape 4x8192*(2x32^3)_NN_d
Tuning for shape 4x512*(2x32^4)_NN_d
Tuning for shape 4x32*(2x32^5)_NN_d
Tuning for shape 4x131072*(2x32^1)_NN_d
Tuning for shape 4x8192*(2x32^2)_NN_d
Tuning for shape 4x512*(2x32^3)_NN_d
Tuning for shape 4x32*(2x32^4)_NN_d
Tuning for shape 4x8192*(2x32^1)_NN_d
Tuning for shape 4x512*(2x32^2)_NN_d
Tuning for shape 4x32*(2x32^3)_NN_d
Tuning for shape 4x512*(2x32^1)_NN_d
Tuning for shape 4x32*(2x32^2)_NN_d
Tuning for shape 4x32*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x32^5  &  1.000 & 1.000 & 116.787 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 2 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32] with F_0 [2, 32] x F_1 [2, 32] x F_2 [2, 32] x F_3 [2, 32] x F_4 [2, 32] x to produce Y[16, 33554432]
Matmul: 16 x 33554432 x 32, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2097152*(2x32^1)_NN_d
Tuning for shape 16x131072*(2x32^2)_NN_d
Tuning for shape 16x8192*(2x32^3)_NN_d
Tuning for shape 16x512*(2x32^4)_NN_d
Tuning for shape 16x32*(2x32^5)_NN_d
Tuning for shape 16x131072*(2x32^1)_NN_d
Tuning for shape 16x8192*(2x32^2)_NN_d
Tuning for shape 16x512*(2x32^3)_NN_d
Tuning for shape 16x32*(2x32^4)_NN_d
Tuning for shape 16x8192*(2x32^1)_NN_d
Tuning for shape 16x512*(2x32^2)_NN_d
Tuning for shape 16x32*(2x32^3)_NN_d
Tuning for shape 16x512*(2x32^1)_NN_d
Tuning for shape 16x32*(2x32^2)_NN_d
Tuning for shape 16x32*(2x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x32^5  &  1.000 & 1.000 & 123.980 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2] with F_0 [2, 64] x to produce Y[1, 64]
Matmul: 1 x 64 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x64^1  &  1.000 & 1.000 & 0.001 & 996.515
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2] with F_0 [2, 64] x to produce Y[4, 64]
Matmul: 4 x 64 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x64^1  &  1.000 & 1.000 & 0.003 & 310.782
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2] with F_0 [2, 64] x to produce Y[16, 64]
Matmul: 16 x 64 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x64^1  &  1.000 & 1.000 & 0.013 & 75.263
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2] with F_0 [2, 64] x to produce Y[64, 64]
Matmul: 64 x 64 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x64^1  &  1.000 & 1.000 & 0.050 & 19.962
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2] with F_0 [2, 64] x to produce Y[256, 64]
Matmul: 256 x 64 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x64^1  &  1.000 & 1.000 & 0.211 & 4.742
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 2] with F_0 [2, 64] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x64^1  &  1.000 & 1.000 & 0.873 & 1.145
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [2, 64] x F_1 [2, 64] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(2x64^1)_NN_d
Tuning for shape 1x4*(2x64^2)_NN_d
Tuning for shape 1x4*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x64^2  &  1.000 & 1.000 & 0.041 & 24.146
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [2, 64] x F_1 [2, 64] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(2x64^1)_NN_d
Tuning for shape 4x4*(2x64^2)_NN_d
Tuning for shape 4x4*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x64^2  &  1.000 & 1.000 & 0.153 & 6.519
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [2, 64] x F_1 [2, 64] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(2x64^1)_NN_d
Tuning for shape 16x4*(2x64^2)_NN_d
Tuning for shape 16x4*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x64^2  &  1.000 & 1.000 & 0.597 & 1.674
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [2, 64] x F_1 [2, 64] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(2x64^1)_NN_d
Tuning for shape 64x4*(2x64^2)_NN_d
Tuning for shape 64x4*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x64^2  &  1.000 & 1.000 & 2.343 & 0.427
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [2, 64] x F_1 [2, 64] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(2x64^1)_NN_d
Tuning for shape 256x4*(2x64^2)_NN_d
Tuning for shape 256x4*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x64^2  &  1.000 & 1.000 & 9.071 & 0.110
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [2, 64] x F_1 [2, 64] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(2x64^1)_NN_d
Tuning for shape 1024x4*(2x64^2)_NN_d
Tuning for shape 1024x4*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x64^2  &  1.000 & 1.000 & 37.691 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [2, 64] x F_1 [2, 64] x F_2 [2, 64] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8192*(2x64^1)_NN_d
Tuning for shape 1x256*(2x64^2)_NN_d
Tuning for shape 1x8*(2x64^3)_NN_d
Tuning for shape 1x256*(2x64^1)_NN_d
Tuning for shape 1x8*(2x64^2)_NN_d
Tuning for shape 1x8*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x64^3  &  1.000 & 1.000 & 2.033 & 0.492
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [2, 64] x F_1 [2, 64] x F_2 [2, 64] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8192*(2x64^1)_NN_d
Tuning for shape 4x256*(2x64^2)_NN_d
Tuning for shape 4x8*(2x64^3)_NN_d
Tuning for shape 4x256*(2x64^1)_NN_d
Tuning for shape 4x8*(2x64^2)_NN_d
Tuning for shape 4x8*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x64^3  &  1.000 & 1.000 & 7.703 & 0.130
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [2, 64] x F_1 [2, 64] x F_2 [2, 64] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8192*(2x64^1)_NN_d
Tuning for shape 16x256*(2x64^2)_NN_d
Tuning for shape 16x8*(2x64^3)_NN_d
Tuning for shape 16x256*(2x64^1)_NN_d
Tuning for shape 16x8*(2x64^2)_NN_d
Tuning for shape 16x8*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x64^3  &  1.000 & 1.000 & 30.796 & 0.032
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [2, 64] x F_1 [2, 64] x F_2 [2, 64] x to produce Y[64, 262144]
Matmul: 64 x 262144 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8192*(2x64^1)_NN_d
Tuning for shape 64x256*(2x64^2)_NN_d
Tuning for shape 64x8*(2x64^3)_NN_d
Tuning for shape 64x256*(2x64^1)_NN_d
Tuning for shape 64x8*(2x64^2)_NN_d
Tuning for shape 64x8*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x64^3  &  1.000 & 1.000 & 120.970 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [2, 64] x F_1 [2, 64] x F_2 [2, 64] x to produce Y[256, 262144]
Matmul: 256 x 262144 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8192*(2x64^1)_NN_d
Tuning for shape 256x256*(2x64^2)_NN_d
Tuning for shape 256x8*(2x64^3)_NN_d
Tuning for shape 256x256*(2x64^1)_NN_d
Tuning for shape 256x8*(2x64^2)_NN_d
Tuning for shape 256x8*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x64^3  &  1.000 & 1.000 & 129.053 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [2, 64] x F_1 [2, 64] x F_2 [2, 64] x to produce Y[1024, 262144]
Matmul: 1024 x 262144 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8192*(2x64^1)_NN_d
Tuning for shape 1024x256*(2x64^2)_NN_d
Tuning for shape 1024x8*(2x64^3)_NN_d
Tuning for shape 1024x256*(2x64^1)_NN_d
Tuning for shape 1024x8*(2x64^2)_NN_d
Tuning for shape 1024x8*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x64^3  &  1.000 & 1.000 & 132.717 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [2, 64] x F_1 [2, 64] x F_2 [2, 64] x F_3 [2, 64] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x524288*(2x64^1)_NN_d
Tuning for shape 1x16384*(2x64^2)_NN_d
Tuning for shape 1x512*(2x64^3)_NN_d
Tuning for shape 1x16*(2x64^4)_NN_d
Tuning for shape 1x16384*(2x64^1)_NN_d
Tuning for shape 1x512*(2x64^2)_NN_d
Tuning for shape 1x16*(2x64^3)_NN_d
Tuning for shape 1x512*(2x64^1)_NN_d
Tuning for shape 1x16*(2x64^2)_NN_d
Tuning for shape 1x16*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x64^4  &  1.000 & 1.000 & 86.878 & 0.012
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [2, 64] x F_1 [2, 64] x F_2 [2, 64] x F_3 [2, 64] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x524288*(2x64^1)_NN_d
Tuning for shape 4x16384*(2x64^2)_NN_d
Tuning for shape 4x512*(2x64^3)_NN_d
Tuning for shape 4x16*(2x64^4)_NN_d
Tuning for shape 4x16384*(2x64^1)_NN_d
Tuning for shape 4x512*(2x64^2)_NN_d
Tuning for shape 4x16*(2x64^3)_NN_d
Tuning for shape 4x512*(2x64^1)_NN_d
Tuning for shape 4x16*(2x64^2)_NN_d
Tuning for shape 4x16*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x64^4  &  1.000 & 1.000 & 111.179 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 2 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [2, 64] x F_1 [2, 64] x F_2 [2, 64] x F_3 [2, 64] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x524288*(2x64^1)_NN_d
Tuning for shape 16x16384*(2x64^2)_NN_d
Tuning for shape 16x512*(2x64^3)_NN_d
Tuning for shape 16x16*(2x64^4)_NN_d
Tuning for shape 16x16384*(2x64^1)_NN_d
Tuning for shape 16x512*(2x64^2)_NN_d
Tuning for shape 16x16*(2x64^3)_NN_d
Tuning for shape 16x512*(2x64^1)_NN_d
Tuning for shape 16x16*(2x64^2)_NN_d
Tuning for shape 16x16*(2x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x64^4  &  1.000 & 1.000 & 131.614 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2] with F_0 [2, 128] x to produce Y[1, 128]
Matmul: 1 x 128 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x128^1  &  1.000 & 1.000 & 0.002 & 536.302
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2] with F_0 [2, 128] x to produce Y[4, 128]
Matmul: 4 x 128 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x128^1  &  1.000 & 1.000 & 0.007 & 146.573
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2] with F_0 [2, 128] x to produce Y[16, 128]
Matmul: 16 x 128 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x128^1  &  1.000 & 1.000 & 0.027 & 36.827
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2] with F_0 [2, 128] x to produce Y[64, 128]
Matmul: 64 x 128 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x128^1  &  1.000 & 1.000 & 0.104 & 9.641
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2] with F_0 [2, 128] x to produce Y[256, 128]
Matmul: 256 x 128 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x128^1  &  1.000 & 1.000 & 0.442 & 2.261
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 2] with F_0 [2, 128] x to produce Y[1024, 128]
Matmul: 1024 x 128 x 2, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x128^1  &  1.000 & 1.000 & 1.715 & 0.583
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [2, 128] x F_1 [2, 128] x to produce Y[1, 16384]
Matmul: 1 x 16384 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(2x128^1)_NN_d
Tuning for shape 1x4*(2x128^2)_NN_d
Tuning for shape 1x4*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x128^2  &  1.000 & 1.000 & 0.159 & 6.305
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [2, 128] x F_1 [2, 128] x to produce Y[4, 16384]
Matmul: 4 x 16384 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(2x128^1)_NN_d
Tuning for shape 4x4*(2x128^2)_NN_d
Tuning for shape 4x4*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x128^2  &  1.000 & 1.000 & 0.608 & 1.646
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [2, 128] x F_1 [2, 128] x to produce Y[16, 16384]
Matmul: 16 x 16384 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(2x128^1)_NN_d
Tuning for shape 16x4*(2x128^2)_NN_d
Tuning for shape 16x4*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x128^2  &  1.000 & 1.000 & 2.364 & 0.423
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [2, 128] x F_1 [2, 128] x to produce Y[64, 16384]
Matmul: 64 x 16384 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(2x128^1)_NN_d
Tuning for shape 64x4*(2x128^2)_NN_d
Tuning for shape 64x4*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x128^2  &  1.000 & 1.000 & 9.029 & 0.111
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [2, 128] x F_1 [2, 128] x to produce Y[256, 16384]
Matmul: 256 x 16384 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(2x128^1)_NN_d
Tuning for shape 256x4*(2x128^2)_NN_d
Tuning for shape 256x4*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x128^2  &  1.000 & 1.000 & 36.225 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [2, 128] x F_1 [2, 128] x to produce Y[1024, 16384]
Matmul: 1024 x 16384 x 4, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x256*(2x128^1)_NN_d
Tuning for shape 1024x4*(2x128^2)_NN_d
Tuning for shape 1024x4*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_2x128^2  &  1.000 & 1.000 & 126.644 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [2, 128] x F_1 [2, 128] x F_2 [2, 128] x to produce Y[1, 2097152]
Matmul: 1 x 2097152 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32768*(2x128^1)_NN_d
Tuning for shape 1x512*(2x128^2)_NN_d
Tuning for shape 1x8*(2x128^3)_NN_d
Tuning for shape 1x512*(2x128^1)_NN_d
Tuning for shape 1x8*(2x128^2)_NN_d
Tuning for shape 1x8*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x128^3  &  1.000 & 1.000 & 16.351 & 0.061
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [2, 128] x F_1 [2, 128] x F_2 [2, 128] x to produce Y[4, 2097152]
Matmul: 4 x 2097152 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32768*(2x128^1)_NN_d
Tuning for shape 4x512*(2x128^2)_NN_d
Tuning for shape 4x8*(2x128^3)_NN_d
Tuning for shape 4x512*(2x128^1)_NN_d
Tuning for shape 4x8*(2x128^2)_NN_d
Tuning for shape 4x8*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_2x128^3  &  1.000 & 1.000 & 60.477 & 0.017
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [2, 128] x F_1 [2, 128] x F_2 [2, 128] x to produce Y[16, 2097152]
Matmul: 16 x 2097152 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32768*(2x128^1)_NN_d
Tuning for shape 16x512*(2x128^2)_NN_d
Tuning for shape 16x8*(2x128^3)_NN_d
Tuning for shape 16x512*(2x128^1)_NN_d
Tuning for shape 16x8*(2x128^2)_NN_d
Tuning for shape 16x8*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_2x128^3  &  1.000 & 1.000 & 125.870 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [2, 128] x F_1 [2, 128] x F_2 [2, 128] x to produce Y[64, 2097152]
Matmul: 64 x 2097152 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32768*(2x128^1)_NN_d
Tuning for shape 64x512*(2x128^2)_NN_d
Tuning for shape 64x8*(2x128^3)_NN_d
Tuning for shape 64x512*(2x128^1)_NN_d
Tuning for shape 64x8*(2x128^2)_NN_d
Tuning for shape 64x8*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_2x128^3  &  1.000 & 1.000 & 135.284 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [2, 128] x F_1 [2, 128] x F_2 [2, 128] x to produce Y[256, 2097152]
Matmul: 256 x 2097152 x 8, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32768*(2x128^1)_NN_d
Tuning for shape 256x512*(2x128^2)_NN_d
Tuning for shape 256x8*(2x128^3)_NN_d
Tuning for shape 256x512*(2x128^1)_NN_d
Tuning for shape 256x8*(2x128^2)_NN_d
Tuning for shape 256x8*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_2x128^3  &  1.000 & 1.000 & 135.833 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 2 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [2, 128] x F_1 [2, 128] x F_2 [2, 128] x F_3 [2, 128] x to produce Y[1, 268435456]
Matmul: 1 x 268435456 x 16, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4194304*(2x128^1)_NN_d
Tuning for shape 1x65536*(2x128^2)_NN_d
Tuning for shape 1x1024*(2x128^3)_NN_d
Tuning for shape 1x16*(2x128^4)_NN_d
Tuning for shape 1x65536*(2x128^1)_NN_d
Tuning for shape 1x1024*(2x128^2)_NN_d
Tuning for shape 1x16*(2x128^3)_NN_d
Tuning for shape 1x1024*(2x128^1)_NN_d
Tuning for shape 1x16*(2x128^2)_NN_d
Tuning for shape 1x16*(2x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_2x128^4  &  1.000 & 1.000 & 38.737 & 0.026
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [4, 2] x to produce Y[1, 2]
Matmul: 1 x 2 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^1  &  1.000 & 1.000 & 0.000 & 16140.938
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [4, 2] x to produce Y[4, 2]
Matmul: 4 x 2 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^1  &  1.000 & 1.000 & 0.000 & 4696.846
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [4, 2] x to produce Y[16, 2]
Matmul: 16 x 2 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^1  &  1.000 & 1.000 & 0.001 & 1186.831
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [4, 2] x to produce Y[64, 2]
Matmul: 64 x 2 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^1  &  1.000 & 1.000 & 0.003 & 296.615
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [4, 2] x to produce Y[256, 2]
Matmul: 256 x 2 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^1  &  1.000 & 1.000 & 0.012 & 80.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [4, 2] x to produce Y[1024, 2]
Matmul: 1024 x 2 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x2^1  &  1.000 & 1.000 & 0.052 & 19.253
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [4, 2] x F_1 [4, 2] x to produce Y[1, 4]
Matmul: 1 x 4 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(4x2^1)_NN_d
Tuning for shape 1x16*(4x2^2)_NN_d
Tuning for shape 1x16*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^2  &  1.000 & 1.000 & 0.000 & 4441.291
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [4, 2] x F_1 [4, 2] x to produce Y[4, 4]
Matmul: 4 x 4 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(4x2^1)_NN_d
Tuning for shape 4x16*(4x2^2)_NN_d
Tuning for shape 4x16*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^2  &  1.000 & 1.000 & 0.001 & 1188.864
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [4, 2] x F_1 [4, 2] x to produce Y[16, 4]
Matmul: 16 x 4 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(4x2^1)_NN_d
Tuning for shape 16x16*(4x2^2)_NN_d
Tuning for shape 16x16*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^2  &  1.000 & 1.000 & 0.003 & 300.623
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [4, 2] x F_1 [4, 2] x to produce Y[64, 4]
Matmul: 64 x 4 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(4x2^1)_NN_d
Tuning for shape 64x16*(4x2^2)_NN_d
Tuning for shape 64x16*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^2  &  1.000 & 1.000 & 0.014 & 72.697
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [4, 2] x F_1 [4, 2] x to produce Y[256, 4]
Matmul: 256 x 4 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(4x2^1)_NN_d
Tuning for shape 256x16*(4x2^2)_NN_d
Tuning for shape 256x16*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^2  &  1.000 & 1.000 & 0.055 & 18.237
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [4, 2] x F_1 [4, 2] x to produce Y[1024, 4]
Matmul: 1024 x 4 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(4x2^1)_NN_d
Tuning for shape 1024x16*(4x2^2)_NN_d
Tuning for shape 1024x16*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x2^2  &  1.000 & 1.000 & 0.216 & 4.637
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x to produce Y[1, 8]
Matmul: 1 x 8 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16*(4x2^1)_NN_d
Tuning for shape 1x32*(4x2^2)_NN_d
Tuning for shape 1x64*(4x2^3)_NN_d
Tuning for shape 1x32*(4x2^1)_NN_d
Tuning for shape 1x64*(4x2^2)_NN_d
Tuning for shape 1x64*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^3  &  1.000 & 1.000 & 0.001 & 1148.693
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x to produce Y[4, 8]
Matmul: 4 x 8 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16*(4x2^1)_NN_d
Tuning for shape 4x32*(4x2^2)_NN_d
Tuning for shape 4x64*(4x2^3)_NN_d
Tuning for shape 4x32*(4x2^1)_NN_d
Tuning for shape 4x64*(4x2^2)_NN_d
Tuning for shape 4x64*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^3  &  1.000 & 1.000 & 0.003 & 311.481
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x to produce Y[16, 8]
Matmul: 16 x 8 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16*(4x2^1)_NN_d
Tuning for shape 16x32*(4x2^2)_NN_d
Tuning for shape 16x64*(4x2^3)_NN_d
Tuning for shape 16x32*(4x2^1)_NN_d
Tuning for shape 16x64*(4x2^2)_NN_d
Tuning for shape 16x64*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^3  &  1.000 & 1.000 & 0.013 & 79.420
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x to produce Y[64, 8]
Matmul: 64 x 8 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16*(4x2^1)_NN_d
Tuning for shape 64x32*(4x2^2)_NN_d
Tuning for shape 64x64*(4x2^3)_NN_d
Tuning for shape 64x32*(4x2^1)_NN_d
Tuning for shape 64x64*(4x2^2)_NN_d
Tuning for shape 64x64*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^3  &  1.000 & 1.000 & 0.051 & 19.791
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x to produce Y[256, 8]
Matmul: 256 x 8 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16*(4x2^1)_NN_d
Tuning for shape 256x32*(4x2^2)_NN_d
Tuning for shape 256x64*(4x2^3)_NN_d
Tuning for shape 256x32*(4x2^1)_NN_d
Tuning for shape 256x64*(4x2^2)_NN_d
Tuning for shape 256x64*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^3  &  1.000 & 1.000 & 0.208 & 4.803
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x to produce Y[1024, 8]
Matmul: 1024 x 8 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16*(4x2^1)_NN_d
Tuning for shape 1024x32*(4x2^2)_NN_d
Tuning for shape 1024x64*(4x2^3)_NN_d
Tuning for shape 1024x32*(4x2^1)_NN_d
Tuning for shape 1024x64*(4x2^2)_NN_d
Tuning for shape 1024x64*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x2^3  &  1.000 & 1.000 & 0.845 & 1.183
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x to produce Y[1, 16]
Matmul: 1 x 16 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32*(4x2^1)_NN_d
Tuning for shape 1x64*(4x2^2)_NN_d
Tuning for shape 1x128*(4x2^3)_NN_d
Tuning for shape 1x256*(4x2^4)_NN_d
Tuning for shape 1x64*(4x2^1)_NN_d
Tuning for shape 1x128*(4x2^2)_NN_d
Tuning for shape 1x256*(4x2^3)_NN_d
Tuning for shape 1x128*(4x2^1)_NN_d
Tuning for shape 1x256*(4x2^2)_NN_d
Tuning for shape 1x256*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^4  &  1.000 & 1.000 & 0.003 & 330.669
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x to produce Y[4, 16]
Matmul: 4 x 16 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32*(4x2^1)_NN_d
Tuning for shape 4x64*(4x2^2)_NN_d
Tuning for shape 4x128*(4x2^3)_NN_d
Tuning for shape 4x256*(4x2^4)_NN_d
Tuning for shape 4x64*(4x2^1)_NN_d
Tuning for shape 4x128*(4x2^2)_NN_d
Tuning for shape 4x256*(4x2^3)_NN_d
Tuning for shape 4x128*(4x2^1)_NN_d
Tuning for shape 4x256*(4x2^2)_NN_d
Tuning for shape 4x256*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^4  &  1.000 & 1.000 & 0.012 & 84.789
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x to produce Y[16, 16]
Matmul: 16 x 16 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32*(4x2^1)_NN_d
Tuning for shape 16x64*(4x2^2)_NN_d
Tuning for shape 16x128*(4x2^3)_NN_d
Tuning for shape 16x256*(4x2^4)_NN_d
Tuning for shape 16x64*(4x2^1)_NN_d
Tuning for shape 16x128*(4x2^2)_NN_d
Tuning for shape 16x256*(4x2^3)_NN_d
Tuning for shape 16x128*(4x2^1)_NN_d
Tuning for shape 16x256*(4x2^2)_NN_d
Tuning for shape 16x256*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^4  &  1.000 & 1.000 & 0.047 & 21.241
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x to produce Y[64, 16]
Matmul: 64 x 16 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32*(4x2^1)_NN_d
Tuning for shape 64x64*(4x2^2)_NN_d
Tuning for shape 64x128*(4x2^3)_NN_d
Tuning for shape 64x256*(4x2^4)_NN_d
Tuning for shape 64x64*(4x2^1)_NN_d
Tuning for shape 64x128*(4x2^2)_NN_d
Tuning for shape 64x256*(4x2^3)_NN_d
Tuning for shape 64x128*(4x2^1)_NN_d
Tuning for shape 64x256*(4x2^2)_NN_d
Tuning for shape 64x256*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^4  &  1.000 & 1.000 & 0.183 & 5.468
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x to produce Y[256, 16]
Matmul: 256 x 16 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32*(4x2^1)_NN_d
Tuning for shape 256x64*(4x2^2)_NN_d
Tuning for shape 256x128*(4x2^3)_NN_d
Tuning for shape 256x256*(4x2^4)_NN_d
Tuning for shape 256x64*(4x2^1)_NN_d
Tuning for shape 256x128*(4x2^2)_NN_d
Tuning for shape 256x256*(4x2^3)_NN_d
Tuning for shape 256x128*(4x2^1)_NN_d
Tuning for shape 256x256*(4x2^2)_NN_d
Tuning for shape 256x256*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^4  &  1.000 & 1.000 & 0.751 & 1.332
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 256] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32*(4x2^1)_NN_d
Tuning for shape 1024x64*(4x2^2)_NN_d
Tuning for shape 1024x128*(4x2^3)_NN_d
Tuning for shape 1024x256*(4x2^4)_NN_d
Tuning for shape 1024x64*(4x2^1)_NN_d
Tuning for shape 1024x128*(4x2^2)_NN_d
Tuning for shape 1024x256*(4x2^3)_NN_d
Tuning for shape 1024x128*(4x2^1)_NN_d
Tuning for shape 1024x256*(4x2^2)_NN_d
Tuning for shape 1024x256*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x2^4  &  1.000 & 1.000 & 2.818 & 0.355
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1024] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x to produce Y[1, 32]
Matmul: 1 x 32 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x64*(4x2^1)_NN_d
Tuning for shape 1x128*(4x2^2)_NN_d
Tuning for shape 1x256*(4x2^3)_NN_d
Tuning for shape 1x512*(4x2^4)_NN_d
Tuning for shape 1x1024*(4x2^5)_NN_d
Tuning for shape 1x128*(4x2^1)_NN_d
Tuning for shape 1x256*(4x2^2)_NN_d
Tuning for shape 1x512*(4x2^3)_NN_d
Tuning for shape 1x1024*(4x2^4)_NN_d
Tuning for shape 1x256*(4x2^1)_NN_d
Tuning for shape 1x512*(4x2^2)_NN_d
Tuning for shape 1x1024*(4x2^3)_NN_d
Tuning for shape 1x512*(4x2^1)_NN_d
Tuning for shape 1x1024*(4x2^2)_NN_d
Tuning for shape 1x1024*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^5  &  1.000 & 1.000 & 0.011 & 90.540
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1024] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x to produce Y[4, 32]
Matmul: 4 x 32 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x64*(4x2^1)_NN_d
Tuning for shape 4x128*(4x2^2)_NN_d
Tuning for shape 4x256*(4x2^3)_NN_d
Tuning for shape 4x512*(4x2^4)_NN_d
Tuning for shape 4x1024*(4x2^5)_NN_d
Tuning for shape 4x128*(4x2^1)_NN_d
Tuning for shape 4x256*(4x2^2)_NN_d
Tuning for shape 4x512*(4x2^3)_NN_d
Tuning for shape 4x1024*(4x2^4)_NN_d
Tuning for shape 4x256*(4x2^1)_NN_d
Tuning for shape 4x512*(4x2^2)_NN_d
Tuning for shape 4x1024*(4x2^3)_NN_d
Tuning for shape 4x512*(4x2^1)_NN_d
Tuning for shape 4x1024*(4x2^2)_NN_d
Tuning for shape 4x1024*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^5  &  1.000 & 1.000 & 0.040 & 24.700
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1024] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x to produce Y[16, 32]
Matmul: 16 x 32 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x64*(4x2^1)_NN_d
Tuning for shape 16x128*(4x2^2)_NN_d
Tuning for shape 16x256*(4x2^3)_NN_d
Tuning for shape 16x512*(4x2^4)_NN_d
Tuning for shape 16x1024*(4x2^5)_NN_d
Tuning for shape 16x128*(4x2^1)_NN_d
Tuning for shape 16x256*(4x2^2)_NN_d
Tuning for shape 16x512*(4x2^3)_NN_d
Tuning for shape 16x1024*(4x2^4)_NN_d
Tuning for shape 16x256*(4x2^1)_NN_d
Tuning for shape 16x512*(4x2^2)_NN_d
Tuning for shape 16x1024*(4x2^3)_NN_d
Tuning for shape 16x512*(4x2^1)_NN_d
Tuning for shape 16x1024*(4x2^2)_NN_d
Tuning for shape 16x1024*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^5  &  1.000 & 1.000 & 0.163 & 6.123
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1024] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x to produce Y[64, 32]
Matmul: 64 x 32 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x64*(4x2^1)_NN_d
Tuning for shape 64x128*(4x2^2)_NN_d
Tuning for shape 64x256*(4x2^3)_NN_d
Tuning for shape 64x512*(4x2^4)_NN_d
Tuning for shape 64x1024*(4x2^5)_NN_d
Tuning for shape 64x128*(4x2^1)_NN_d
Tuning for shape 64x256*(4x2^2)_NN_d
Tuning for shape 64x512*(4x2^3)_NN_d
Tuning for shape 64x1024*(4x2^4)_NN_d
Tuning for shape 64x256*(4x2^1)_NN_d
Tuning for shape 64x512*(4x2^2)_NN_d
Tuning for shape 64x1024*(4x2^3)_NN_d
Tuning for shape 64x512*(4x2^1)_NN_d
Tuning for shape 64x1024*(4x2^2)_NN_d
Tuning for shape 64x1024*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^5  &  1.000 & 1.000 & 0.665 & 1.503
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1024] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x to produce Y[256, 32]
Matmul: 256 x 32 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x64*(4x2^1)_NN_d
Tuning for shape 256x128*(4x2^2)_NN_d
Tuning for shape 256x256*(4x2^3)_NN_d
Tuning for shape 256x512*(4x2^4)_NN_d
Tuning for shape 256x1024*(4x2^5)_NN_d
Tuning for shape 256x128*(4x2^1)_NN_d
Tuning for shape 256x256*(4x2^2)_NN_d
Tuning for shape 256x512*(4x2^3)_NN_d
Tuning for shape 256x1024*(4x2^4)_NN_d
Tuning for shape 256x256*(4x2^1)_NN_d
Tuning for shape 256x512*(4x2^2)_NN_d
Tuning for shape 256x1024*(4x2^3)_NN_d
Tuning for shape 256x512*(4x2^1)_NN_d
Tuning for shape 256x1024*(4x2^2)_NN_d
Tuning for shape 256x1024*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^5  &  1.000 & 1.000 & 2.709 & 0.369
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 5 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 1024] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x to produce Y[1024, 32]
Matmul: 1024 x 32 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x64*(4x2^1)_NN_d
Tuning for shape 1024x128*(4x2^2)_NN_d
Tuning for shape 1024x256*(4x2^3)_NN_d
Tuning for shape 1024x512*(4x2^4)_NN_d
Tuning for shape 1024x1024*(4x2^5)_NN_d
Tuning for shape 1024x128*(4x2^1)_NN_d
Tuning for shape 1024x256*(4x2^2)_NN_d
Tuning for shape 1024x512*(4x2^3)_NN_d
Tuning for shape 1024x1024*(4x2^4)_NN_d
Tuning for shape 1024x256*(4x2^1)_NN_d
Tuning for shape 1024x512*(4x2^2)_NN_d
Tuning for shape 1024x1024*(4x2^3)_NN_d
Tuning for shape 1024x512*(4x2^1)_NN_d
Tuning for shape 1024x1024*(4x2^2)_NN_d
Tuning for shape 1024x1024*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x2^5  &  1.000 & 1.000 & 10.822 & 0.092
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x to produce Y[1, 64]
Matmul: 1 x 64 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(4x2^1)_NN_d
Tuning for shape 1x256*(4x2^2)_NN_d
Tuning for shape 1x512*(4x2^3)_NN_d
Tuning for shape 1x1024*(4x2^4)_NN_d
Tuning for shape 1x2048*(4x2^5)_NN_d
Tuning for shape 1x4096*(4x2^6)_NN_d
Tuning for shape 1x256*(4x2^1)_NN_d
Tuning for shape 1x512*(4x2^2)_NN_d
Tuning for shape 1x1024*(4x2^3)_NN_d
Tuning for shape 1x2048*(4x2^4)_NN_d
Tuning for shape 1x4096*(4x2^5)_NN_d
Tuning for shape 1x512*(4x2^1)_NN_d
Tuning for shape 1x1024*(4x2^2)_NN_d
Tuning for shape 1x2048*(4x2^3)_NN_d
Tuning for shape 1x4096*(4x2^4)_NN_d
Tuning for shape 1x1024*(4x2^1)_NN_d
Tuning for shape 1x2048*(4x2^2)_NN_d
Tuning for shape 1x4096*(4x2^3)_NN_d
Tuning for shape 1x2048*(4x2^1)_NN_d
Tuning for shape 1x4096*(4x2^2)_NN_d
Tuning for shape 1x4096*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^6  &  1.000 & 1.000 & 0.040 & 25.240
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x to produce Y[4, 64]
Matmul: 4 x 64 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(4x2^1)_NN_d
Tuning for shape 4x256*(4x2^2)_NN_d
Tuning for shape 4x512*(4x2^3)_NN_d
Tuning for shape 4x1024*(4x2^4)_NN_d
Tuning for shape 4x2048*(4x2^5)_NN_d
Tuning for shape 4x4096*(4x2^6)_NN_d
Tuning for shape 4x256*(4x2^1)_NN_d
Tuning for shape 4x512*(4x2^2)_NN_d
Tuning for shape 4x1024*(4x2^3)_NN_d
Tuning for shape 4x2048*(4x2^4)_NN_d
Tuning for shape 4x4096*(4x2^5)_NN_d
Tuning for shape 4x512*(4x2^1)_NN_d
Tuning for shape 4x1024*(4x2^2)_NN_d
Tuning for shape 4x2048*(4x2^3)_NN_d
Tuning for shape 4x4096*(4x2^4)_NN_d
Tuning for shape 4x1024*(4x2^1)_NN_d
Tuning for shape 4x2048*(4x2^2)_NN_d
Tuning for shape 4x4096*(4x2^3)_NN_d
Tuning for shape 4x2048*(4x2^1)_NN_d
Tuning for shape 4x4096*(4x2^2)_NN_d
Tuning for shape 4x4096*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^6  &  1.000 & 1.000 & 0.151 & 6.639
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x to produce Y[16, 64]
Matmul: 16 x 64 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(4x2^1)_NN_d
Tuning for shape 16x256*(4x2^2)_NN_d
Tuning for shape 16x512*(4x2^3)_NN_d
Tuning for shape 16x1024*(4x2^4)_NN_d
Tuning for shape 16x2048*(4x2^5)_NN_d
Tuning for shape 16x4096*(4x2^6)_NN_d
Tuning for shape 16x256*(4x2^1)_NN_d
Tuning for shape 16x512*(4x2^2)_NN_d
Tuning for shape 16x1024*(4x2^3)_NN_d
Tuning for shape 16x2048*(4x2^4)_NN_d
Tuning for shape 16x4096*(4x2^5)_NN_d
Tuning for shape 16x512*(4x2^1)_NN_d
Tuning for shape 16x1024*(4x2^2)_NN_d
Tuning for shape 16x2048*(4x2^3)_NN_d
Tuning for shape 16x4096*(4x2^4)_NN_d
Tuning for shape 16x1024*(4x2^1)_NN_d
Tuning for shape 16x2048*(4x2^2)_NN_d
Tuning for shape 16x4096*(4x2^3)_NN_d
Tuning for shape 16x2048*(4x2^1)_NN_d
Tuning for shape 16x4096*(4x2^2)_NN_d
Tuning for shape 16x4096*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^6  &  1.000 & 1.000 & 0.609 & 1.641
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 6 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x to produce Y[64, 64]
Matmul: 64 x 64 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(4x2^1)_NN_d
Tuning for shape 64x256*(4x2^2)_NN_d
Tuning for shape 64x512*(4x2^3)_NN_d
Tuning for shape 64x1024*(4x2^4)_NN_d
Tuning for shape 64x2048*(4x2^5)_NN_d
Tuning for shape 64x4096*(4x2^6)_NN_d
Tuning for shape 64x256*(4x2^1)_NN_d
Tuning for shape 64x512*(4x2^2)_NN_d
Tuning for shape 64x1024*(4x2^3)_NN_d
Tuning for shape 64x2048*(4x2^4)_NN_d
Tuning for shape 64x4096*(4x2^5)_NN_d
Tuning for shape 64x512*(4x2^1)_NN_d
Tuning for shape 64x1024*(4x2^2)_NN_d
Tuning for shape 64x2048*(4x2^3)_NN_d
Tuning for shape 64x4096*(4x2^4)_NN_d
Tuning for shape 64x1024*(4x2^1)_NN_d
Tuning for shape 64x2048*(4x2^2)_NN_d
Tuning for shape 64x4096*(4x2^3)_NN_d
Tuning for shape 64x2048*(4x2^1)_NN_d
Tuning for shape 64x4096*(4x2^2)_NN_d
Tuning for shape 64x4096*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^6  &  1.000 & 1.000 & 2.415 & 0.414
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 6 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x to produce Y[256, 64]
Matmul: 256 x 64 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(4x2^1)_NN_d
Tuning for shape 256x256*(4x2^2)_NN_d
Tuning for shape 256x512*(4x2^3)_NN_d
Tuning for shape 256x1024*(4x2^4)_NN_d
Tuning for shape 256x2048*(4x2^5)_NN_d
Tuning for shape 256x4096*(4x2^6)_NN_d
Tuning for shape 256x256*(4x2^1)_NN_d
Tuning for shape 256x512*(4x2^2)_NN_d
Tuning for shape 256x1024*(4x2^3)_NN_d
Tuning for shape 256x2048*(4x2^4)_NN_d
Tuning for shape 256x4096*(4x2^5)_NN_d
Tuning for shape 256x512*(4x2^1)_NN_d
Tuning for shape 256x1024*(4x2^2)_NN_d
Tuning for shape 256x2048*(4x2^3)_NN_d
Tuning for shape 256x4096*(4x2^4)_NN_d
Tuning for shape 256x1024*(4x2^1)_NN_d
Tuning for shape 256x2048*(4x2^2)_NN_d
Tuning for shape 256x4096*(4x2^3)_NN_d
Tuning for shape 256x2048*(4x2^1)_NN_d
Tuning for shape 256x4096*(4x2^2)_NN_d
Tuning for shape 256x4096*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^6  &  1.000 & 1.000 & 9.494 & 0.105
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 6 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(4x2^1)_NN_d
Tuning for shape 1024x256*(4x2^2)_NN_d
Tuning for shape 1024x512*(4x2^3)_NN_d
Tuning for shape 1024x1024*(4x2^4)_NN_d
Tuning for shape 1024x2048*(4x2^5)_NN_d
Tuning for shape 1024x4096*(4x2^6)_NN_d
Tuning for shape 1024x256*(4x2^1)_NN_d
Tuning for shape 1024x512*(4x2^2)_NN_d
Tuning for shape 1024x1024*(4x2^3)_NN_d
Tuning for shape 1024x2048*(4x2^4)_NN_d
Tuning for shape 1024x4096*(4x2^5)_NN_d
Tuning for shape 1024x512*(4x2^1)_NN_d
Tuning for shape 1024x1024*(4x2^2)_NN_d
Tuning for shape 1024x2048*(4x2^3)_NN_d
Tuning for shape 1024x4096*(4x2^4)_NN_d
Tuning for shape 1024x1024*(4x2^1)_NN_d
Tuning for shape 1024x2048*(4x2^2)_NN_d
Tuning for shape 1024x4096*(4x2^3)_NN_d
Tuning for shape 1024x2048*(4x2^1)_NN_d
Tuning for shape 1024x4096*(4x2^2)_NN_d
Tuning for shape 1024x4096*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x2^6  &  1.000 & 1.000 & 30.884 & 0.032
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16384] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x to produce Y[1, 128]
Matmul: 1 x 128 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(4x2^1)_NN_d
Tuning for shape 1x512*(4x2^2)_NN_d
Tuning for shape 1x1024*(4x2^3)_NN_d
Tuning for shape 1x2048*(4x2^4)_NN_d
Tuning for shape 1x4096*(4x2^5)_NN_d
Tuning for shape 1x8192*(4x2^6)_NN_d
Tuning for shape 1x16384*(4x2^7)_NN_d
Tuning for shape 1x512*(4x2^1)_NN_d
Tuning for shape 1x1024*(4x2^2)_NN_d
Tuning for shape 1x2048*(4x2^3)_NN_d
Tuning for shape 1x4096*(4x2^4)_NN_d
Tuning for shape 1x8192*(4x2^5)_NN_d
Tuning for shape 1x16384*(4x2^6)_NN_d
Tuning for shape 1x1024*(4x2^1)_NN_d
Tuning for shape 1x2048*(4x2^2)_NN_d
Tuning for shape 1x4096*(4x2^3)_NN_d
Tuning for shape 1x8192*(4x2^4)_NN_d
Tuning for shape 1x16384*(4x2^5)_NN_d
Tuning for shape 1x2048*(4x2^1)_NN_d
Tuning for shape 1x4096*(4x2^2)_NN_d
Tuning for shape 1x8192*(4x2^3)_NN_d
Tuning for shape 1x16384*(4x2^4)_NN_d
Tuning for shape 1x4096*(4x2^1)_NN_d
Tuning for shape 1x8192*(4x2^2)_NN_d
Tuning for shape 1x16384*(4x2^3)_NN_d
Tuning for shape 1x8192*(4x2^1)_NN_d
Tuning for shape 1x16384*(4x2^2)_NN_d
Tuning for shape 1x16384*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^7  &  1.000 & 1.000 & 0.140 & 7.144
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 7 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16384] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x to produce Y[4, 128]
Matmul: 4 x 128 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(4x2^1)_NN_d
Tuning for shape 4x512*(4x2^2)_NN_d
Tuning for shape 4x1024*(4x2^3)_NN_d
Tuning for shape 4x2048*(4x2^4)_NN_d
Tuning for shape 4x4096*(4x2^5)_NN_d
Tuning for shape 4x8192*(4x2^6)_NN_d
Tuning for shape 4x16384*(4x2^7)_NN_d
Tuning for shape 4x512*(4x2^1)_NN_d
Tuning for shape 4x1024*(4x2^2)_NN_d
Tuning for shape 4x2048*(4x2^3)_NN_d
Tuning for shape 4x4096*(4x2^4)_NN_d
Tuning for shape 4x8192*(4x2^5)_NN_d
Tuning for shape 4x16384*(4x2^6)_NN_d
Tuning for shape 4x1024*(4x2^1)_NN_d
Tuning for shape 4x2048*(4x2^2)_NN_d
Tuning for shape 4x4096*(4x2^3)_NN_d
Tuning for shape 4x8192*(4x2^4)_NN_d
Tuning for shape 4x16384*(4x2^5)_NN_d
Tuning for shape 4x2048*(4x2^1)_NN_d
Tuning for shape 4x4096*(4x2^2)_NN_d
Tuning for shape 4x8192*(4x2^3)_NN_d
Tuning for shape 4x16384*(4x2^4)_NN_d
Tuning for shape 4x4096*(4x2^1)_NN_d
Tuning for shape 4x8192*(4x2^2)_NN_d
Tuning for shape 4x16384*(4x2^3)_NN_d
Tuning for shape 4x8192*(4x2^1)_NN_d
Tuning for shape 4x16384*(4x2^2)_NN_d
Tuning for shape 4x16384*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^7  &  1.000 & 1.000 & 0.545 & 1.836
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 7 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16384] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x to produce Y[16, 128]
Matmul: 16 x 128 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(4x2^1)_NN_d
Tuning for shape 16x512*(4x2^2)_NN_d
Tuning for shape 16x1024*(4x2^3)_NN_d
Tuning for shape 16x2048*(4x2^4)_NN_d
Tuning for shape 16x4096*(4x2^5)_NN_d
Tuning for shape 16x8192*(4x2^6)_NN_d
Tuning for shape 16x16384*(4x2^7)_NN_d
Tuning for shape 16x512*(4x2^1)_NN_d
Tuning for shape 16x1024*(4x2^2)_NN_d
Tuning for shape 16x2048*(4x2^3)_NN_d
Tuning for shape 16x4096*(4x2^4)_NN_d
Tuning for shape 16x8192*(4x2^5)_NN_d
Tuning for shape 16x16384*(4x2^6)_NN_d
Tuning for shape 16x1024*(4x2^1)_NN_d
Tuning for shape 16x2048*(4x2^2)_NN_d
Tuning for shape 16x4096*(4x2^3)_NN_d
Tuning for shape 16x8192*(4x2^4)_NN_d
Tuning for shape 16x16384*(4x2^5)_NN_d
Tuning for shape 16x2048*(4x2^1)_NN_d
Tuning for shape 16x4096*(4x2^2)_NN_d
Tuning for shape 16x8192*(4x2^3)_NN_d
Tuning for shape 16x16384*(4x2^4)_NN_d
Tuning for shape 16x4096*(4x2^1)_NN_d
Tuning for shape 16x8192*(4x2^2)_NN_d
Tuning for shape 16x16384*(4x2^3)_NN_d
Tuning for shape 16x8192*(4x2^1)_NN_d
Tuning for shape 16x16384*(4x2^2)_NN_d
Tuning for shape 16x16384*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^7  &  1.000 & 1.000 & 1.471 & 0.680
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 7 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16384] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x to produce Y[64, 128]
Matmul: 64 x 128 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(4x2^1)_NN_d
Tuning for shape 64x512*(4x2^2)_NN_d
Tuning for shape 64x1024*(4x2^3)_NN_d
Tuning for shape 64x2048*(4x2^4)_NN_d
Tuning for shape 64x4096*(4x2^5)_NN_d
Tuning for shape 64x8192*(4x2^6)_NN_d
Tuning for shape 64x16384*(4x2^7)_NN_d
Tuning for shape 64x512*(4x2^1)_NN_d
Tuning for shape 64x1024*(4x2^2)_NN_d
Tuning for shape 64x2048*(4x2^3)_NN_d
Tuning for shape 64x4096*(4x2^4)_NN_d
Tuning for shape 64x8192*(4x2^5)_NN_d
Tuning for shape 64x16384*(4x2^6)_NN_d
Tuning for shape 64x1024*(4x2^1)_NN_d
Tuning for shape 64x2048*(4x2^2)_NN_d
Tuning for shape 64x4096*(4x2^3)_NN_d
Tuning for shape 64x8192*(4x2^4)_NN_d
Tuning for shape 64x16384*(4x2^5)_NN_d
Tuning for shape 64x2048*(4x2^1)_NN_d
Tuning for shape 64x4096*(4x2^2)_NN_d
Tuning for shape 64x8192*(4x2^3)_NN_d
Tuning for shape 64x16384*(4x2^4)_NN_d
Tuning for shape 64x4096*(4x2^1)_NN_d
Tuning for shape 64x8192*(4x2^2)_NN_d
Tuning for shape 64x16384*(4x2^3)_NN_d
Tuning for shape 64x8192*(4x2^1)_NN_d
Tuning for shape 64x16384*(4x2^2)_NN_d
Tuning for shape 64x16384*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^7  &  1.000 & 1.000 & 8.760 & 0.114
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 7 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16384] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x to produce Y[256, 128]
Matmul: 256 x 128 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(4x2^1)_NN_d
Tuning for shape 256x512*(4x2^2)_NN_d
Tuning for shape 256x1024*(4x2^3)_NN_d
Tuning for shape 256x2048*(4x2^4)_NN_d
Tuning for shape 256x4096*(4x2^5)_NN_d
Tuning for shape 256x8192*(4x2^6)_NN_d
Tuning for shape 256x16384*(4x2^7)_NN_d
Tuning for shape 256x512*(4x2^1)_NN_d
Tuning for shape 256x1024*(4x2^2)_NN_d
Tuning for shape 256x2048*(4x2^3)_NN_d
Tuning for shape 256x4096*(4x2^4)_NN_d
Tuning for shape 256x8192*(4x2^5)_NN_d
Tuning for shape 256x16384*(4x2^6)_NN_d
Tuning for shape 256x1024*(4x2^1)_NN_d
Tuning for shape 256x2048*(4x2^2)_NN_d
Tuning for shape 256x4096*(4x2^3)_NN_d
Tuning for shape 256x8192*(4x2^4)_NN_d
Tuning for shape 256x16384*(4x2^5)_NN_d
Tuning for shape 256x2048*(4x2^1)_NN_d
Tuning for shape 256x4096*(4x2^2)_NN_d
Tuning for shape 256x8192*(4x2^3)_NN_d
Tuning for shape 256x16384*(4x2^4)_NN_d
Tuning for shape 256x4096*(4x2^1)_NN_d
Tuning for shape 256x8192*(4x2^2)_NN_d
Tuning for shape 256x16384*(4x2^3)_NN_d
Tuning for shape 256x8192*(4x2^1)_NN_d
Tuning for shape 256x16384*(4x2^2)_NN_d
Tuning for shape 256x16384*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^7  &  1.000 & 1.000 & 30.563 & 0.033
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 7 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16384] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x to produce Y[1024, 128]
Matmul: 1024 x 128 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x256*(4x2^1)_NN_d
Tuning for shape 1024x512*(4x2^2)_NN_d
Tuning for shape 1024x1024*(4x2^3)_NN_d
Tuning for shape 1024x2048*(4x2^4)_NN_d
Tuning for shape 1024x4096*(4x2^5)_NN_d
Tuning for shape 1024x8192*(4x2^6)_NN_d
Tuning for shape 1024x16384*(4x2^7)_NN_d
Tuning for shape 1024x512*(4x2^1)_NN_d
Tuning for shape 1024x1024*(4x2^2)_NN_d
Tuning for shape 1024x2048*(4x2^3)_NN_d
Tuning for shape 1024x4096*(4x2^4)_NN_d
Tuning for shape 1024x8192*(4x2^5)_NN_d
Tuning for shape 1024x16384*(4x2^6)_NN_d
Tuning for shape 1024x1024*(4x2^1)_NN_d
Tuning for shape 1024x2048*(4x2^2)_NN_d
Tuning for shape 1024x4096*(4x2^3)_NN_d
Tuning for shape 1024x8192*(4x2^4)_NN_d
Tuning for shape 1024x16384*(4x2^5)_NN_d
Tuning for shape 1024x2048*(4x2^1)_NN_d
Tuning for shape 1024x4096*(4x2^2)_NN_d
Tuning for shape 1024x8192*(4x2^3)_NN_d
Tuning for shape 1024x16384*(4x2^4)_NN_d
Tuning for shape 1024x4096*(4x2^1)_NN_d
Tuning for shape 1024x8192*(4x2^2)_NN_d
Tuning for shape 1024x16384*(4x2^3)_NN_d
Tuning for shape 1024x8192*(4x2^1)_NN_d
Tuning for shape 1024x16384*(4x2^2)_NN_d
Tuning for shape 1024x16384*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x2^7  &  1.000 & 1.000 & 31.639 & 0.032
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 8 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 65536] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x to produce Y[1, 256]
Matmul: 1 x 256 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(4x2^1)_NN_d
Tuning for shape 1x1024*(4x2^2)_NN_d
Tuning for shape 1x2048*(4x2^3)_NN_d
Tuning for shape 1x4096*(4x2^4)_NN_d
Tuning for shape 1x8192*(4x2^5)_NN_d
Tuning for shape 1x16384*(4x2^6)_NN_d
Tuning for shape 1x32768*(4x2^7)_NN_d
Tuning for shape 1x65536*(4x2^8)_NN_d
Tuning for shape 1x1024*(4x2^1)_NN_d
Tuning for shape 1x2048*(4x2^2)_NN_d
Tuning for shape 1x4096*(4x2^3)_NN_d
Tuning for shape 1x8192*(4x2^4)_NN_d
Tuning for shape 1x16384*(4x2^5)_NN_d
Tuning for shape 1x32768*(4x2^6)_NN_d
Tuning for shape 1x65536*(4x2^7)_NN_d
Tuning for shape 1x2048*(4x2^1)_NN_d
Tuning for shape 1x4096*(4x2^2)_NN_d
Tuning for shape 1x8192*(4x2^3)_NN_d
Tuning for shape 1x16384*(4x2^4)_NN_d
Tuning for shape 1x32768*(4x2^5)_NN_d
Tuning for shape 1x65536*(4x2^6)_NN_d
Tuning for shape 1x4096*(4x2^1)_NN_d
Tuning for shape 1x8192*(4x2^2)_NN_d
Tuning for shape 1x16384*(4x2^3)_NN_d
Tuning for shape 1x32768*(4x2^4)_NN_d
Tuning for shape 1x65536*(4x2^5)_NN_d
Tuning for shape 1x8192*(4x2^1)_NN_d
Tuning for shape 1x16384*(4x2^2)_NN_d
Tuning for shape 1x32768*(4x2^3)_NN_d
Tuning for shape 1x65536*(4x2^4)_NN_d
Tuning for shape 1x16384*(4x2^1)_NN_d
Tuning for shape 1x32768*(4x2^2)_NN_d
Tuning for shape 1x65536*(4x2^3)_NN_d
Tuning for shape 1x32768*(4x2^1)_NN_d
Tuning for shape 1x65536*(4x2^2)_NN_d
Tuning for shape 1x65536*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^8  &  1.000 & 1.000 & 0.516 & 1.937
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 8 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 65536] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x to produce Y[4, 256]
Matmul: 4 x 256 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(4x2^1)_NN_d
Tuning for shape 4x1024*(4x2^2)_NN_d
Tuning for shape 4x2048*(4x2^3)_NN_d
Tuning for shape 4x4096*(4x2^4)_NN_d
Tuning for shape 4x8192*(4x2^5)_NN_d
Tuning for shape 4x16384*(4x2^6)_NN_d
Tuning for shape 4x32768*(4x2^7)_NN_d
Tuning for shape 4x65536*(4x2^8)_NN_d
Tuning for shape 4x1024*(4x2^1)_NN_d
Tuning for shape 4x2048*(4x2^2)_NN_d
Tuning for shape 4x4096*(4x2^3)_NN_d
Tuning for shape 4x8192*(4x2^4)_NN_d
Tuning for shape 4x16384*(4x2^5)_NN_d
Tuning for shape 4x32768*(4x2^6)_NN_d
Tuning for shape 4x65536*(4x2^7)_NN_d
Tuning for shape 4x2048*(4x2^1)_NN_d
Tuning for shape 4x4096*(4x2^2)_NN_d
Tuning for shape 4x8192*(4x2^3)_NN_d
Tuning for shape 4x16384*(4x2^4)_NN_d
Tuning for shape 4x32768*(4x2^5)_NN_d
Tuning for shape 4x65536*(4x2^6)_NN_d
Tuning for shape 4x4096*(4x2^1)_NN_d
Tuning for shape 4x8192*(4x2^2)_NN_d
Tuning for shape 4x16384*(4x2^3)_NN_d
Tuning for shape 4x32768*(4x2^4)_NN_d
Tuning for shape 4x65536*(4x2^5)_NN_d
Tuning for shape 4x8192*(4x2^1)_NN_d
Tuning for shape 4x16384*(4x2^2)_NN_d
Tuning for shape 4x32768*(4x2^3)_NN_d
Tuning for shape 4x65536*(4x2^4)_NN_d
Tuning for shape 4x16384*(4x2^1)_NN_d
Tuning for shape 4x32768*(4x2^2)_NN_d
Tuning for shape 4x65536*(4x2^3)_NN_d
Tuning for shape 4x32768*(4x2^1)_NN_d
Tuning for shape 4x65536*(4x2^2)_NN_d
Tuning for shape 4x65536*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^8  &  1.000 & 1.000 & 2.034 & 0.492
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 8 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 65536] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x to produce Y[16, 256]
Matmul: 16 x 256 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(4x2^1)_NN_d
Tuning for shape 16x1024*(4x2^2)_NN_d
Tuning for shape 16x2048*(4x2^3)_NN_d
Tuning for shape 16x4096*(4x2^4)_NN_d
Tuning for shape 16x8192*(4x2^5)_NN_d
Tuning for shape 16x16384*(4x2^6)_NN_d
Tuning for shape 16x32768*(4x2^7)_NN_d
Tuning for shape 16x65536*(4x2^8)_NN_d
Tuning for shape 16x1024*(4x2^1)_NN_d
Tuning for shape 16x2048*(4x2^2)_NN_d
Tuning for shape 16x4096*(4x2^3)_NN_d
Tuning for shape 16x8192*(4x2^4)_NN_d
Tuning for shape 16x16384*(4x2^5)_NN_d
Tuning for shape 16x32768*(4x2^6)_NN_d
Tuning for shape 16x65536*(4x2^7)_NN_d
Tuning for shape 16x2048*(4x2^1)_NN_d
Tuning for shape 16x4096*(4x2^2)_NN_d
Tuning for shape 16x8192*(4x2^3)_NN_d
Tuning for shape 16x16384*(4x2^4)_NN_d
Tuning for shape 16x32768*(4x2^5)_NN_d
Tuning for shape 16x65536*(4x2^6)_NN_d
Tuning for shape 16x4096*(4x2^1)_NN_d
Tuning for shape 16x8192*(4x2^2)_NN_d
Tuning for shape 16x16384*(4x2^3)_NN_d
Tuning for shape 16x32768*(4x2^4)_NN_d
Tuning for shape 16x65536*(4x2^5)_NN_d
Tuning for shape 16x8192*(4x2^1)_NN_d
Tuning for shape 16x16384*(4x2^2)_NN_d
Tuning for shape 16x32768*(4x2^3)_NN_d
Tuning for shape 16x65536*(4x2^4)_NN_d
Tuning for shape 16x16384*(4x2^1)_NN_d
Tuning for shape 16x32768*(4x2^2)_NN_d
Tuning for shape 16x65536*(4x2^3)_NN_d
Tuning for shape 16x32768*(4x2^1)_NN_d
Tuning for shape 16x65536*(4x2^2)_NN_d
Tuning for shape 16x65536*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^8  &  1.000 & 1.000 & 8.114 & 0.123
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 8 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 65536] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x to produce Y[64, 256]
Matmul: 64 x 256 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x512*(4x2^1)_NN_d
Tuning for shape 64x1024*(4x2^2)_NN_d
Tuning for shape 64x2048*(4x2^3)_NN_d
Tuning for shape 64x4096*(4x2^4)_NN_d
Tuning for shape 64x8192*(4x2^5)_NN_d
Tuning for shape 64x16384*(4x2^6)_NN_d
Tuning for shape 64x32768*(4x2^7)_NN_d
Tuning for shape 64x65536*(4x2^8)_NN_d
Tuning for shape 64x1024*(4x2^1)_NN_d
Tuning for shape 64x2048*(4x2^2)_NN_d
Tuning for shape 64x4096*(4x2^3)_NN_d
Tuning for shape 64x8192*(4x2^4)_NN_d
Tuning for shape 64x16384*(4x2^5)_NN_d
Tuning for shape 64x32768*(4x2^6)_NN_d
Tuning for shape 64x65536*(4x2^7)_NN_d
Tuning for shape 64x2048*(4x2^1)_NN_d
Tuning for shape 64x4096*(4x2^2)_NN_d
Tuning for shape 64x8192*(4x2^3)_NN_d
Tuning for shape 64x16384*(4x2^4)_NN_d
Tuning for shape 64x32768*(4x2^5)_NN_d
Tuning for shape 64x65536*(4x2^6)_NN_d
Tuning for shape 64x4096*(4x2^1)_NN_d
Tuning for shape 64x8192*(4x2^2)_NN_d
Tuning for shape 64x16384*(4x2^3)_NN_d
Tuning for shape 64x32768*(4x2^4)_NN_d
Tuning for shape 64x65536*(4x2^5)_NN_d
Tuning for shape 64x8192*(4x2^1)_NN_d
Tuning for shape 64x16384*(4x2^2)_NN_d
Tuning for shape 64x32768*(4x2^3)_NN_d
Tuning for shape 64x65536*(4x2^4)_NN_d
Tuning for shape 64x16384*(4x2^1)_NN_d
Tuning for shape 64x32768*(4x2^2)_NN_d
Tuning for shape 64x65536*(4x2^3)_NN_d
Tuning for shape 64x32768*(4x2^1)_NN_d
Tuning for shape 64x65536*(4x2^2)_NN_d
Tuning for shape 64x65536*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^8  &  1.000 & 1.000 & 32.058 & 0.031
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 8 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 65536] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x to produce Y[256, 256]
Matmul: 256 x 256 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x512*(4x2^1)_NN_d
Tuning for shape 256x1024*(4x2^2)_NN_d
Tuning for shape 256x2048*(4x2^3)_NN_d
Tuning for shape 256x4096*(4x2^4)_NN_d
Tuning for shape 256x8192*(4x2^5)_NN_d
Tuning for shape 256x16384*(4x2^6)_NN_d
Tuning for shape 256x32768*(4x2^7)_NN_d
Tuning for shape 256x65536*(4x2^8)_NN_d
Tuning for shape 256x1024*(4x2^1)_NN_d
Tuning for shape 256x2048*(4x2^2)_NN_d
Tuning for shape 256x4096*(4x2^3)_NN_d
Tuning for shape 256x8192*(4x2^4)_NN_d
Tuning for shape 256x16384*(4x2^5)_NN_d
Tuning for shape 256x32768*(4x2^6)_NN_d
Tuning for shape 256x65536*(4x2^7)_NN_d
Tuning for shape 256x2048*(4x2^1)_NN_d
Tuning for shape 256x4096*(4x2^2)_NN_d
Tuning for shape 256x8192*(4x2^3)_NN_d
Tuning for shape 256x16384*(4x2^4)_NN_d
Tuning for shape 256x32768*(4x2^5)_NN_d
Tuning for shape 256x65536*(4x2^6)_NN_d
Tuning for shape 256x4096*(4x2^1)_NN_d
Tuning for shape 256x8192*(4x2^2)_NN_d
Tuning for shape 256x16384*(4x2^3)_NN_d
Tuning for shape 256x32768*(4x2^4)_NN_d
Tuning for shape 256x65536*(4x2^5)_NN_d
Tuning for shape 256x8192*(4x2^1)_NN_d
Tuning for shape 256x16384*(4x2^2)_NN_d
Tuning for shape 256x32768*(4x2^3)_NN_d
Tuning for shape 256x65536*(4x2^4)_NN_d
Tuning for shape 256x16384*(4x2^1)_NN_d
Tuning for shape 256x32768*(4x2^2)_NN_d
Tuning for shape 256x65536*(4x2^3)_NN_d
Tuning for shape 256x32768*(4x2^1)_NN_d
Tuning for shape 256x65536*(4x2^2)_NN_d
Tuning for shape 256x65536*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^8  &  1.000 & 1.000 & 27.410 & 0.036
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 8 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 65536] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x to produce Y[1024, 256]
Matmul: 1024 x 256 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x512*(4x2^1)_NN_d
Tuning for shape 1024x1024*(4x2^2)_NN_d
Tuning for shape 1024x2048*(4x2^3)_NN_d
Tuning for shape 1024x4096*(4x2^4)_NN_d
Tuning for shape 1024x8192*(4x2^5)_NN_d
Tuning for shape 1024x16384*(4x2^6)_NN_d
Tuning for shape 1024x32768*(4x2^7)_NN_d
Tuning for shape 1024x65536*(4x2^8)_NN_d
Tuning for shape 1024x1024*(4x2^1)_NN_d
Tuning for shape 1024x2048*(4x2^2)_NN_d
Tuning for shape 1024x4096*(4x2^3)_NN_d
Tuning for shape 1024x8192*(4x2^4)_NN_d
Tuning for shape 1024x16384*(4x2^5)_NN_d
Tuning for shape 1024x32768*(4x2^6)_NN_d
Tuning for shape 1024x65536*(4x2^7)_NN_d
Tuning for shape 1024x2048*(4x2^1)_NN_d
Tuning for shape 1024x4096*(4x2^2)_NN_d
Tuning for shape 1024x8192*(4x2^3)_NN_d
Tuning for shape 1024x16384*(4x2^4)_NN_d
Tuning for shape 1024x32768*(4x2^5)_NN_d
Tuning for shape 1024x65536*(4x2^6)_NN_d
Tuning for shape 1024x4096*(4x2^1)_NN_d
Tuning for shape 1024x8192*(4x2^2)_NN_d
Tuning for shape 1024x16384*(4x2^3)_NN_d
Tuning for shape 1024x32768*(4x2^4)_NN_d
Tuning for shape 1024x65536*(4x2^5)_NN_d
Tuning for shape 1024x8192*(4x2^1)_NN_d
Tuning for shape 1024x16384*(4x2^2)_NN_d
Tuning for shape 1024x32768*(4x2^3)_NN_d
Tuning for shape 1024x65536*(4x2^4)_NN_d
Tuning for shape 1024x16384*(4x2^1)_NN_d
Tuning for shape 1024x32768*(4x2^2)_NN_d
Tuning for shape 1024x65536*(4x2^3)_NN_d
Tuning for shape 1024x32768*(4x2^1)_NN_d
Tuning for shape 1024x65536*(4x2^2)_NN_d
Tuning for shape 1024x65536*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x2^8  &  1.000 & 1.000 & 27.545 & 0.036
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 9 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 262144] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x to produce Y[1, 512]
Matmul: 1 x 512 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(4x2^1)_NN_d
Tuning for shape 1x2048*(4x2^2)_NN_d
Tuning for shape 1x4096*(4x2^3)_NN_d
Tuning for shape 1x8192*(4x2^4)_NN_d
Tuning for shape 1x16384*(4x2^5)_NN_d
Tuning for shape 1x32768*(4x2^6)_NN_d
Tuning for shape 1x65536*(4x2^7)_NN_d
Tuning for shape 1x131072*(4x2^8)_NN_d
Tuning for shape 1x262144*(4x2^9)_NN_d
Tuning for shape 1x2048*(4x2^1)_NN_d
Tuning for shape 1x4096*(4x2^2)_NN_d
Tuning for shape 1x8192*(4x2^3)_NN_d
Tuning for shape 1x16384*(4x2^4)_NN_d
Tuning for shape 1x32768*(4x2^5)_NN_d
Tuning for shape 1x65536*(4x2^6)_NN_d
Tuning for shape 1x131072*(4x2^7)_NN_d
Tuning for shape 1x262144*(4x2^8)_NN_d
Tuning for shape 1x4096*(4x2^1)_NN_d
Tuning for shape 1x8192*(4x2^2)_NN_d
Tuning for shape 1x16384*(4x2^3)_NN_d
Tuning for shape 1x32768*(4x2^4)_NN_d
Tuning for shape 1x65536*(4x2^5)_NN_d
Tuning for shape 1x131072*(4x2^6)_NN_d
Tuning for shape 1x262144*(4x2^7)_NN_d
Tuning for shape 1x8192*(4x2^1)_NN_d
Tuning for shape 1x16384*(4x2^2)_NN_d
Tuning for shape 1x32768*(4x2^3)_NN_d
Tuning for shape 1x65536*(4x2^4)_NN_d
Tuning for shape 1x131072*(4x2^5)_NN_d
Tuning for shape 1x262144*(4x2^6)_NN_d
Tuning for shape 1x16384*(4x2^1)_NN_d
Tuning for shape 1x32768*(4x2^2)_NN_d
Tuning for shape 1x65536*(4x2^3)_NN_d
Tuning for shape 1x131072*(4x2^4)_NN_d
Tuning for shape 1x262144*(4x2^5)_NN_d
Tuning for shape 1x32768*(4x2^1)_NN_d
Tuning for shape 1x65536*(4x2^2)_NN_d
Tuning for shape 1x131072*(4x2^3)_NN_d
Tuning for shape 1x262144*(4x2^4)_NN_d
Tuning for shape 1x65536*(4x2^1)_NN_d
Tuning for shape 1x131072*(4x2^2)_NN_d
Tuning for shape 1x262144*(4x2^3)_NN_d
Tuning for shape 1x131072*(4x2^1)_NN_d
Tuning for shape 1x262144*(4x2^2)_NN_d
Tuning for shape 1x262144*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^9  &  1.000 & 1.000 & 1.909 & 0.524
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 9 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 262144] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x to produce Y[4, 512]
Matmul: 4 x 512 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1024*(4x2^1)_NN_d
Tuning for shape 4x2048*(4x2^2)_NN_d
Tuning for shape 4x4096*(4x2^3)_NN_d
Tuning for shape 4x8192*(4x2^4)_NN_d
Tuning for shape 4x16384*(4x2^5)_NN_d
Tuning for shape 4x32768*(4x2^6)_NN_d
Tuning for shape 4x65536*(4x2^7)_NN_d
Tuning for shape 4x131072*(4x2^8)_NN_d
Tuning for shape 4x262144*(4x2^9)_NN_d
Tuning for shape 4x2048*(4x2^1)_NN_d
Tuning for shape 4x4096*(4x2^2)_NN_d
Tuning for shape 4x8192*(4x2^3)_NN_d
Tuning for shape 4x16384*(4x2^4)_NN_d
Tuning for shape 4x32768*(4x2^5)_NN_d
Tuning for shape 4x65536*(4x2^6)_NN_d
Tuning for shape 4x131072*(4x2^7)_NN_d
Tuning for shape 4x262144*(4x2^8)_NN_d
Tuning for shape 4x4096*(4x2^1)_NN_d
Tuning for shape 4x8192*(4x2^2)_NN_d
Tuning for shape 4x16384*(4x2^3)_NN_d
Tuning for shape 4x32768*(4x2^4)_NN_d
Tuning for shape 4x65536*(4x2^5)_NN_d
Tuning for shape 4x131072*(4x2^6)_NN_d
Tuning for shape 4x262144*(4x2^7)_NN_d
Tuning for shape 4x8192*(4x2^1)_NN_d
Tuning for shape 4x16384*(4x2^2)_NN_d
Tuning for shape 4x32768*(4x2^3)_NN_d
Tuning for shape 4x65536*(4x2^4)_NN_d
Tuning for shape 4x131072*(4x2^5)_NN_d
Tuning for shape 4x262144*(4x2^6)_NN_d
Tuning for shape 4x16384*(4x2^1)_NN_d
Tuning for shape 4x32768*(4x2^2)_NN_d
Tuning for shape 4x65536*(4x2^3)_NN_d
Tuning for shape 4x131072*(4x2^4)_NN_d
Tuning for shape 4x262144*(4x2^5)_NN_d
Tuning for shape 4x32768*(4x2^1)_NN_d
Tuning for shape 4x65536*(4x2^2)_NN_d
Tuning for shape 4x131072*(4x2^3)_NN_d
Tuning for shape 4x262144*(4x2^4)_NN_d
Tuning for shape 4x65536*(4x2^1)_NN_d
Tuning for shape 4x131072*(4x2^2)_NN_d
Tuning for shape 4x262144*(4x2^3)_NN_d
Tuning for shape 4x131072*(4x2^1)_NN_d
Tuning for shape 4x262144*(4x2^2)_NN_d
Tuning for shape 4x262144*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^9  &  1.000 & 1.000 & 6.980 & 0.143
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 9 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 262144] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x to produce Y[16, 512]
Matmul: 16 x 512 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1024*(4x2^1)_NN_d
Tuning for shape 16x2048*(4x2^2)_NN_d
Tuning for shape 16x4096*(4x2^3)_NN_d
Tuning for shape 16x8192*(4x2^4)_NN_d
Tuning for shape 16x16384*(4x2^5)_NN_d
Tuning for shape 16x32768*(4x2^6)_NN_d
Tuning for shape 16x65536*(4x2^7)_NN_d
Tuning for shape 16x131072*(4x2^8)_NN_d
Tuning for shape 16x262144*(4x2^9)_NN_d
Tuning for shape 16x2048*(4x2^1)_NN_d
Tuning for shape 16x4096*(4x2^2)_NN_d
Tuning for shape 16x8192*(4x2^3)_NN_d
Tuning for shape 16x16384*(4x2^4)_NN_d
Tuning for shape 16x32768*(4x2^5)_NN_d
Tuning for shape 16x65536*(4x2^6)_NN_d
Tuning for shape 16x131072*(4x2^7)_NN_d
Tuning for shape 16x262144*(4x2^8)_NN_d
Tuning for shape 16x4096*(4x2^1)_NN_d
Tuning for shape 16x8192*(4x2^2)_NN_d
Tuning for shape 16x16384*(4x2^3)_NN_d
Tuning for shape 16x32768*(4x2^4)_NN_d
Tuning for shape 16x65536*(4x2^5)_NN_d
Tuning for shape 16x131072*(4x2^6)_NN_d
Tuning for shape 16x262144*(4x2^7)_NN_d
Tuning for shape 16x8192*(4x2^1)_NN_d
Tuning for shape 16x16384*(4x2^2)_NN_d
Tuning for shape 16x32768*(4x2^3)_NN_d
Tuning for shape 16x65536*(4x2^4)_NN_d
Tuning for shape 16x131072*(4x2^5)_NN_d
Tuning for shape 16x262144*(4x2^6)_NN_d
Tuning for shape 16x16384*(4x2^1)_NN_d
Tuning for shape 16x32768*(4x2^2)_NN_d
Tuning for shape 16x65536*(4x2^3)_NN_d
Tuning for shape 16x131072*(4x2^4)_NN_d
Tuning for shape 16x262144*(4x2^5)_NN_d
Tuning for shape 16x32768*(4x2^1)_NN_d
Tuning for shape 16x65536*(4x2^2)_NN_d
Tuning for shape 16x131072*(4x2^3)_NN_d
Tuning for shape 16x262144*(4x2^4)_NN_d
Tuning for shape 16x65536*(4x2^1)_NN_d
Tuning for shape 16x131072*(4x2^2)_NN_d
Tuning for shape 16x262144*(4x2^3)_NN_d
Tuning for shape 16x131072*(4x2^1)_NN_d
Tuning for shape 16x262144*(4x2^2)_NN_d
Tuning for shape 16x262144*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^9  &  1.000 & 1.000 & 28.951 & 0.035
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 9 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 262144] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x to produce Y[64, 512]
Matmul: 64 x 512 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1024*(4x2^1)_NN_d
Tuning for shape 64x2048*(4x2^2)_NN_d
Tuning for shape 64x4096*(4x2^3)_NN_d
Tuning for shape 64x8192*(4x2^4)_NN_d
Tuning for shape 64x16384*(4x2^5)_NN_d
Tuning for shape 64x32768*(4x2^6)_NN_d
Tuning for shape 64x65536*(4x2^7)_NN_d
Tuning for shape 64x131072*(4x2^8)_NN_d
Tuning for shape 64x262144*(4x2^9)_NN_d
Tuning for shape 64x2048*(4x2^1)_NN_d
Tuning for shape 64x4096*(4x2^2)_NN_d
Tuning for shape 64x8192*(4x2^3)_NN_d
Tuning for shape 64x16384*(4x2^4)_NN_d
Tuning for shape 64x32768*(4x2^5)_NN_d
Tuning for shape 64x65536*(4x2^6)_NN_d
Tuning for shape 64x131072*(4x2^7)_NN_d
Tuning for shape 64x262144*(4x2^8)_NN_d
Tuning for shape 64x4096*(4x2^1)_NN_d
Tuning for shape 64x8192*(4x2^2)_NN_d
Tuning for shape 64x16384*(4x2^3)_NN_d
Tuning for shape 64x32768*(4x2^4)_NN_d
Tuning for shape 64x65536*(4x2^5)_NN_d
Tuning for shape 64x131072*(4x2^6)_NN_d
Tuning for shape 64x262144*(4x2^7)_NN_d
Tuning for shape 64x8192*(4x2^1)_NN_d
Tuning for shape 64x16384*(4x2^2)_NN_d
Tuning for shape 64x32768*(4x2^3)_NN_d
Tuning for shape 64x65536*(4x2^4)_NN_d
Tuning for shape 64x131072*(4x2^5)_NN_d
Tuning for shape 64x262144*(4x2^6)_NN_d
Tuning for shape 64x16384*(4x2^1)_NN_d
Tuning for shape 64x32768*(4x2^2)_NN_d
Tuning for shape 64x65536*(4x2^3)_NN_d
Tuning for shape 64x131072*(4x2^4)_NN_d
Tuning for shape 64x262144*(4x2^5)_NN_d
Tuning for shape 64x32768*(4x2^1)_NN_d
Tuning for shape 64x65536*(4x2^2)_NN_d
Tuning for shape 64x131072*(4x2^3)_NN_d
Tuning for shape 64x262144*(4x2^4)_NN_d
Tuning for shape 64x65536*(4x2^1)_NN_d
Tuning for shape 64x131072*(4x2^2)_NN_d
Tuning for shape 64x262144*(4x2^3)_NN_d
Tuning for shape 64x131072*(4x2^1)_NN_d
Tuning for shape 64x262144*(4x2^2)_NN_d
Tuning for shape 64x262144*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^9  &  1.000 & 1.000 & 33.910 & 0.029
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 9 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 262144] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x to produce Y[256, 512]
Matmul: 256 x 512 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1024*(4x2^1)_NN_d
Tuning for shape 256x2048*(4x2^2)_NN_d
Tuning for shape 256x4096*(4x2^3)_NN_d
Tuning for shape 256x8192*(4x2^4)_NN_d
Tuning for shape 256x16384*(4x2^5)_NN_d
Tuning for shape 256x32768*(4x2^6)_NN_d
Tuning for shape 256x65536*(4x2^7)_NN_d
Tuning for shape 256x131072*(4x2^8)_NN_d
Tuning for shape 256x262144*(4x2^9)_NN_d
Tuning for shape 256x2048*(4x2^1)_NN_d
Tuning for shape 256x4096*(4x2^2)_NN_d
Tuning for shape 256x8192*(4x2^3)_NN_d
Tuning for shape 256x16384*(4x2^4)_NN_d
Tuning for shape 256x32768*(4x2^5)_NN_d
Tuning for shape 256x65536*(4x2^6)_NN_d
Tuning for shape 256x131072*(4x2^7)_NN_d
Tuning for shape 256x262144*(4x2^8)_NN_d
Tuning for shape 256x4096*(4x2^1)_NN_d
Tuning for shape 256x8192*(4x2^2)_NN_d
Tuning for shape 256x16384*(4x2^3)_NN_d
Tuning for shape 256x32768*(4x2^4)_NN_d
Tuning for shape 256x65536*(4x2^5)_NN_d
Tuning for shape 256x131072*(4x2^6)_NN_d
Tuning for shape 256x262144*(4x2^7)_NN_d
Tuning for shape 256x8192*(4x2^1)_NN_d
Tuning for shape 256x16384*(4x2^2)_NN_d
Tuning for shape 256x32768*(4x2^3)_NN_d
Tuning for shape 256x65536*(4x2^4)_NN_d
Tuning for shape 256x131072*(4x2^5)_NN_d
Tuning for shape 256x262144*(4x2^6)_NN_d
Tuning for shape 256x16384*(4x2^1)_NN_d
Tuning for shape 256x32768*(4x2^2)_NN_d
Tuning for shape 256x65536*(4x2^3)_NN_d
Tuning for shape 256x131072*(4x2^4)_NN_d
Tuning for shape 256x262144*(4x2^5)_NN_d
Tuning for shape 256x32768*(4x2^1)_NN_d
Tuning for shape 256x65536*(4x2^2)_NN_d
Tuning for shape 256x131072*(4x2^3)_NN_d
Tuning for shape 256x262144*(4x2^4)_NN_d
Tuning for shape 256x65536*(4x2^1)_NN_d
Tuning for shape 256x131072*(4x2^2)_NN_d
Tuning for shape 256x262144*(4x2^3)_NN_d
Tuning for shape 256x131072*(4x2^1)_NN_d
Tuning for shape 256x262144*(4x2^2)_NN_d
Tuning for shape 256x262144*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^9  &  1.000 & 1.000 & 25.648 & 0.039
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 9 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 262144] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x to produce Y[1024, 512]
Matmul: 1024 x 512 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x1024*(4x2^1)_NN_d
Tuning for shape 1024x2048*(4x2^2)_NN_d
Tuning for shape 1024x4096*(4x2^3)_NN_d
Tuning for shape 1024x8192*(4x2^4)_NN_d
Tuning for shape 1024x16384*(4x2^5)_NN_d
Tuning for shape 1024x32768*(4x2^6)_NN_d
Tuning for shape 1024x65536*(4x2^7)_NN_d
Tuning for shape 1024x131072*(4x2^8)_NN_d
Tuning for shape 1024x262144*(4x2^9)_NN_d
Tuning for shape 1024x2048*(4x2^1)_NN_d
Tuning for shape 1024x4096*(4x2^2)_NN_d
Tuning for shape 1024x8192*(4x2^3)_NN_d
Tuning for shape 1024x16384*(4x2^4)_NN_d
Tuning for shape 1024x32768*(4x2^5)_NN_d
Tuning for shape 1024x65536*(4x2^6)_NN_d
Tuning for shape 1024x131072*(4x2^7)_NN_d
Tuning for shape 1024x262144*(4x2^8)_NN_d
Tuning for shape 1024x4096*(4x2^1)_NN_d
Tuning for shape 1024x8192*(4x2^2)_NN_d
Tuning for shape 1024x16384*(4x2^3)_NN_d
Tuning for shape 1024x32768*(4x2^4)_NN_d
Tuning for shape 1024x65536*(4x2^5)_NN_d
Tuning for shape 1024x131072*(4x2^6)_NN_d
Tuning for shape 1024x262144*(4x2^7)_NN_d
Tuning for shape 1024x8192*(4x2^1)_NN_d
Tuning for shape 1024x16384*(4x2^2)_NN_d
Tuning for shape 1024x32768*(4x2^3)_NN_d
Tuning for shape 1024x65536*(4x2^4)_NN_d
Tuning for shape 1024x131072*(4x2^5)_NN_d
Tuning for shape 1024x262144*(4x2^6)_NN_d
Tuning for shape 1024x16384*(4x2^1)_NN_d
Tuning for shape 1024x32768*(4x2^2)_NN_d
Tuning for shape 1024x65536*(4x2^3)_NN_d
Tuning for shape 1024x131072*(4x2^4)_NN_d
Tuning for shape 1024x262144*(4x2^5)_NN_d
Tuning for shape 1024x32768*(4x2^1)_NN_d
Tuning for shape 1024x65536*(4x2^2)_NN_d
Tuning for shape 1024x131072*(4x2^3)_NN_d
Tuning for shape 1024x262144*(4x2^4)_NN_d
Tuning for shape 1024x65536*(4x2^1)_NN_d
Tuning for shape 1024x131072*(4x2^2)_NN_d
Tuning for shape 1024x262144*(4x2^3)_NN_d
Tuning for shape 1024x131072*(4x2^1)_NN_d
Tuning for shape 1024x262144*(4x2^2)_NN_d
Tuning for shape 1024x262144*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x2^9  &  1.000 & 1.000 & 25.249 & 0.040
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 10 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1048576] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x to produce Y[1, 1024]
Matmul: 1 x 1024 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2048*(4x2^1)_NN_d
Tuning for shape 1x4096*(4x2^2)_NN_d
Tuning for shape 1x8192*(4x2^3)_NN_d
Tuning for shape 1x16384*(4x2^4)_NN_d
Tuning for shape 1x32768*(4x2^5)_NN_d
Tuning for shape 1x65536*(4x2^6)_NN_d
Tuning for shape 1x131072*(4x2^7)_NN_d
Tuning for shape 1x262144*(4x2^8)_NN_d
Tuning for shape 1x524288*(4x2^9)_NN_d
Tuning for shape 1x1048576*(4x2^10)_NN_d
Tuning for shape 1x4096*(4x2^1)_NN_d
Tuning for shape 1x8192*(4x2^2)_NN_d
Tuning for shape 1x16384*(4x2^3)_NN_d
Tuning for shape 1x32768*(4x2^4)_NN_d
Tuning for shape 1x65536*(4x2^5)_NN_d
Tuning for shape 1x131072*(4x2^6)_NN_d
Tuning for shape 1x262144*(4x2^7)_NN_d
Tuning for shape 1x524288*(4x2^8)_NN_d
Tuning for shape 1x1048576*(4x2^9)_NN_d
Tuning for shape 1x8192*(4x2^1)_NN_d
Tuning for shape 1x16384*(4x2^2)_NN_d
Tuning for shape 1x32768*(4x2^3)_NN_d
Tuning for shape 1x65536*(4x2^4)_NN_d
Tuning for shape 1x131072*(4x2^5)_NN_d
Tuning for shape 1x262144*(4x2^6)_NN_d
Tuning for shape 1x524288*(4x2^7)_NN_d
Tuning for shape 1x1048576*(4x2^8)_NN_d
Tuning for shape 1x16384*(4x2^1)_NN_d
Tuning for shape 1x32768*(4x2^2)_NN_d
Tuning for shape 1x65536*(4x2^3)_NN_d
Tuning for shape 1x131072*(4x2^4)_NN_d
Tuning for shape 1x262144*(4x2^5)_NN_d
Tuning for shape 1x524288*(4x2^6)_NN_d
Tuning for shape 1x1048576*(4x2^7)_NN_d
Tuning for shape 1x32768*(4x2^1)_NN_d
Tuning for shape 1x65536*(4x2^2)_NN_d
Tuning for shape 1x131072*(4x2^3)_NN_d
Tuning for shape 1x262144*(4x2^4)_NN_d
Tuning for shape 1x524288*(4x2^5)_NN_d
Tuning for shape 1x1048576*(4x2^6)_NN_d
Tuning for shape 1x65536*(4x2^1)_NN_d
Tuning for shape 1x131072*(4x2^2)_NN_d
Tuning for shape 1x262144*(4x2^3)_NN_d
Tuning for shape 1x524288*(4x2^4)_NN_d
Tuning for shape 1x1048576*(4x2^5)_NN_d
Tuning for shape 1x131072*(4x2^1)_NN_d
Tuning for shape 1x262144*(4x2^2)_NN_d
Tuning for shape 1x524288*(4x2^3)_NN_d
Tuning for shape 1x1048576*(4x2^4)_NN_d
Tuning for shape 1x262144*(4x2^1)_NN_d
Tuning for shape 1x524288*(4x2^2)_NN_d
Tuning for shape 1x1048576*(4x2^3)_NN_d
Tuning for shape 1x524288*(4x2^1)_NN_d
Tuning for shape 1x1048576*(4x2^2)_NN_d
Tuning for shape 1x1048576*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^10  &  1.000 & 1.000 & 6.834 & 0.146
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 10 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1048576] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x to produce Y[4, 1024]
Matmul: 4 x 1024 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2048*(4x2^1)_NN_d
Tuning for shape 4x4096*(4x2^2)_NN_d
Tuning for shape 4x8192*(4x2^3)_NN_d
Tuning for shape 4x16384*(4x2^4)_NN_d
Tuning for shape 4x32768*(4x2^5)_NN_d
Tuning for shape 4x65536*(4x2^6)_NN_d
Tuning for shape 4x131072*(4x2^7)_NN_d
Tuning for shape 4x262144*(4x2^8)_NN_d
Tuning for shape 4x524288*(4x2^9)_NN_d
Tuning for shape 4x1048576*(4x2^10)_NN_d
Tuning for shape 4x4096*(4x2^1)_NN_d
Tuning for shape 4x8192*(4x2^2)_NN_d
Tuning for shape 4x16384*(4x2^3)_NN_d
Tuning for shape 4x32768*(4x2^4)_NN_d
Tuning for shape 4x65536*(4x2^5)_NN_d
Tuning for shape 4x131072*(4x2^6)_NN_d
Tuning for shape 4x262144*(4x2^7)_NN_d
Tuning for shape 4x524288*(4x2^8)_NN_d
Tuning for shape 4x1048576*(4x2^9)_NN_d
Tuning for shape 4x8192*(4x2^1)_NN_d
Tuning for shape 4x16384*(4x2^2)_NN_d
Tuning for shape 4x32768*(4x2^3)_NN_d
Tuning for shape 4x65536*(4x2^4)_NN_d
Tuning for shape 4x131072*(4x2^5)_NN_d
Tuning for shape 4x262144*(4x2^6)_NN_d
Tuning for shape 4x524288*(4x2^7)_NN_d
Tuning for shape 4x1048576*(4x2^8)_NN_d
Tuning for shape 4x16384*(4x2^1)_NN_d
Tuning for shape 4x32768*(4x2^2)_NN_d
Tuning for shape 4x65536*(4x2^3)_NN_d
Tuning for shape 4x131072*(4x2^4)_NN_d
Tuning for shape 4x262144*(4x2^5)_NN_d
Tuning for shape 4x524288*(4x2^6)_NN_d
Tuning for shape 4x1048576*(4x2^7)_NN_d
Tuning for shape 4x32768*(4x2^1)_NN_d
Tuning for shape 4x65536*(4x2^2)_NN_d
Tuning for shape 4x131072*(4x2^3)_NN_d
Tuning for shape 4x262144*(4x2^4)_NN_d
Tuning for shape 4x524288*(4x2^5)_NN_d
Tuning for shape 4x1048576*(4x2^6)_NN_d
Tuning for shape 4x65536*(4x2^1)_NN_d
Tuning for shape 4x131072*(4x2^2)_NN_d
Tuning for shape 4x262144*(4x2^3)_NN_d
Tuning for shape 4x524288*(4x2^4)_NN_d
Tuning for shape 4x1048576*(4x2^5)_NN_d
Tuning for shape 4x131072*(4x2^1)_NN_d
Tuning for shape 4x262144*(4x2^2)_NN_d
Tuning for shape 4x524288*(4x2^3)_NN_d
Tuning for shape 4x1048576*(4x2^4)_NN_d
Tuning for shape 4x262144*(4x2^1)_NN_d
Tuning for shape 4x524288*(4x2^2)_NN_d
Tuning for shape 4x1048576*(4x2^3)_NN_d
Tuning for shape 4x524288*(4x2^1)_NN_d
Tuning for shape 4x1048576*(4x2^2)_NN_d
Tuning for shape 4x1048576*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^10  &  1.000 & 1.000 & 27.056 & 0.037
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 10 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1048576] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x to produce Y[16, 1024]
Matmul: 16 x 1024 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2048*(4x2^1)_NN_d
Tuning for shape 16x4096*(4x2^2)_NN_d
Tuning for shape 16x8192*(4x2^3)_NN_d
Tuning for shape 16x16384*(4x2^4)_NN_d
Tuning for shape 16x32768*(4x2^5)_NN_d
Tuning for shape 16x65536*(4x2^6)_NN_d
Tuning for shape 16x131072*(4x2^7)_NN_d
Tuning for shape 16x262144*(4x2^8)_NN_d
Tuning for shape 16x524288*(4x2^9)_NN_d
Tuning for shape 16x1048576*(4x2^10)_NN_d
Tuning for shape 16x4096*(4x2^1)_NN_d
Tuning for shape 16x8192*(4x2^2)_NN_d
Tuning for shape 16x16384*(4x2^3)_NN_d
Tuning for shape 16x32768*(4x2^4)_NN_d
Tuning for shape 16x65536*(4x2^5)_NN_d
Tuning for shape 16x131072*(4x2^6)_NN_d
Tuning for shape 16x262144*(4x2^7)_NN_d
Tuning for shape 16x524288*(4x2^8)_NN_d
Tuning for shape 16x1048576*(4x2^9)_NN_d
Tuning for shape 16x8192*(4x2^1)_NN_d
Tuning for shape 16x16384*(4x2^2)_NN_d
Tuning for shape 16x32768*(4x2^3)_NN_d
Tuning for shape 16x65536*(4x2^4)_NN_d
Tuning for shape 16x131072*(4x2^5)_NN_d
Tuning for shape 16x262144*(4x2^6)_NN_d
Tuning for shape 16x524288*(4x2^7)_NN_d
Tuning for shape 16x1048576*(4x2^8)_NN_d
Tuning for shape 16x16384*(4x2^1)_NN_d
Tuning for shape 16x32768*(4x2^2)_NN_d
Tuning for shape 16x65536*(4x2^3)_NN_d
Tuning for shape 16x131072*(4x2^4)_NN_d
Tuning for shape 16x262144*(4x2^5)_NN_d
Tuning for shape 16x524288*(4x2^6)_NN_d
Tuning for shape 16x1048576*(4x2^7)_NN_d
Tuning for shape 16x32768*(4x2^1)_NN_d
Tuning for shape 16x65536*(4x2^2)_NN_d
Tuning for shape 16x131072*(4x2^3)_NN_d
Tuning for shape 16x262144*(4x2^4)_NN_d
Tuning for shape 16x524288*(4x2^5)_NN_d
Tuning for shape 16x1048576*(4x2^6)_NN_d
Tuning for shape 16x65536*(4x2^1)_NN_d
Tuning for shape 16x131072*(4x2^2)_NN_d
Tuning for shape 16x262144*(4x2^3)_NN_d
Tuning for shape 16x524288*(4x2^4)_NN_d
Tuning for shape 16x1048576*(4x2^5)_NN_d
Tuning for shape 16x131072*(4x2^1)_NN_d
Tuning for shape 16x262144*(4x2^2)_NN_d
Tuning for shape 16x524288*(4x2^3)_NN_d
Tuning for shape 16x1048576*(4x2^4)_NN_d
Tuning for shape 16x262144*(4x2^1)_NN_d
Tuning for shape 16x524288*(4x2^2)_NN_d
Tuning for shape 16x1048576*(4x2^3)_NN_d
Tuning for shape 16x524288*(4x2^1)_NN_d
Tuning for shape 16x1048576*(4x2^2)_NN_d
Tuning for shape 16x1048576*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^10  &  1.000 & 1.000 & 35.765 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 10 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1048576] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x to produce Y[64, 1024]
Matmul: 64 x 1024 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2048*(4x2^1)_NN_d
Tuning for shape 64x4096*(4x2^2)_NN_d
Tuning for shape 64x8192*(4x2^3)_NN_d
Tuning for shape 64x16384*(4x2^4)_NN_d
Tuning for shape 64x32768*(4x2^5)_NN_d
Tuning for shape 64x65536*(4x2^6)_NN_d
Tuning for shape 64x131072*(4x2^7)_NN_d
Tuning for shape 64x262144*(4x2^8)_NN_d
Tuning for shape 64x524288*(4x2^9)_NN_d
Tuning for shape 64x1048576*(4x2^10)_NN_d
Tuning for shape 64x4096*(4x2^1)_NN_d
Tuning for shape 64x8192*(4x2^2)_NN_d
Tuning for shape 64x16384*(4x2^3)_NN_d
Tuning for shape 64x32768*(4x2^4)_NN_d
Tuning for shape 64x65536*(4x2^5)_NN_d
Tuning for shape 64x131072*(4x2^6)_NN_d
Tuning for shape 64x262144*(4x2^7)_NN_d
Tuning for shape 64x524288*(4x2^8)_NN_d
Tuning for shape 64x1048576*(4x2^9)_NN_d
Tuning for shape 64x8192*(4x2^1)_NN_d
Tuning for shape 64x16384*(4x2^2)_NN_d
Tuning for shape 64x32768*(4x2^3)_NN_d
Tuning for shape 64x65536*(4x2^4)_NN_d
Tuning for shape 64x131072*(4x2^5)_NN_d
Tuning for shape 64x262144*(4x2^6)_NN_d
Tuning for shape 64x524288*(4x2^7)_NN_d
Tuning for shape 64x1048576*(4x2^8)_NN_d
Tuning for shape 64x16384*(4x2^1)_NN_d
Tuning for shape 64x32768*(4x2^2)_NN_d
Tuning for shape 64x65536*(4x2^3)_NN_d
Tuning for shape 64x131072*(4x2^4)_NN_d
Tuning for shape 64x262144*(4x2^5)_NN_d
Tuning for shape 64x524288*(4x2^6)_NN_d
Tuning for shape 64x1048576*(4x2^7)_NN_d
Tuning for shape 64x32768*(4x2^1)_NN_d
Tuning for shape 64x65536*(4x2^2)_NN_d
Tuning for shape 64x131072*(4x2^3)_NN_d
Tuning for shape 64x262144*(4x2^4)_NN_d
Tuning for shape 64x524288*(4x2^5)_NN_d
Tuning for shape 64x1048576*(4x2^6)_NN_d
Tuning for shape 64x65536*(4x2^1)_NN_d
Tuning for shape 64x131072*(4x2^2)_NN_d
Tuning for shape 64x262144*(4x2^3)_NN_d
Tuning for shape 64x524288*(4x2^4)_NN_d
Tuning for shape 64x1048576*(4x2^5)_NN_d
Tuning for shape 64x131072*(4x2^1)_NN_d
Tuning for shape 64x262144*(4x2^2)_NN_d
Tuning for shape 64x524288*(4x2^3)_NN_d
Tuning for shape 64x1048576*(4x2^4)_NN_d
Tuning for shape 64x262144*(4x2^1)_NN_d
Tuning for shape 64x524288*(4x2^2)_NN_d
Tuning for shape 64x1048576*(4x2^3)_NN_d
Tuning for shape 64x524288*(4x2^1)_NN_d
Tuning for shape 64x1048576*(4x2^2)_NN_d
Tuning for shape 64x1048576*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^10  &  1.000 & 1.000 & 34.501 & 0.029
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 10 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1048576] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x to produce Y[256, 1024]
Matmul: 256 x 1024 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2048*(4x2^1)_NN_d
Tuning for shape 256x4096*(4x2^2)_NN_d
Tuning for shape 256x8192*(4x2^3)_NN_d
Tuning for shape 256x16384*(4x2^4)_NN_d
Tuning for shape 256x32768*(4x2^5)_NN_d
Tuning for shape 256x65536*(4x2^6)_NN_d
Tuning for shape 256x131072*(4x2^7)_NN_d
Tuning for shape 256x262144*(4x2^8)_NN_d
Tuning for shape 256x524288*(4x2^9)_NN_d
Tuning for shape 256x1048576*(4x2^10)_NN_d
Tuning for shape 256x4096*(4x2^1)_NN_d
Tuning for shape 256x8192*(4x2^2)_NN_d
Tuning for shape 256x16384*(4x2^3)_NN_d
Tuning for shape 256x32768*(4x2^4)_NN_d
Tuning for shape 256x65536*(4x2^5)_NN_d
Tuning for shape 256x131072*(4x2^6)_NN_d
Tuning for shape 256x262144*(4x2^7)_NN_d
Tuning for shape 256x524288*(4x2^8)_NN_d
Tuning for shape 256x1048576*(4x2^9)_NN_d
Tuning for shape 256x8192*(4x2^1)_NN_d
Tuning for shape 256x16384*(4x2^2)_NN_d
Tuning for shape 256x32768*(4x2^3)_NN_d
Tuning for shape 256x65536*(4x2^4)_NN_d
Tuning for shape 256x131072*(4x2^5)_NN_d
Tuning for shape 256x262144*(4x2^6)_NN_d
Tuning for shape 256x524288*(4x2^7)_NN_d
Tuning for shape 256x1048576*(4x2^8)_NN_d
Tuning for shape 256x16384*(4x2^1)_NN_d
Tuning for shape 256x32768*(4x2^2)_NN_d
Tuning for shape 256x65536*(4x2^3)_NN_d
Tuning for shape 256x131072*(4x2^4)_NN_d
Tuning for shape 256x262144*(4x2^5)_NN_d
Tuning for shape 256x524288*(4x2^6)_NN_d
Tuning for shape 256x1048576*(4x2^7)_NN_d
Tuning for shape 256x32768*(4x2^1)_NN_d
Tuning for shape 256x65536*(4x2^2)_NN_d
Tuning for shape 256x131072*(4x2^3)_NN_d
Tuning for shape 256x262144*(4x2^4)_NN_d
Tuning for shape 256x524288*(4x2^5)_NN_d
Tuning for shape 256x1048576*(4x2^6)_NN_d
Tuning for shape 256x65536*(4x2^1)_NN_d
Tuning for shape 256x131072*(4x2^2)_NN_d
Tuning for shape 256x262144*(4x2^3)_NN_d
Tuning for shape 256x524288*(4x2^4)_NN_d
Tuning for shape 256x1048576*(4x2^5)_NN_d
Tuning for shape 256x131072*(4x2^1)_NN_d
Tuning for shape 256x262144*(4x2^2)_NN_d
Tuning for shape 256x524288*(4x2^3)_NN_d
Tuning for shape 256x1048576*(4x2^4)_NN_d
Tuning for shape 256x262144*(4x2^1)_NN_d
Tuning for shape 256x524288*(4x2^2)_NN_d
Tuning for shape 256x1048576*(4x2^3)_NN_d
Tuning for shape 256x524288*(4x2^1)_NN_d
Tuning for shape 256x1048576*(4x2^2)_NN_d
Tuning for shape 256x1048576*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x2^10  &  1.000 & 1.000 & 25.712 & 0.039
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 11 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4194304] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x to produce Y[1, 2048]
Matmul: 1 x 2048 x 4194304, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4096*(4x2^1)_NN_d
Tuning for shape 1x8192*(4x2^2)_NN_d
Tuning for shape 1x16384*(4x2^3)_NN_d
Tuning for shape 1x32768*(4x2^4)_NN_d
Tuning for shape 1x65536*(4x2^5)_NN_d
Tuning for shape 1x131072*(4x2^6)_NN_d
Tuning for shape 1x262144*(4x2^7)_NN_d
Tuning for shape 1x524288*(4x2^8)_NN_d
Tuning for shape 1x1048576*(4x2^9)_NN_d
Tuning for shape 1x2097152*(4x2^10)_NN_d
Tuning for shape 1x4194304*(4x2^11)_NN_d
Tuning for shape 1x8192*(4x2^1)_NN_d
Tuning for shape 1x16384*(4x2^2)_NN_d
Tuning for shape 1x32768*(4x2^3)_NN_d
Tuning for shape 1x65536*(4x2^4)_NN_d
Tuning for shape 1x131072*(4x2^5)_NN_d
Tuning for shape 1x262144*(4x2^6)_NN_d
Tuning for shape 1x524288*(4x2^7)_NN_d
Tuning for shape 1x1048576*(4x2^8)_NN_d
Tuning for shape 1x2097152*(4x2^9)_NN_d
Tuning for shape 1x4194304*(4x2^10)_NN_d
Tuning for shape 1x16384*(4x2^1)_NN_d
Tuning for shape 1x32768*(4x2^2)_NN_d
Tuning for shape 1x65536*(4x2^3)_NN_d
Tuning for shape 1x131072*(4x2^4)_NN_d
Tuning for shape 1x262144*(4x2^5)_NN_d
Tuning for shape 1x524288*(4x2^6)_NN_d
Tuning for shape 1x1048576*(4x2^7)_NN_d
Tuning for shape 1x2097152*(4x2^8)_NN_d
Tuning for shape 1x4194304*(4x2^9)_NN_d
Tuning for shape 1x32768*(4x2^1)_NN_d
Tuning for shape 1x65536*(4x2^2)_NN_d
Tuning for shape 1x131072*(4x2^3)_NN_d
Tuning for shape 1x262144*(4x2^4)_NN_d
Tuning for shape 1x524288*(4x2^5)_NN_d
Tuning for shape 1x1048576*(4x2^6)_NN_d
Tuning for shape 1x2097152*(4x2^7)_NN_d
Tuning for shape 1x4194304*(4x2^8)_NN_d
Tuning for shape 1x65536*(4x2^1)_NN_d
Tuning for shape 1x131072*(4x2^2)_NN_d
Tuning for shape 1x262144*(4x2^3)_NN_d
Tuning for shape 1x524288*(4x2^4)_NN_d
Tuning for shape 1x1048576*(4x2^5)_NN_d
Tuning for shape 1x2097152*(4x2^6)_NN_d
Tuning for shape 1x4194304*(4x2^7)_NN_d
Tuning for shape 1x131072*(4x2^1)_NN_d
Tuning for shape 1x262144*(4x2^2)_NN_d
Tuning for shape 1x524288*(4x2^3)_NN_d
Tuning for shape 1x1048576*(4x2^4)_NN_d
Tuning for shape 1x2097152*(4x2^5)_NN_d
Tuning for shape 1x4194304*(4x2^6)_NN_d
Tuning for shape 1x262144*(4x2^1)_NN_d
Tuning for shape 1x524288*(4x2^2)_NN_d
Tuning for shape 1x1048576*(4x2^3)_NN_d
Tuning for shape 1x2097152*(4x2^4)_NN_d
Tuning for shape 1x4194304*(4x2^5)_NN_d
Tuning for shape 1x524288*(4x2^1)_NN_d
Tuning for shape 1x1048576*(4x2^2)_NN_d
Tuning for shape 1x2097152*(4x2^3)_NN_d
Tuning for shape 1x4194304*(4x2^4)_NN_d
Tuning for shape 1x1048576*(4x2^1)_NN_d
Tuning for shape 1x2097152*(4x2^2)_NN_d
Tuning for shape 1x4194304*(4x2^3)_NN_d
Tuning for shape 1x2097152*(4x2^1)_NN_d
Tuning for shape 1x4194304*(4x2^2)_NN_d
Tuning for shape 1x4194304*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^11  &  1.000 & 1.000 & 25.687 & 0.039
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 11 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4194304] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x to produce Y[4, 2048]
Matmul: 4 x 2048 x 4194304, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4096*(4x2^1)_NN_d
Tuning for shape 4x8192*(4x2^2)_NN_d
Tuning for shape 4x16384*(4x2^3)_NN_d
Tuning for shape 4x32768*(4x2^4)_NN_d
Tuning for shape 4x65536*(4x2^5)_NN_d
Tuning for shape 4x131072*(4x2^6)_NN_d
Tuning for shape 4x262144*(4x2^7)_NN_d
Tuning for shape 4x524288*(4x2^8)_NN_d
Tuning for shape 4x1048576*(4x2^9)_NN_d
Tuning for shape 4x2097152*(4x2^10)_NN_d
Tuning for shape 4x4194304*(4x2^11)_NN_d
Tuning for shape 4x8192*(4x2^1)_NN_d
Tuning for shape 4x16384*(4x2^2)_NN_d
Tuning for shape 4x32768*(4x2^3)_NN_d
Tuning for shape 4x65536*(4x2^4)_NN_d
Tuning for shape 4x131072*(4x2^5)_NN_d
Tuning for shape 4x262144*(4x2^6)_NN_d
Tuning for shape 4x524288*(4x2^7)_NN_d
Tuning for shape 4x1048576*(4x2^8)_NN_d
Tuning for shape 4x2097152*(4x2^9)_NN_d
Tuning for shape 4x4194304*(4x2^10)_NN_d
Tuning for shape 4x16384*(4x2^1)_NN_d
Tuning for shape 4x32768*(4x2^2)_NN_d
Tuning for shape 4x65536*(4x2^3)_NN_d
Tuning for shape 4x131072*(4x2^4)_NN_d
Tuning for shape 4x262144*(4x2^5)_NN_d
Tuning for shape 4x524288*(4x2^6)_NN_d
Tuning for shape 4x1048576*(4x2^7)_NN_d
Tuning for shape 4x2097152*(4x2^8)_NN_d
Tuning for shape 4x4194304*(4x2^9)_NN_d
Tuning for shape 4x32768*(4x2^1)_NN_d
Tuning for shape 4x65536*(4x2^2)_NN_d
Tuning for shape 4x131072*(4x2^3)_NN_d
Tuning for shape 4x262144*(4x2^4)_NN_d
Tuning for shape 4x524288*(4x2^5)_NN_d
Tuning for shape 4x1048576*(4x2^6)_NN_d
Tuning for shape 4x2097152*(4x2^7)_NN_d
Tuning for shape 4x4194304*(4x2^8)_NN_d
Tuning for shape 4x65536*(4x2^1)_NN_d
Tuning for shape 4x131072*(4x2^2)_NN_d
Tuning for shape 4x262144*(4x2^3)_NN_d
Tuning for shape 4x524288*(4x2^4)_NN_d
Tuning for shape 4x1048576*(4x2^5)_NN_d
Tuning for shape 4x2097152*(4x2^6)_NN_d
Tuning for shape 4x4194304*(4x2^7)_NN_d
Tuning for shape 4x131072*(4x2^1)_NN_d
Tuning for shape 4x262144*(4x2^2)_NN_d
Tuning for shape 4x524288*(4x2^3)_NN_d
Tuning for shape 4x1048576*(4x2^4)_NN_d
Tuning for shape 4x2097152*(4x2^5)_NN_d
Tuning for shape 4x4194304*(4x2^6)_NN_d
Tuning for shape 4x262144*(4x2^1)_NN_d
Tuning for shape 4x524288*(4x2^2)_NN_d
Tuning for shape 4x1048576*(4x2^3)_NN_d
Tuning for shape 4x2097152*(4x2^4)_NN_d
Tuning for shape 4x4194304*(4x2^5)_NN_d
Tuning for shape 4x524288*(4x2^1)_NN_d
Tuning for shape 4x1048576*(4x2^2)_NN_d
Tuning for shape 4x2097152*(4x2^3)_NN_d
Tuning for shape 4x4194304*(4x2^4)_NN_d
Tuning for shape 4x1048576*(4x2^1)_NN_d
Tuning for shape 4x2097152*(4x2^2)_NN_d
Tuning for shape 4x4194304*(4x2^3)_NN_d
Tuning for shape 4x2097152*(4x2^1)_NN_d
Tuning for shape 4x4194304*(4x2^2)_NN_d
Tuning for shape 4x4194304*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^11  &  1.000 & 1.000 & 35.677 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 11 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4194304] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x to produce Y[16, 2048]
Matmul: 16 x 2048 x 4194304, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4096*(4x2^1)_NN_d
Tuning for shape 16x8192*(4x2^2)_NN_d
Tuning for shape 16x16384*(4x2^3)_NN_d
Tuning for shape 16x32768*(4x2^4)_NN_d
Tuning for shape 16x65536*(4x2^5)_NN_d
Tuning for shape 16x131072*(4x2^6)_NN_d
Tuning for shape 16x262144*(4x2^7)_NN_d
Tuning for shape 16x524288*(4x2^8)_NN_d
Tuning for shape 16x1048576*(4x2^9)_NN_d
Tuning for shape 16x2097152*(4x2^10)_NN_d
Tuning for shape 16x4194304*(4x2^11)_NN_d
Tuning for shape 16x8192*(4x2^1)_NN_d
Tuning for shape 16x16384*(4x2^2)_NN_d
Tuning for shape 16x32768*(4x2^3)_NN_d
Tuning for shape 16x65536*(4x2^4)_NN_d
Tuning for shape 16x131072*(4x2^5)_NN_d
Tuning for shape 16x262144*(4x2^6)_NN_d
Tuning for shape 16x524288*(4x2^7)_NN_d
Tuning for shape 16x1048576*(4x2^8)_NN_d
Tuning for shape 16x2097152*(4x2^9)_NN_d
Tuning for shape 16x4194304*(4x2^10)_NN_d
Tuning for shape 16x16384*(4x2^1)_NN_d
Tuning for shape 16x32768*(4x2^2)_NN_d
Tuning for shape 16x65536*(4x2^3)_NN_d
Tuning for shape 16x131072*(4x2^4)_NN_d
Tuning for shape 16x262144*(4x2^5)_NN_d
Tuning for shape 16x524288*(4x2^6)_NN_d
Tuning for shape 16x1048576*(4x2^7)_NN_d
Tuning for shape 16x2097152*(4x2^8)_NN_d
Tuning for shape 16x4194304*(4x2^9)_NN_d
Tuning for shape 16x32768*(4x2^1)_NN_d
Tuning for shape 16x65536*(4x2^2)_NN_d
Tuning for shape 16x131072*(4x2^3)_NN_d
Tuning for shape 16x262144*(4x2^4)_NN_d
Tuning for shape 16x524288*(4x2^5)_NN_d
Tuning for shape 16x1048576*(4x2^6)_NN_d
Tuning for shape 16x2097152*(4x2^7)_NN_d
Tuning for shape 16x4194304*(4x2^8)_NN_d
Tuning for shape 16x65536*(4x2^1)_NN_d
Tuning for shape 16x131072*(4x2^2)_NN_d
Tuning for shape 16x262144*(4x2^3)_NN_d
Tuning for shape 16x524288*(4x2^4)_NN_d
Tuning for shape 16x1048576*(4x2^5)_NN_d
Tuning for shape 16x2097152*(4x2^6)_NN_d
Tuning for shape 16x4194304*(4x2^7)_NN_d
Tuning for shape 16x131072*(4x2^1)_NN_d
Tuning for shape 16x262144*(4x2^2)_NN_d
Tuning for shape 16x524288*(4x2^3)_NN_d
Tuning for shape 16x1048576*(4x2^4)_NN_d
Tuning for shape 16x2097152*(4x2^5)_NN_d
Tuning for shape 16x4194304*(4x2^6)_NN_d
Tuning for shape 16x262144*(4x2^1)_NN_d
Tuning for shape 16x524288*(4x2^2)_NN_d
Tuning for shape 16x1048576*(4x2^3)_NN_d
Tuning for shape 16x2097152*(4x2^4)_NN_d
Tuning for shape 16x4194304*(4x2^5)_NN_d
Tuning for shape 16x524288*(4x2^1)_NN_d
Tuning for shape 16x1048576*(4x2^2)_NN_d
Tuning for shape 16x2097152*(4x2^3)_NN_d
Tuning for shape 16x4194304*(4x2^4)_NN_d
Tuning for shape 16x1048576*(4x2^1)_NN_d
Tuning for shape 16x2097152*(4x2^2)_NN_d
Tuning for shape 16x4194304*(4x2^3)_NN_d
Tuning for shape 16x2097152*(4x2^1)_NN_d
Tuning for shape 16x4194304*(4x2^2)_NN_d
Tuning for shape 16x4194304*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^11  &  1.000 & 1.000 & 36.441 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 11 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4194304] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x to produce Y[64, 2048]
Matmul: 64 x 2048 x 4194304, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4096*(4x2^1)_NN_d
Tuning for shape 64x8192*(4x2^2)_NN_d
Tuning for shape 64x16384*(4x2^3)_NN_d
Tuning for shape 64x32768*(4x2^4)_NN_d
Tuning for shape 64x65536*(4x2^5)_NN_d
Tuning for shape 64x131072*(4x2^6)_NN_d
Tuning for shape 64x262144*(4x2^7)_NN_d
Tuning for shape 64x524288*(4x2^8)_NN_d
Tuning for shape 64x1048576*(4x2^9)_NN_d
Tuning for shape 64x2097152*(4x2^10)_NN_d
Tuning for shape 64x4194304*(4x2^11)_NN_d
Tuning for shape 64x8192*(4x2^1)_NN_d
Tuning for shape 64x16384*(4x2^2)_NN_d
Tuning for shape 64x32768*(4x2^3)_NN_d
Tuning for shape 64x65536*(4x2^4)_NN_d
Tuning for shape 64x131072*(4x2^5)_NN_d
Tuning for shape 64x262144*(4x2^6)_NN_d
Tuning for shape 64x524288*(4x2^7)_NN_d
Tuning for shape 64x1048576*(4x2^8)_NN_d
Tuning for shape 64x2097152*(4x2^9)_NN_d
Tuning for shape 64x4194304*(4x2^10)_NN_d
Tuning for shape 64x16384*(4x2^1)_NN_d
Tuning for shape 64x32768*(4x2^2)_NN_d
Tuning for shape 64x65536*(4x2^3)_NN_d
Tuning for shape 64x131072*(4x2^4)_NN_d
Tuning for shape 64x262144*(4x2^5)_NN_d
Tuning for shape 64x524288*(4x2^6)_NN_d
Tuning for shape 64x1048576*(4x2^7)_NN_d
Tuning for shape 64x2097152*(4x2^8)_NN_d
Tuning for shape 64x4194304*(4x2^9)_NN_d
Tuning for shape 64x32768*(4x2^1)_NN_d
Tuning for shape 64x65536*(4x2^2)_NN_d
Tuning for shape 64x131072*(4x2^3)_NN_d
Tuning for shape 64x262144*(4x2^4)_NN_d
Tuning for shape 64x524288*(4x2^5)_NN_d
Tuning for shape 64x1048576*(4x2^6)_NN_d
Tuning for shape 64x2097152*(4x2^7)_NN_d
Tuning for shape 64x4194304*(4x2^8)_NN_d
Tuning for shape 64x65536*(4x2^1)_NN_d
Tuning for shape 64x131072*(4x2^2)_NN_d
Tuning for shape 64x262144*(4x2^3)_NN_d
Tuning for shape 64x524288*(4x2^4)_NN_d
Tuning for shape 64x1048576*(4x2^5)_NN_d
Tuning for shape 64x2097152*(4x2^6)_NN_d
Tuning for shape 64x4194304*(4x2^7)_NN_d
Tuning for shape 64x131072*(4x2^1)_NN_d
Tuning for shape 64x262144*(4x2^2)_NN_d
Tuning for shape 64x524288*(4x2^3)_NN_d
Tuning for shape 64x1048576*(4x2^4)_NN_d
Tuning for shape 64x2097152*(4x2^5)_NN_d
Tuning for shape 64x4194304*(4x2^6)_NN_d
Tuning for shape 64x262144*(4x2^1)_NN_d
Tuning for shape 64x524288*(4x2^2)_NN_d
Tuning for shape 64x1048576*(4x2^3)_NN_d
Tuning for shape 64x2097152*(4x2^4)_NN_d
Tuning for shape 64x4194304*(4x2^5)_NN_d
Tuning for shape 64x524288*(4x2^1)_NN_d
Tuning for shape 64x1048576*(4x2^2)_NN_d
Tuning for shape 64x2097152*(4x2^3)_NN_d
Tuning for shape 64x4194304*(4x2^4)_NN_d
Tuning for shape 64x1048576*(4x2^1)_NN_d
Tuning for shape 64x2097152*(4x2^2)_NN_d
Tuning for shape 64x4194304*(4x2^3)_NN_d
Tuning for shape 64x2097152*(4x2^1)_NN_d
Tuning for shape 64x4194304*(4x2^2)_NN_d
Tuning for shape 64x4194304*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x2^11  &  1.000 & 1.000 & 34.434 & 0.029
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 12 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16777216] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x F_11 [4, 2] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 16777216, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8192*(4x2^1)_NN_d
Tuning for shape 1x16384*(4x2^2)_NN_d
Tuning for shape 1x32768*(4x2^3)_NN_d
Tuning for shape 1x65536*(4x2^4)_NN_d
Tuning for shape 1x131072*(4x2^5)_NN_d
Tuning for shape 1x262144*(4x2^6)_NN_d
Tuning for shape 1x524288*(4x2^7)_NN_d
Tuning for shape 1x1048576*(4x2^8)_NN_d
Tuning for shape 1x2097152*(4x2^9)_NN_d
Tuning for shape 1x4194304*(4x2^10)_NN_d
Tuning for shape 1x8388608*(4x2^11)_NN_d
Tuning for shape 1x16777216*(4x2^12)_NN_d
Tuning for shape 1x16384*(4x2^1)_NN_d
Tuning for shape 1x32768*(4x2^2)_NN_d
Tuning for shape 1x65536*(4x2^3)_NN_d
Tuning for shape 1x131072*(4x2^4)_NN_d
Tuning for shape 1x262144*(4x2^5)_NN_d
Tuning for shape 1x524288*(4x2^6)_NN_d
Tuning for shape 1x1048576*(4x2^7)_NN_d
Tuning for shape 1x2097152*(4x2^8)_NN_d
Tuning for shape 1x4194304*(4x2^9)_NN_d
Tuning for shape 1x8388608*(4x2^10)_NN_d
Tuning for shape 1x16777216*(4x2^11)_NN_d
Tuning for shape 1x32768*(4x2^1)_NN_d
Tuning for shape 1x65536*(4x2^2)_NN_d
Tuning for shape 1x131072*(4x2^3)_NN_d
Tuning for shape 1x262144*(4x2^4)_NN_d
Tuning for shape 1x524288*(4x2^5)_NN_d
Tuning for shape 1x1048576*(4x2^6)_NN_d
Tuning for shape 1x2097152*(4x2^7)_NN_d
Tuning for shape 1x4194304*(4x2^8)_NN_d
Tuning for shape 1x8388608*(4x2^9)_NN_d
Tuning for shape 1x16777216*(4x2^10)_NN_d
Tuning for shape 1x65536*(4x2^1)_NN_d
Tuning for shape 1x131072*(4x2^2)_NN_d
Tuning for shape 1x262144*(4x2^3)_NN_d
Tuning for shape 1x524288*(4x2^4)_NN_d
Tuning for shape 1x1048576*(4x2^5)_NN_d
Tuning for shape 1x2097152*(4x2^6)_NN_d
Tuning for shape 1x4194304*(4x2^7)_NN_d
Tuning for shape 1x8388608*(4x2^8)_NN_d
Tuning for shape 1x16777216*(4x2^9)_NN_d
Tuning for shape 1x131072*(4x2^1)_NN_d
Tuning for shape 1x262144*(4x2^2)_NN_d
Tuning for shape 1x524288*(4x2^3)_NN_d
Tuning for shape 1x1048576*(4x2^4)_NN_d
Tuning for shape 1x2097152*(4x2^5)_NN_d
Tuning for shape 1x4194304*(4x2^6)_NN_d
Tuning for shape 1x8388608*(4x2^7)_NN_d
Tuning for shape 1x16777216*(4x2^8)_NN_d
Tuning for shape 1x262144*(4x2^1)_NN_d
Tuning for shape 1x524288*(4x2^2)_NN_d
Tuning for shape 1x1048576*(4x2^3)_NN_d
Tuning for shape 1x2097152*(4x2^4)_NN_d
Tuning for shape 1x4194304*(4x2^5)_NN_d
Tuning for shape 1x8388608*(4x2^6)_NN_d
Tuning for shape 1x16777216*(4x2^7)_NN_d
Tuning for shape 1x524288*(4x2^1)_NN_d
Tuning for shape 1x1048576*(4x2^2)_NN_d
Tuning for shape 1x2097152*(4x2^3)_NN_d
Tuning for shape 1x4194304*(4x2^4)_NN_d
Tuning for shape 1x8388608*(4x2^5)_NN_d
Tuning for shape 1x16777216*(4x2^6)_NN_d
Tuning for shape 1x1048576*(4x2^1)_NN_d
Tuning for shape 1x2097152*(4x2^2)_NN_d
Tuning for shape 1x4194304*(4x2^3)_NN_d
Tuning for shape 1x8388608*(4x2^4)_NN_d
Tuning for shape 1x16777216*(4x2^5)_NN_d
Tuning for shape 1x2097152*(4x2^1)_NN_d
Tuning for shape 1x4194304*(4x2^2)_NN_d
Tuning for shape 1x8388608*(4x2^3)_NN_d
Tuning for shape 1x16777216*(4x2^4)_NN_d
Tuning for shape 1x4194304*(4x2^1)_NN_d
Tuning for shape 1x8388608*(4x2^2)_NN_d
Tuning for shape 1x16777216*(4x2^3)_NN_d
Tuning for shape 1x8388608*(4x2^1)_NN_d
Tuning for shape 1x16777216*(4x2^2)_NN_d
Tuning for shape 1x16777216*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^12  &  1.000 & 1.000 & 38.948 & 0.026
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 12 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16777216] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x F_11 [4, 2] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 16777216, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8192*(4x2^1)_NN_d
Tuning for shape 4x16384*(4x2^2)_NN_d
Tuning for shape 4x32768*(4x2^3)_NN_d
Tuning for shape 4x65536*(4x2^4)_NN_d
Tuning for shape 4x131072*(4x2^5)_NN_d
Tuning for shape 4x262144*(4x2^6)_NN_d
Tuning for shape 4x524288*(4x2^7)_NN_d
Tuning for shape 4x1048576*(4x2^8)_NN_d
Tuning for shape 4x2097152*(4x2^9)_NN_d
Tuning for shape 4x4194304*(4x2^10)_NN_d
Tuning for shape 4x8388608*(4x2^11)_NN_d
Tuning for shape 4x16777216*(4x2^12)_NN_d
Tuning for shape 4x16384*(4x2^1)_NN_d
Tuning for shape 4x32768*(4x2^2)_NN_d
Tuning for shape 4x65536*(4x2^3)_NN_d
Tuning for shape 4x131072*(4x2^4)_NN_d
Tuning for shape 4x262144*(4x2^5)_NN_d
Tuning for shape 4x524288*(4x2^6)_NN_d
Tuning for shape 4x1048576*(4x2^7)_NN_d
Tuning for shape 4x2097152*(4x2^8)_NN_d
Tuning for shape 4x4194304*(4x2^9)_NN_d
Tuning for shape 4x8388608*(4x2^10)_NN_d
Tuning for shape 4x16777216*(4x2^11)_NN_d
Tuning for shape 4x32768*(4x2^1)_NN_d
Tuning for shape 4x65536*(4x2^2)_NN_d
Tuning for shape 4x131072*(4x2^3)_NN_d
Tuning for shape 4x262144*(4x2^4)_NN_d
Tuning for shape 4x524288*(4x2^5)_NN_d
Tuning for shape 4x1048576*(4x2^6)_NN_d
Tuning for shape 4x2097152*(4x2^7)_NN_d
Tuning for shape 4x4194304*(4x2^8)_NN_d
Tuning for shape 4x8388608*(4x2^9)_NN_d
Tuning for shape 4x16777216*(4x2^10)_NN_d
Tuning for shape 4x65536*(4x2^1)_NN_d
Tuning for shape 4x131072*(4x2^2)_NN_d
Tuning for shape 4x262144*(4x2^3)_NN_d
Tuning for shape 4x524288*(4x2^4)_NN_d
Tuning for shape 4x1048576*(4x2^5)_NN_d
Tuning for shape 4x2097152*(4x2^6)_NN_d
Tuning for shape 4x4194304*(4x2^7)_NN_d
Tuning for shape 4x8388608*(4x2^8)_NN_d
Tuning for shape 4x16777216*(4x2^9)_NN_d
Tuning for shape 4x131072*(4x2^1)_NN_d
Tuning for shape 4x262144*(4x2^2)_NN_d
Tuning for shape 4x524288*(4x2^3)_NN_d
Tuning for shape 4x1048576*(4x2^4)_NN_d
Tuning for shape 4x2097152*(4x2^5)_NN_d
Tuning for shape 4x4194304*(4x2^6)_NN_d
Tuning for shape 4x8388608*(4x2^7)_NN_d
Tuning for shape 4x16777216*(4x2^8)_NN_d
Tuning for shape 4x262144*(4x2^1)_NN_d
Tuning for shape 4x524288*(4x2^2)_NN_d
Tuning for shape 4x1048576*(4x2^3)_NN_d
Tuning for shape 4x2097152*(4x2^4)_NN_d
Tuning for shape 4x4194304*(4x2^5)_NN_d
Tuning for shape 4x8388608*(4x2^6)_NN_d
Tuning for shape 4x16777216*(4x2^7)_NN_d
Tuning for shape 4x524288*(4x2^1)_NN_d
Tuning for shape 4x1048576*(4x2^2)_NN_d
Tuning for shape 4x2097152*(4x2^3)_NN_d
Tuning for shape 4x4194304*(4x2^4)_NN_d
Tuning for shape 4x8388608*(4x2^5)_NN_d
Tuning for shape 4x16777216*(4x2^6)_NN_d
Tuning for shape 4x1048576*(4x2^1)_NN_d
Tuning for shape 4x2097152*(4x2^2)_NN_d
Tuning for shape 4x4194304*(4x2^3)_NN_d
Tuning for shape 4x8388608*(4x2^4)_NN_d
Tuning for shape 4x16777216*(4x2^5)_NN_d
Tuning for shape 4x2097152*(4x2^1)_NN_d
Tuning for shape 4x4194304*(4x2^2)_NN_d
Tuning for shape 4x8388608*(4x2^3)_NN_d
Tuning for shape 4x16777216*(4x2^4)_NN_d
Tuning for shape 4x4194304*(4x2^1)_NN_d
Tuning for shape 4x8388608*(4x2^2)_NN_d
Tuning for shape 4x16777216*(4x2^3)_NN_d
Tuning for shape 4x8388608*(4x2^1)_NN_d
Tuning for shape 4x16777216*(4x2^2)_NN_d
Tuning for shape 4x16777216*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^12  &  1.000 & 1.000 & 36.453 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 12 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16777216] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x F_11 [4, 2] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 16777216, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8192*(4x2^1)_NN_d
Tuning for shape 16x16384*(4x2^2)_NN_d
Tuning for shape 16x32768*(4x2^3)_NN_d
Tuning for shape 16x65536*(4x2^4)_NN_d
Tuning for shape 16x131072*(4x2^5)_NN_d
Tuning for shape 16x262144*(4x2^6)_NN_d
Tuning for shape 16x524288*(4x2^7)_NN_d
Tuning for shape 16x1048576*(4x2^8)_NN_d
Tuning for shape 16x2097152*(4x2^9)_NN_d
Tuning for shape 16x4194304*(4x2^10)_NN_d
Tuning for shape 16x8388608*(4x2^11)_NN_d
Tuning for shape 16x16777216*(4x2^12)_NN_d
Tuning for shape 16x16384*(4x2^1)_NN_d
Tuning for shape 16x32768*(4x2^2)_NN_d
Tuning for shape 16x65536*(4x2^3)_NN_d
Tuning for shape 16x131072*(4x2^4)_NN_d
Tuning for shape 16x262144*(4x2^5)_NN_d
Tuning for shape 16x524288*(4x2^6)_NN_d
Tuning for shape 16x1048576*(4x2^7)_NN_d
Tuning for shape 16x2097152*(4x2^8)_NN_d
Tuning for shape 16x4194304*(4x2^9)_NN_d
Tuning for shape 16x8388608*(4x2^10)_NN_d
Tuning for shape 16x16777216*(4x2^11)_NN_d
Tuning for shape 16x32768*(4x2^1)_NN_d
Tuning for shape 16x65536*(4x2^2)_NN_d
Tuning for shape 16x131072*(4x2^3)_NN_d
Tuning for shape 16x262144*(4x2^4)_NN_d
Tuning for shape 16x524288*(4x2^5)_NN_d
Tuning for shape 16x1048576*(4x2^6)_NN_d
Tuning for shape 16x2097152*(4x2^7)_NN_d
Tuning for shape 16x4194304*(4x2^8)_NN_d
Tuning for shape 16x8388608*(4x2^9)_NN_d
Tuning for shape 16x16777216*(4x2^10)_NN_d
Tuning for shape 16x65536*(4x2^1)_NN_d
Tuning for shape 16x131072*(4x2^2)_NN_d
Tuning for shape 16x262144*(4x2^3)_NN_d
Tuning for shape 16x524288*(4x2^4)_NN_d
Tuning for shape 16x1048576*(4x2^5)_NN_d
Tuning for shape 16x2097152*(4x2^6)_NN_d
Tuning for shape 16x4194304*(4x2^7)_NN_d
Tuning for shape 16x8388608*(4x2^8)_NN_d
Tuning for shape 16x16777216*(4x2^9)_NN_d
Tuning for shape 16x131072*(4x2^1)_NN_d
Tuning for shape 16x262144*(4x2^2)_NN_d
Tuning for shape 16x524288*(4x2^3)_NN_d
Tuning for shape 16x1048576*(4x2^4)_NN_d
Tuning for shape 16x2097152*(4x2^5)_NN_d
Tuning for shape 16x4194304*(4x2^6)_NN_d
Tuning for shape 16x8388608*(4x2^7)_NN_d
Tuning for shape 16x16777216*(4x2^8)_NN_d
Tuning for shape 16x262144*(4x2^1)_NN_d
Tuning for shape 16x524288*(4x2^2)_NN_d
Tuning for shape 16x1048576*(4x2^3)_NN_d
Tuning for shape 16x2097152*(4x2^4)_NN_d
Tuning for shape 16x4194304*(4x2^5)_NN_d
Tuning for shape 16x8388608*(4x2^6)_NN_d
Tuning for shape 16x16777216*(4x2^7)_NN_d
Tuning for shape 16x524288*(4x2^1)_NN_d
Tuning for shape 16x1048576*(4x2^2)_NN_d
Tuning for shape 16x2097152*(4x2^3)_NN_d
Tuning for shape 16x4194304*(4x2^4)_NN_d
Tuning for shape 16x8388608*(4x2^5)_NN_d
Tuning for shape 16x16777216*(4x2^6)_NN_d
Tuning for shape 16x1048576*(4x2^1)_NN_d
Tuning for shape 16x2097152*(4x2^2)_NN_d
Tuning for shape 16x4194304*(4x2^3)_NN_d
Tuning for shape 16x8388608*(4x2^4)_NN_d
Tuning for shape 16x16777216*(4x2^5)_NN_d
Tuning for shape 16x2097152*(4x2^1)_NN_d
Tuning for shape 16x4194304*(4x2^2)_NN_d
Tuning for shape 16x8388608*(4x2^3)_NN_d
Tuning for shape 16x16777216*(4x2^4)_NN_d
Tuning for shape 16x4194304*(4x2^1)_NN_d
Tuning for shape 16x8388608*(4x2^2)_NN_d
Tuning for shape 16x16777216*(4x2^3)_NN_d
Tuning for shape 16x8388608*(4x2^1)_NN_d
Tuning for shape 16x16777216*(4x2^2)_NN_d
Tuning for shape 16x16777216*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x2^12  &  1.000 & 1.000 & 36.570 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 13 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 67108864] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x F_11 [4, 2] x F_12 [4, 2] x to produce Y[1, 8192]
Matmul: 1 x 8192 x 67108864, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16384*(4x2^1)_NN_d
Tuning for shape 1x32768*(4x2^2)_NN_d
Tuning for shape 1x65536*(4x2^3)_NN_d
Tuning for shape 1x131072*(4x2^4)_NN_d
Tuning for shape 1x262144*(4x2^5)_NN_d
Tuning for shape 1x524288*(4x2^6)_NN_d
Tuning for shape 1x1048576*(4x2^7)_NN_d
Tuning for shape 1x2097152*(4x2^8)_NN_d
Tuning for shape 1x4194304*(4x2^9)_NN_d
Tuning for shape 1x8388608*(4x2^10)_NN_d
Tuning for shape 1x16777216*(4x2^11)_NN_d
Tuning for shape 1x33554432*(4x2^12)_NN_d
Tuning for shape 1x67108864*(4x2^13)_NN_d
Tuning for shape 1x32768*(4x2^1)_NN_d
Tuning for shape 1x65536*(4x2^2)_NN_d
Tuning for shape 1x131072*(4x2^3)_NN_d
Tuning for shape 1x262144*(4x2^4)_NN_d
Tuning for shape 1x524288*(4x2^5)_NN_d
Tuning for shape 1x1048576*(4x2^6)_NN_d
Tuning for shape 1x2097152*(4x2^7)_NN_d
Tuning for shape 1x4194304*(4x2^8)_NN_d
Tuning for shape 1x8388608*(4x2^9)_NN_d
Tuning for shape 1x16777216*(4x2^10)_NN_d
Tuning for shape 1x33554432*(4x2^11)_NN_d
Tuning for shape 1x67108864*(4x2^12)_NN_d
Tuning for shape 1x65536*(4x2^1)_NN_d
Tuning for shape 1x131072*(4x2^2)_NN_d
Tuning for shape 1x262144*(4x2^3)_NN_d
Tuning for shape 1x524288*(4x2^4)_NN_d
Tuning for shape 1x1048576*(4x2^5)_NN_d
Tuning for shape 1x2097152*(4x2^6)_NN_d
Tuning for shape 1x4194304*(4x2^7)_NN_d
Tuning for shape 1x8388608*(4x2^8)_NN_d
Tuning for shape 1x16777216*(4x2^9)_NN_d
Tuning for shape 1x33554432*(4x2^10)_NN_d
Tuning for shape 1x67108864*(4x2^11)_NN_d
Tuning for shape 1x131072*(4x2^1)_NN_d
Tuning for shape 1x262144*(4x2^2)_NN_d
Tuning for shape 1x524288*(4x2^3)_NN_d
Tuning for shape 1x1048576*(4x2^4)_NN_d
Tuning for shape 1x2097152*(4x2^5)_NN_d
Tuning for shape 1x4194304*(4x2^6)_NN_d
Tuning for shape 1x8388608*(4x2^7)_NN_d
Tuning for shape 1x16777216*(4x2^8)_NN_d
Tuning for shape 1x33554432*(4x2^9)_NN_d
Tuning for shape 1x67108864*(4x2^10)_NN_d
Tuning for shape 1x262144*(4x2^1)_NN_d
Tuning for shape 1x524288*(4x2^2)_NN_d
Tuning for shape 1x1048576*(4x2^3)_NN_d
Tuning for shape 1x2097152*(4x2^4)_NN_d
Tuning for shape 1x4194304*(4x2^5)_NN_d
Tuning for shape 1x8388608*(4x2^6)_NN_d
Tuning for shape 1x16777216*(4x2^7)_NN_d
Tuning for shape 1x33554432*(4x2^8)_NN_d
Tuning for shape 1x67108864*(4x2^9)_NN_d
Tuning for shape 1x524288*(4x2^1)_NN_d
Tuning for shape 1x1048576*(4x2^2)_NN_d
Tuning for shape 1x2097152*(4x2^3)_NN_d
Tuning for shape 1x4194304*(4x2^4)_NN_d
Tuning for shape 1x8388608*(4x2^5)_NN_d
Tuning for shape 1x16777216*(4x2^6)_NN_d
Tuning for shape 1x33554432*(4x2^7)_NN_d
Tuning for shape 1x67108864*(4x2^8)_NN_d
Tuning for shape 1x1048576*(4x2^1)_NN_d
Tuning for shape 1x2097152*(4x2^2)_NN_d
Tuning for shape 1x4194304*(4x2^3)_NN_d
Tuning for shape 1x8388608*(4x2^4)_NN_d
Tuning for shape 1x16777216*(4x2^5)_NN_d
Tuning for shape 1x33554432*(4x2^6)_NN_d
Tuning for shape 1x67108864*(4x2^7)_NN_d
Tuning for shape 1x2097152*(4x2^1)_NN_d
Tuning for shape 1x4194304*(4x2^2)_NN_d
Tuning for shape 1x8388608*(4x2^3)_NN_d
Tuning for shape 1x16777216*(4x2^4)_NN_d
Tuning for shape 1x33554432*(4x2^5)_NN_d
Tuning for shape 1x67108864*(4x2^6)_NN_d
Tuning for shape 1x4194304*(4x2^1)_NN_d
Tuning for shape 1x8388608*(4x2^2)_NN_d
Tuning for shape 1x16777216*(4x2^3)_NN_d
Tuning for shape 1x33554432*(4x2^4)_NN_d
Tuning for shape 1x67108864*(4x2^5)_NN_d
Tuning for shape 1x8388608*(4x2^1)_NN_d
Tuning for shape 1x16777216*(4x2^2)_NN_d
Tuning for shape 1x33554432*(4x2^3)_NN_d
Tuning for shape 1x67108864*(4x2^4)_NN_d
Tuning for shape 1x16777216*(4x2^1)_NN_d
Tuning for shape 1x33554432*(4x2^2)_NN_d
Tuning for shape 1x67108864*(4x2^3)_NN_d
Tuning for shape 1x33554432*(4x2^1)_NN_d
Tuning for shape 1x67108864*(4x2^2)_NN_d
Tuning for shape 1x67108864*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^13  &  1.000 & 1.000 & 39.969 & 0.025
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 13 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 67108864] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x F_11 [4, 2] x F_12 [4, 2] x to produce Y[4, 8192]
Matmul: 4 x 8192 x 67108864, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16384*(4x2^1)_NN_d
Tuning for shape 4x32768*(4x2^2)_NN_d
Tuning for shape 4x65536*(4x2^3)_NN_d
Tuning for shape 4x131072*(4x2^4)_NN_d
Tuning for shape 4x262144*(4x2^5)_NN_d
Tuning for shape 4x524288*(4x2^6)_NN_d
Tuning for shape 4x1048576*(4x2^7)_NN_d
Tuning for shape 4x2097152*(4x2^8)_NN_d
Tuning for shape 4x4194304*(4x2^9)_NN_d
Tuning for shape 4x8388608*(4x2^10)_NN_d
Tuning for shape 4x16777216*(4x2^11)_NN_d
Tuning for shape 4x33554432*(4x2^12)_NN_d
Tuning for shape 4x67108864*(4x2^13)_NN_d
Tuning for shape 4x32768*(4x2^1)_NN_d
Tuning for shape 4x65536*(4x2^2)_NN_d
Tuning for shape 4x131072*(4x2^3)_NN_d
Tuning for shape 4x262144*(4x2^4)_NN_d
Tuning for shape 4x524288*(4x2^5)_NN_d
Tuning for shape 4x1048576*(4x2^6)_NN_d
Tuning for shape 4x2097152*(4x2^7)_NN_d
Tuning for shape 4x4194304*(4x2^8)_NN_d
Tuning for shape 4x8388608*(4x2^9)_NN_d
Tuning for shape 4x16777216*(4x2^10)_NN_d
Tuning for shape 4x33554432*(4x2^11)_NN_d
Tuning for shape 4x67108864*(4x2^12)_NN_d
Tuning for shape 4x65536*(4x2^1)_NN_d
Tuning for shape 4x131072*(4x2^2)_NN_d
Tuning for shape 4x262144*(4x2^3)_NN_d
Tuning for shape 4x524288*(4x2^4)_NN_d
Tuning for shape 4x1048576*(4x2^5)_NN_d
Tuning for shape 4x2097152*(4x2^6)_NN_d
Tuning for shape 4x4194304*(4x2^7)_NN_d
Tuning for shape 4x8388608*(4x2^8)_NN_d
Tuning for shape 4x16777216*(4x2^9)_NN_d
Tuning for shape 4x33554432*(4x2^10)_NN_d
Tuning for shape 4x67108864*(4x2^11)_NN_d
Tuning for shape 4x131072*(4x2^1)_NN_d
Tuning for shape 4x262144*(4x2^2)_NN_d
Tuning for shape 4x524288*(4x2^3)_NN_d
Tuning for shape 4x1048576*(4x2^4)_NN_d
Tuning for shape 4x2097152*(4x2^5)_NN_d
Tuning for shape 4x4194304*(4x2^6)_NN_d
Tuning for shape 4x8388608*(4x2^7)_NN_d
Tuning for shape 4x16777216*(4x2^8)_NN_d
Tuning for shape 4x33554432*(4x2^9)_NN_d
Tuning for shape 4x67108864*(4x2^10)_NN_d
Tuning for shape 4x262144*(4x2^1)_NN_d
Tuning for shape 4x524288*(4x2^2)_NN_d
Tuning for shape 4x1048576*(4x2^3)_NN_d
Tuning for shape 4x2097152*(4x2^4)_NN_d
Tuning for shape 4x4194304*(4x2^5)_NN_d
Tuning for shape 4x8388608*(4x2^6)_NN_d
Tuning for shape 4x16777216*(4x2^7)_NN_d
Tuning for shape 4x33554432*(4x2^8)_NN_d
Tuning for shape 4x67108864*(4x2^9)_NN_d
Tuning for shape 4x524288*(4x2^1)_NN_d
Tuning for shape 4x1048576*(4x2^2)_NN_d
Tuning for shape 4x2097152*(4x2^3)_NN_d
Tuning for shape 4x4194304*(4x2^4)_NN_d
Tuning for shape 4x8388608*(4x2^5)_NN_d
Tuning for shape 4x16777216*(4x2^6)_NN_d
Tuning for shape 4x33554432*(4x2^7)_NN_d
Tuning for shape 4x67108864*(4x2^8)_NN_d
Tuning for shape 4x1048576*(4x2^1)_NN_d
Tuning for shape 4x2097152*(4x2^2)_NN_d
Tuning for shape 4x4194304*(4x2^3)_NN_d
Tuning for shape 4x8388608*(4x2^4)_NN_d
Tuning for shape 4x16777216*(4x2^5)_NN_d
Tuning for shape 4x33554432*(4x2^6)_NN_d
Tuning for shape 4x67108864*(4x2^7)_NN_d
Tuning for shape 4x2097152*(4x2^1)_NN_d
Tuning for shape 4x4194304*(4x2^2)_NN_d
Tuning for shape 4x8388608*(4x2^3)_NN_d
Tuning for shape 4x16777216*(4x2^4)_NN_d
Tuning for shape 4x33554432*(4x2^5)_NN_d
Tuning for shape 4x67108864*(4x2^6)_NN_d
Tuning for shape 4x4194304*(4x2^1)_NN_d
Tuning for shape 4x8388608*(4x2^2)_NN_d
Tuning for shape 4x16777216*(4x2^3)_NN_d
Tuning for shape 4x33554432*(4x2^4)_NN_d
Tuning for shape 4x67108864*(4x2^5)_NN_d
Tuning for shape 4x8388608*(4x2^1)_NN_d
Tuning for shape 4x16777216*(4x2^2)_NN_d
Tuning for shape 4x33554432*(4x2^3)_NN_d
Tuning for shape 4x67108864*(4x2^4)_NN_d
Tuning for shape 4x16777216*(4x2^1)_NN_d
Tuning for shape 4x33554432*(4x2^2)_NN_d
Tuning for shape 4x67108864*(4x2^3)_NN_d
Tuning for shape 4x33554432*(4x2^1)_NN_d
Tuning for shape 4x67108864*(4x2^2)_NN_d
Tuning for shape 4x67108864*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x2^13  &  1.000 & 1.000 & 36.675 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 14 -p 4 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 268435456] with F_0 [4, 2] x F_1 [4, 2] x F_2 [4, 2] x F_3 [4, 2] x F_4 [4, 2] x F_5 [4, 2] x F_6 [4, 2] x F_7 [4, 2] x F_8 [4, 2] x F_9 [4, 2] x F_10 [4, 2] x F_11 [4, 2] x F_12 [4, 2] x F_13 [4, 2] x to produce Y[1, 16384]
Matmul: 1 x 16384 x 268435456, Num KP Factors: 14
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32768*(4x2^1)_NN_d
Tuning for shape 1x65536*(4x2^2)_NN_d
Tuning for shape 1x131072*(4x2^3)_NN_d
Tuning for shape 1x262144*(4x2^4)_NN_d
Tuning for shape 1x524288*(4x2^5)_NN_d
Tuning for shape 1x1048576*(4x2^6)_NN_d
Tuning for shape 1x2097152*(4x2^7)_NN_d
Tuning for shape 1x4194304*(4x2^8)_NN_d
Tuning for shape 1x8388608*(4x2^9)_NN_d
Tuning for shape 1x16777216*(4x2^10)_NN_d
Tuning for shape 1x33554432*(4x2^11)_NN_d
Tuning for shape 1x67108864*(4x2^12)_NN_d
Tuning for shape 1x134217728*(4x2^13)_NN_d
Tuning for shape 1x268435456*(4x2^14)_NN_d
Tuning for shape 1x65536*(4x2^1)_NN_d
Tuning for shape 1x131072*(4x2^2)_NN_d
Tuning for shape 1x262144*(4x2^3)_NN_d
Tuning for shape 1x524288*(4x2^4)_NN_d
Tuning for shape 1x1048576*(4x2^5)_NN_d
Tuning for shape 1x2097152*(4x2^6)_NN_d
Tuning for shape 1x4194304*(4x2^7)_NN_d
Tuning for shape 1x8388608*(4x2^8)_NN_d
Tuning for shape 1x16777216*(4x2^9)_NN_d
Tuning for shape 1x33554432*(4x2^10)_NN_d
Tuning for shape 1x67108864*(4x2^11)_NN_d
Tuning for shape 1x134217728*(4x2^12)_NN_d
Tuning for shape 1x268435456*(4x2^13)_NN_d
Tuning for shape 1x131072*(4x2^1)_NN_d
Tuning for shape 1x262144*(4x2^2)_NN_d
Tuning for shape 1x524288*(4x2^3)_NN_d
Tuning for shape 1x1048576*(4x2^4)_NN_d
Tuning for shape 1x2097152*(4x2^5)_NN_d
Tuning for shape 1x4194304*(4x2^6)_NN_d
Tuning for shape 1x8388608*(4x2^7)_NN_d
Tuning for shape 1x16777216*(4x2^8)_NN_d
Tuning for shape 1x33554432*(4x2^9)_NN_d
Tuning for shape 1x67108864*(4x2^10)_NN_d
Tuning for shape 1x134217728*(4x2^11)_NN_d
Tuning for shape 1x268435456*(4x2^12)_NN_d
Tuning for shape 1x262144*(4x2^1)_NN_d
Tuning for shape 1x524288*(4x2^2)_NN_d
Tuning for shape 1x1048576*(4x2^3)_NN_d
Tuning for shape 1x2097152*(4x2^4)_NN_d
Tuning for shape 1x4194304*(4x2^5)_NN_d
Tuning for shape 1x8388608*(4x2^6)_NN_d
Tuning for shape 1x16777216*(4x2^7)_NN_d
Tuning for shape 1x33554432*(4x2^8)_NN_d
Tuning for shape 1x67108864*(4x2^9)_NN_d
Tuning for shape 1x134217728*(4x2^10)_NN_d
Tuning for shape 1x268435456*(4x2^11)_NN_d
Tuning for shape 1x524288*(4x2^1)_NN_d
Tuning for shape 1x1048576*(4x2^2)_NN_d
Tuning for shape 1x2097152*(4x2^3)_NN_d
Tuning for shape 1x4194304*(4x2^4)_NN_d
Tuning for shape 1x8388608*(4x2^5)_NN_d
Tuning for shape 1x16777216*(4x2^6)_NN_d
Tuning for shape 1x33554432*(4x2^7)_NN_d
Tuning for shape 1x67108864*(4x2^8)_NN_d
Tuning for shape 1x134217728*(4x2^9)_NN_d
Tuning for shape 1x268435456*(4x2^10)_NN_d
Tuning for shape 1x1048576*(4x2^1)_NN_d
Tuning for shape 1x2097152*(4x2^2)_NN_d
Tuning for shape 1x4194304*(4x2^3)_NN_d
Tuning for shape 1x8388608*(4x2^4)_NN_d
Tuning for shape 1x16777216*(4x2^5)_NN_d
Tuning for shape 1x33554432*(4x2^6)_NN_d
Tuning for shape 1x67108864*(4x2^7)_NN_d
Tuning for shape 1x134217728*(4x2^8)_NN_d
Tuning for shape 1x268435456*(4x2^9)_NN_d
Tuning for shape 1x2097152*(4x2^1)_NN_d
Tuning for shape 1x4194304*(4x2^2)_NN_d
Tuning for shape 1x8388608*(4x2^3)_NN_d
Tuning for shape 1x16777216*(4x2^4)_NN_d
Tuning for shape 1x33554432*(4x2^5)_NN_d
Tuning for shape 1x67108864*(4x2^6)_NN_d
Tuning for shape 1x134217728*(4x2^7)_NN_d
Tuning for shape 1x268435456*(4x2^8)_NN_d
Tuning for shape 1x4194304*(4x2^1)_NN_d
Tuning for shape 1x8388608*(4x2^2)_NN_d
Tuning for shape 1x16777216*(4x2^3)_NN_d
Tuning for shape 1x33554432*(4x2^4)_NN_d
Tuning for shape 1x67108864*(4x2^5)_NN_d
Tuning for shape 1x134217728*(4x2^6)_NN_d
Tuning for shape 1x268435456*(4x2^7)_NN_d
Tuning for shape 1x8388608*(4x2^1)_NN_d
Tuning for shape 1x16777216*(4x2^2)_NN_d
Tuning for shape 1x33554432*(4x2^3)_NN_d
Tuning for shape 1x67108864*(4x2^4)_NN_d
Tuning for shape 1x134217728*(4x2^5)_NN_d
Tuning for shape 1x268435456*(4x2^6)_NN_d
Tuning for shape 1x16777216*(4x2^1)_NN_d
Tuning for shape 1x33554432*(4x2^2)_NN_d
Tuning for shape 1x67108864*(4x2^3)_NN_d
Tuning for shape 1x134217728*(4x2^4)_NN_d
Tuning for shape 1x268435456*(4x2^5)_NN_d
Tuning for shape 1x33554432*(4x2^1)_NN_d
Tuning for shape 1x67108864*(4x2^2)_NN_d
Tuning for shape 1x134217728*(4x2^3)_NN_d
Tuning for shape 1x268435456*(4x2^4)_NN_d
Tuning for shape 1x67108864*(4x2^1)_NN_d
Tuning for shape 1x134217728*(4x2^2)_NN_d
Tuning for shape 1x268435456*(4x2^3)_NN_d
Tuning for shape 1x134217728*(4x2^1)_NN_d
Tuning for shape 1x268435456*(4x2^2)_NN_d
Tuning for shape 1x268435456*(4x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x2^14  &  1.000 & 1.000 & 40.281 & 0.025
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [4, 4] x to produce Y[1, 4]
Matmul: 1 x 4 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^1  &  1.000 & 1.000 & 0.000 & 8247.420
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [4, 4] x to produce Y[4, 4]
Matmul: 4 x 4 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^1  &  1.000 & 1.000 & 0.000 & 3251.713
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [4, 4] x to produce Y[16, 4]
Matmul: 16 x 4 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^1  &  1.000 & 1.000 & 0.002 & 602.775
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [4, 4] x to produce Y[64, 4]
Matmul: 64 x 4 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^1  &  1.000 & 1.000 & 0.007 & 151.619
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [4, 4] x to produce Y[256, 4]
Matmul: 256 x 4 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^1  &  1.000 & 1.000 & 0.027 & 37.694
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [4, 4] x to produce Y[1024, 4]
Matmul: 1024 x 4 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x4^1  &  1.000 & 1.000 & 0.103 & 9.696
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [4, 4] x F_1 [4, 4] x to produce Y[1, 16]
Matmul: 1 x 16 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16*(4x4^1)_NN_d
Tuning for shape 1x16*(4x4^2)_NN_d
Tuning for shape 1x16*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^2  &  1.000 & 1.000 & 0.001 & 1643.598
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [4, 4] x F_1 [4, 4] x to produce Y[4, 16]
Matmul: 4 x 16 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16*(4x4^1)_NN_d
Tuning for shape 4x16*(4x4^2)_NN_d
Tuning for shape 4x16*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^2  &  1.000 & 1.000 & 0.002 & 438.106
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [4, 4] x F_1 [4, 4] x to produce Y[16, 16]
Matmul: 16 x 16 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16*(4x4^1)_NN_d
Tuning for shape 16x16*(4x4^2)_NN_d
Tuning for shape 16x16*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^2  &  1.000 & 1.000 & 0.009 & 109.826
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [4, 4] x F_1 [4, 4] x to produce Y[64, 16]
Matmul: 64 x 16 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16*(4x4^1)_NN_d
Tuning for shape 64x16*(4x4^2)_NN_d
Tuning for shape 64x16*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^2  &  1.000 & 1.000 & 0.035 & 28.361
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [4, 4] x F_1 [4, 4] x to produce Y[256, 16]
Matmul: 256 x 16 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16*(4x4^1)_NN_d
Tuning for shape 256x16*(4x4^2)_NN_d
Tuning for shape 256x16*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^2  &  1.000 & 1.000 & 0.143 & 6.999
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [4, 4] x F_1 [4, 4] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16*(4x4^1)_NN_d
Tuning for shape 1024x16*(4x4^2)_NN_d
Tuning for shape 1024x16*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x4^2  &  1.000 & 1.000 & 0.562 & 1.781
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x to produce Y[1, 64]
Matmul: 1 x 64 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x64*(4x4^1)_NN_d
Tuning for shape 1x64*(4x4^2)_NN_d
Tuning for shape 1x64*(4x4^3)_NN_d
Tuning for shape 1x64*(4x4^1)_NN_d
Tuning for shape 1x64*(4x4^2)_NN_d
Tuning for shape 1x64*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^3  &  1.000 & 1.000 & 0.003 & 341.462
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x to produce Y[4, 64]
Matmul: 4 x 64 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x64*(4x4^1)_NN_d
Tuning for shape 4x64*(4x4^2)_NN_d
Tuning for shape 4x64*(4x4^3)_NN_d
Tuning for shape 4x64*(4x4^1)_NN_d
Tuning for shape 4x64*(4x4^2)_NN_d
Tuning for shape 4x64*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^3  &  1.000 & 1.000 & 0.011 & 91.823
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x to produce Y[16, 64]
Matmul: 16 x 64 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x64*(4x4^1)_NN_d
Tuning for shape 16x64*(4x4^2)_NN_d
Tuning for shape 16x64*(4x4^3)_NN_d
Tuning for shape 16x64*(4x4^1)_NN_d
Tuning for shape 16x64*(4x4^2)_NN_d
Tuning for shape 16x64*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^3  &  1.000 & 1.000 & 0.044 & 22.895
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x to produce Y[64, 64]
Matmul: 64 x 64 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x64*(4x4^1)_NN_d
Tuning for shape 64x64*(4x4^2)_NN_d
Tuning for shape 64x64*(4x4^3)_NN_d
Tuning for shape 64x64*(4x4^1)_NN_d
Tuning for shape 64x64*(4x4^2)_NN_d
Tuning for shape 64x64*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^3  &  1.000 & 1.000 & 0.175 & 5.710
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x to produce Y[256, 64]
Matmul: 256 x 64 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x64*(4x4^1)_NN_d
Tuning for shape 256x64*(4x4^2)_NN_d
Tuning for shape 256x64*(4x4^3)_NN_d
Tuning for shape 256x64*(4x4^1)_NN_d
Tuning for shape 256x64*(4x4^2)_NN_d
Tuning for shape 256x64*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^3  &  1.000 & 1.000 & 0.732 & 1.366
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x64*(4x4^1)_NN_d
Tuning for shape 1024x64*(4x4^2)_NN_d
Tuning for shape 1024x64*(4x4^3)_NN_d
Tuning for shape 1024x64*(4x4^1)_NN_d
Tuning for shape 1024x64*(4x4^2)_NN_d
Tuning for shape 1024x64*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x4^3  &  1.000 & 1.000 & 2.832 & 0.353
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x to produce Y[1, 256]
Matmul: 1 x 256 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(4x4^1)_NN_d
Tuning for shape 1x256*(4x4^2)_NN_d
Tuning for shape 1x256*(4x4^3)_NN_d
Tuning for shape 1x256*(4x4^4)_NN_d
Tuning for shape 1x256*(4x4^1)_NN_d
Tuning for shape 1x256*(4x4^2)_NN_d
Tuning for shape 1x256*(4x4^3)_NN_d
Tuning for shape 1x256*(4x4^1)_NN_d
Tuning for shape 1x256*(4x4^2)_NN_d
Tuning for shape 1x256*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^4  &  1.000 & 1.000 & 0.013 & 75.980
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x to produce Y[4, 256]
Matmul: 4 x 256 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(4x4^1)_NN_d
Tuning for shape 4x256*(4x4^2)_NN_d
Tuning for shape 4x256*(4x4^3)_NN_d
Tuning for shape 4x256*(4x4^4)_NN_d
Tuning for shape 4x256*(4x4^1)_NN_d
Tuning for shape 4x256*(4x4^2)_NN_d
Tuning for shape 4x256*(4x4^3)_NN_d
Tuning for shape 4x256*(4x4^1)_NN_d
Tuning for shape 4x256*(4x4^2)_NN_d
Tuning for shape 4x256*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^4  &  1.000 & 1.000 & 0.041 & 24.647
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x to produce Y[16, 256]
Matmul: 16 x 256 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(4x4^1)_NN_d
Tuning for shape 16x256*(4x4^2)_NN_d
Tuning for shape 16x256*(4x4^3)_NN_d
Tuning for shape 16x256*(4x4^4)_NN_d
Tuning for shape 16x256*(4x4^1)_NN_d
Tuning for shape 16x256*(4x4^2)_NN_d
Tuning for shape 16x256*(4x4^3)_NN_d
Tuning for shape 16x256*(4x4^1)_NN_d
Tuning for shape 16x256*(4x4^2)_NN_d
Tuning for shape 16x256*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^4  &  1.000 & 1.000 & 0.200 & 4.990
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x to produce Y[64, 256]
Matmul: 64 x 256 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(4x4^1)_NN_d
Tuning for shape 64x256*(4x4^2)_NN_d
Tuning for shape 64x256*(4x4^3)_NN_d
Tuning for shape 64x256*(4x4^4)_NN_d
Tuning for shape 64x256*(4x4^1)_NN_d
Tuning for shape 64x256*(4x4^2)_NN_d
Tuning for shape 64x256*(4x4^3)_NN_d
Tuning for shape 64x256*(4x4^1)_NN_d
Tuning for shape 64x256*(4x4^2)_NN_d
Tuning for shape 64x256*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^4  &  1.000 & 1.000 & 0.817 & 1.225
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x to produce Y[256, 256]
Matmul: 256 x 256 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(4x4^1)_NN_d
Tuning for shape 256x256*(4x4^2)_NN_d
Tuning for shape 256x256*(4x4^3)_NN_d
Tuning for shape 256x256*(4x4^4)_NN_d
Tuning for shape 256x256*(4x4^1)_NN_d
Tuning for shape 256x256*(4x4^2)_NN_d
Tuning for shape 256x256*(4x4^3)_NN_d
Tuning for shape 256x256*(4x4^1)_NN_d
Tuning for shape 256x256*(4x4^2)_NN_d
Tuning for shape 256x256*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^4  &  1.000 & 1.000 & 3.139 & 0.319
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 256] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x to produce Y[1024, 256]
Matmul: 1024 x 256 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x256*(4x4^1)_NN_d
Tuning for shape 1024x256*(4x4^2)_NN_d
Tuning for shape 1024x256*(4x4^3)_NN_d
Tuning for shape 1024x256*(4x4^4)_NN_d
Tuning for shape 1024x256*(4x4^1)_NN_d
Tuning for shape 1024x256*(4x4^2)_NN_d
Tuning for shape 1024x256*(4x4^3)_NN_d
Tuning for shape 1024x256*(4x4^1)_NN_d
Tuning for shape 1024x256*(4x4^2)_NN_d
Tuning for shape 1024x256*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x4^4  &  1.000 & 1.000 & 12.842 & 0.078
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1024] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x to produce Y[1, 1024]
Matmul: 1 x 1024 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(4x4^1)_NN_d
Tuning for shape 1x1024*(4x4^2)_NN_d
Tuning for shape 1x1024*(4x4^3)_NN_d
Tuning for shape 1x1024*(4x4^4)_NN_d
Tuning for shape 1x1024*(4x4^5)_NN_d
Tuning for shape 1x1024*(4x4^1)_NN_d
Tuning for shape 1x1024*(4x4^2)_NN_d
Tuning for shape 1x1024*(4x4^3)_NN_d
Tuning for shape 1x1024*(4x4^4)_NN_d
Tuning for shape 1x1024*(4x4^1)_NN_d
Tuning for shape 1x1024*(4x4^2)_NN_d
Tuning for shape 1x1024*(4x4^3)_NN_d
Tuning for shape 1x1024*(4x4^1)_NN_d
Tuning for shape 1x1024*(4x4^2)_NN_d
Tuning for shape 1x1024*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^5  &  1.000 & 1.000 & 0.057 & 17.647
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1024] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x to produce Y[4, 1024]
Matmul: 4 x 1024 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1024*(4x4^1)_NN_d
Tuning for shape 4x1024*(4x4^2)_NN_d
Tuning for shape 4x1024*(4x4^3)_NN_d
Tuning for shape 4x1024*(4x4^4)_NN_d
Tuning for shape 4x1024*(4x4^5)_NN_d
Tuning for shape 4x1024*(4x4^1)_NN_d
Tuning for shape 4x1024*(4x4^2)_NN_d
Tuning for shape 4x1024*(4x4^3)_NN_d
Tuning for shape 4x1024*(4x4^4)_NN_d
Tuning for shape 4x1024*(4x4^1)_NN_d
Tuning for shape 4x1024*(4x4^2)_NN_d
Tuning for shape 4x1024*(4x4^3)_NN_d
Tuning for shape 4x1024*(4x4^1)_NN_d
Tuning for shape 4x1024*(4x4^2)_NN_d
Tuning for shape 4x1024*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^5  &  1.000 & 1.000 & 0.217 & 4.615
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1024] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x to produce Y[16, 1024]
Matmul: 16 x 1024 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1024*(4x4^1)_NN_d
Tuning for shape 16x1024*(4x4^2)_NN_d
Tuning for shape 16x1024*(4x4^3)_NN_d
Tuning for shape 16x1024*(4x4^4)_NN_d
Tuning for shape 16x1024*(4x4^5)_NN_d
Tuning for shape 16x1024*(4x4^1)_NN_d
Tuning for shape 16x1024*(4x4^2)_NN_d
Tuning for shape 16x1024*(4x4^3)_NN_d
Tuning for shape 16x1024*(4x4^4)_NN_d
Tuning for shape 16x1024*(4x4^1)_NN_d
Tuning for shape 16x1024*(4x4^2)_NN_d
Tuning for shape 16x1024*(4x4^3)_NN_d
Tuning for shape 16x1024*(4x4^1)_NN_d
Tuning for shape 16x1024*(4x4^2)_NN_d
Tuning for shape 16x1024*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^5  &  1.000 & 1.000 & 0.857 & 1.167
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1024] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x to produce Y[64, 1024]
Matmul: 64 x 1024 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1024*(4x4^1)_NN_d
Tuning for shape 64x1024*(4x4^2)_NN_d
Tuning for shape 64x1024*(4x4^3)_NN_d
Tuning for shape 64x1024*(4x4^4)_NN_d
Tuning for shape 64x1024*(4x4^5)_NN_d
Tuning for shape 64x1024*(4x4^1)_NN_d
Tuning for shape 64x1024*(4x4^2)_NN_d
Tuning for shape 64x1024*(4x4^3)_NN_d
Tuning for shape 64x1024*(4x4^4)_NN_d
Tuning for shape 64x1024*(4x4^1)_NN_d
Tuning for shape 64x1024*(4x4^2)_NN_d
Tuning for shape 64x1024*(4x4^3)_NN_d
Tuning for shape 64x1024*(4x4^1)_NN_d
Tuning for shape 64x1024*(4x4^2)_NN_d
Tuning for shape 64x1024*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^5  &  1.000 & 1.000 & 3.422 & 0.292
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1024] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x to produce Y[256, 1024]
Matmul: 256 x 1024 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1024*(4x4^1)_NN_d
Tuning for shape 256x1024*(4x4^2)_NN_d
Tuning for shape 256x1024*(4x4^3)_NN_d
Tuning for shape 256x1024*(4x4^4)_NN_d
Tuning for shape 256x1024*(4x4^5)_NN_d
Tuning for shape 256x1024*(4x4^1)_NN_d
Tuning for shape 256x1024*(4x4^2)_NN_d
Tuning for shape 256x1024*(4x4^3)_NN_d
Tuning for shape 256x1024*(4x4^4)_NN_d
Tuning for shape 256x1024*(4x4^1)_NN_d
Tuning for shape 256x1024*(4x4^2)_NN_d
Tuning for shape 256x1024*(4x4^3)_NN_d
Tuning for shape 256x1024*(4x4^1)_NN_d
Tuning for shape 256x1024*(4x4^2)_NN_d
Tuning for shape 256x1024*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^5  &  1.000 & 1.000 & 13.497 & 0.074
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 5 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 1024] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x to produce Y[1024, 1024]
Matmul: 1024 x 1024 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x1024*(4x4^1)_NN_d
Tuning for shape 1024x1024*(4x4^2)_NN_d
Tuning for shape 1024x1024*(4x4^3)_NN_d
Tuning for shape 1024x1024*(4x4^4)_NN_d
Tuning for shape 1024x1024*(4x4^5)_NN_d
Tuning for shape 1024x1024*(4x4^1)_NN_d
Tuning for shape 1024x1024*(4x4^2)_NN_d
Tuning for shape 1024x1024*(4x4^3)_NN_d
Tuning for shape 1024x1024*(4x4^4)_NN_d
Tuning for shape 1024x1024*(4x4^1)_NN_d
Tuning for shape 1024x1024*(4x4^2)_NN_d
Tuning for shape 1024x1024*(4x4^3)_NN_d
Tuning for shape 1024x1024*(4x4^1)_NN_d
Tuning for shape 1024x1024*(4x4^2)_NN_d
Tuning for shape 1024x1024*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x4^5  &  1.000 & 1.000 & 53.342 & 0.019
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4096*(4x4^1)_NN_d
Tuning for shape 1x4096*(4x4^2)_NN_d
Tuning for shape 1x4096*(4x4^3)_NN_d
Tuning for shape 1x4096*(4x4^4)_NN_d
Tuning for shape 1x4096*(4x4^5)_NN_d
Tuning for shape 1x4096*(4x4^6)_NN_d
Tuning for shape 1x4096*(4x4^1)_NN_d
Tuning for shape 1x4096*(4x4^2)_NN_d
Tuning for shape 1x4096*(4x4^3)_NN_d
Tuning for shape 1x4096*(4x4^4)_NN_d
Tuning for shape 1x4096*(4x4^5)_NN_d
Tuning for shape 1x4096*(4x4^1)_NN_d
Tuning for shape 1x4096*(4x4^2)_NN_d
Tuning for shape 1x4096*(4x4^3)_NN_d
Tuning for shape 1x4096*(4x4^4)_NN_d
Tuning for shape 1x4096*(4x4^1)_NN_d
Tuning for shape 1x4096*(4x4^2)_NN_d
Tuning for shape 1x4096*(4x4^3)_NN_d
Tuning for shape 1x4096*(4x4^1)_NN_d
Tuning for shape 1x4096*(4x4^2)_NN_d
Tuning for shape 1x4096*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^6  &  1.000 & 1.000 & 0.250 & 4.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4096*(4x4^1)_NN_d
Tuning for shape 4x4096*(4x4^2)_NN_d
Tuning for shape 4x4096*(4x4^3)_NN_d
Tuning for shape 4x4096*(4x4^4)_NN_d
Tuning for shape 4x4096*(4x4^5)_NN_d
Tuning for shape 4x4096*(4x4^6)_NN_d
Tuning for shape 4x4096*(4x4^1)_NN_d
Tuning for shape 4x4096*(4x4^2)_NN_d
Tuning for shape 4x4096*(4x4^3)_NN_d
Tuning for shape 4x4096*(4x4^4)_NN_d
Tuning for shape 4x4096*(4x4^5)_NN_d
Tuning for shape 4x4096*(4x4^1)_NN_d
Tuning for shape 4x4096*(4x4^2)_NN_d
Tuning for shape 4x4096*(4x4^3)_NN_d
Tuning for shape 4x4096*(4x4^4)_NN_d
Tuning for shape 4x4096*(4x4^1)_NN_d
Tuning for shape 4x4096*(4x4^2)_NN_d
Tuning for shape 4x4096*(4x4^3)_NN_d
Tuning for shape 4x4096*(4x4^1)_NN_d
Tuning for shape 4x4096*(4x4^2)_NN_d
Tuning for shape 4x4096*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^6  &  1.000 & 1.000 & 0.940 & 1.064
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4096*(4x4^1)_NN_d
Tuning for shape 16x4096*(4x4^2)_NN_d
Tuning for shape 16x4096*(4x4^3)_NN_d
Tuning for shape 16x4096*(4x4^4)_NN_d
Tuning for shape 16x4096*(4x4^5)_NN_d
Tuning for shape 16x4096*(4x4^6)_NN_d
Tuning for shape 16x4096*(4x4^1)_NN_d
Tuning for shape 16x4096*(4x4^2)_NN_d
Tuning for shape 16x4096*(4x4^3)_NN_d
Tuning for shape 16x4096*(4x4^4)_NN_d
Tuning for shape 16x4096*(4x4^5)_NN_d
Tuning for shape 16x4096*(4x4^1)_NN_d
Tuning for shape 16x4096*(4x4^2)_NN_d
Tuning for shape 16x4096*(4x4^3)_NN_d
Tuning for shape 16x4096*(4x4^4)_NN_d
Tuning for shape 16x4096*(4x4^1)_NN_d
Tuning for shape 16x4096*(4x4^2)_NN_d
Tuning for shape 16x4096*(4x4^3)_NN_d
Tuning for shape 16x4096*(4x4^1)_NN_d
Tuning for shape 16x4096*(4x4^2)_NN_d
Tuning for shape 16x4096*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^6  &  1.000 & 1.000 & 3.685 & 0.271
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 6 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4096*(4x4^1)_NN_d
Tuning for shape 64x4096*(4x4^2)_NN_d
Tuning for shape 64x4096*(4x4^3)_NN_d
Tuning for shape 64x4096*(4x4^4)_NN_d
Tuning for shape 64x4096*(4x4^5)_NN_d
Tuning for shape 64x4096*(4x4^6)_NN_d
Tuning for shape 64x4096*(4x4^1)_NN_d
Tuning for shape 64x4096*(4x4^2)_NN_d
Tuning for shape 64x4096*(4x4^3)_NN_d
Tuning for shape 64x4096*(4x4^4)_NN_d
Tuning for shape 64x4096*(4x4^5)_NN_d
Tuning for shape 64x4096*(4x4^1)_NN_d
Tuning for shape 64x4096*(4x4^2)_NN_d
Tuning for shape 64x4096*(4x4^3)_NN_d
Tuning for shape 64x4096*(4x4^4)_NN_d
Tuning for shape 64x4096*(4x4^1)_NN_d
Tuning for shape 64x4096*(4x4^2)_NN_d
Tuning for shape 64x4096*(4x4^3)_NN_d
Tuning for shape 64x4096*(4x4^1)_NN_d
Tuning for shape 64x4096*(4x4^2)_NN_d
Tuning for shape 64x4096*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^6  &  1.000 & 1.000 & 14.545 & 0.069
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 6 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4096*(4x4^1)_NN_d
Tuning for shape 256x4096*(4x4^2)_NN_d
Tuning for shape 256x4096*(4x4^3)_NN_d
Tuning for shape 256x4096*(4x4^4)_NN_d
Tuning for shape 256x4096*(4x4^5)_NN_d
Tuning for shape 256x4096*(4x4^6)_NN_d
Tuning for shape 256x4096*(4x4^1)_NN_d
Tuning for shape 256x4096*(4x4^2)_NN_d
Tuning for shape 256x4096*(4x4^3)_NN_d
Tuning for shape 256x4096*(4x4^4)_NN_d
Tuning for shape 256x4096*(4x4^5)_NN_d
Tuning for shape 256x4096*(4x4^1)_NN_d
Tuning for shape 256x4096*(4x4^2)_NN_d
Tuning for shape 256x4096*(4x4^3)_NN_d
Tuning for shape 256x4096*(4x4^4)_NN_d
Tuning for shape 256x4096*(4x4^1)_NN_d
Tuning for shape 256x4096*(4x4^2)_NN_d
Tuning for shape 256x4096*(4x4^3)_NN_d
Tuning for shape 256x4096*(4x4^1)_NN_d
Tuning for shape 256x4096*(4x4^2)_NN_d
Tuning for shape 256x4096*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^6  &  1.000 & 1.000 & 57.973 & 0.017
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 6 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4096*(4x4^1)_NN_d
Tuning for shape 1024x4096*(4x4^2)_NN_d
Tuning for shape 1024x4096*(4x4^3)_NN_d
Tuning for shape 1024x4096*(4x4^4)_NN_d
Tuning for shape 1024x4096*(4x4^5)_NN_d
Tuning for shape 1024x4096*(4x4^6)_NN_d
Tuning for shape 1024x4096*(4x4^1)_NN_d
Tuning for shape 1024x4096*(4x4^2)_NN_d
Tuning for shape 1024x4096*(4x4^3)_NN_d
Tuning for shape 1024x4096*(4x4^4)_NN_d
Tuning for shape 1024x4096*(4x4^5)_NN_d
Tuning for shape 1024x4096*(4x4^1)_NN_d
Tuning for shape 1024x4096*(4x4^2)_NN_d
Tuning for shape 1024x4096*(4x4^3)_NN_d
Tuning for shape 1024x4096*(4x4^4)_NN_d
Tuning for shape 1024x4096*(4x4^1)_NN_d
Tuning for shape 1024x4096*(4x4^2)_NN_d
Tuning for shape 1024x4096*(4x4^3)_NN_d
Tuning for shape 1024x4096*(4x4^1)_NN_d
Tuning for shape 1024x4096*(4x4^2)_NN_d
Tuning for shape 1024x4096*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x4^6  &  1.000 & 1.000 & 62.926 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16384] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x to produce Y[1, 16384]
Matmul: 1 x 16384 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16384*(4x4^1)_NN_d
Tuning for shape 1x16384*(4x4^2)_NN_d
Tuning for shape 1x16384*(4x4^3)_NN_d
Tuning for shape 1x16384*(4x4^4)_NN_d
Tuning for shape 1x16384*(4x4^5)_NN_d
Tuning for shape 1x16384*(4x4^6)_NN_d
Tuning for shape 1x16384*(4x4^7)_NN_d
Tuning for shape 1x16384*(4x4^1)_NN_d
Tuning for shape 1x16384*(4x4^2)_NN_d
Tuning for shape 1x16384*(4x4^3)_NN_d
Tuning for shape 1x16384*(4x4^4)_NN_d
Tuning for shape 1x16384*(4x4^5)_NN_d
Tuning for shape 1x16384*(4x4^6)_NN_d
Tuning for shape 1x16384*(4x4^1)_NN_d
Tuning for shape 1x16384*(4x4^2)_NN_d
Tuning for shape 1x16384*(4x4^3)_NN_d
Tuning for shape 1x16384*(4x4^4)_NN_d
Tuning for shape 1x16384*(4x4^5)_NN_d
Tuning for shape 1x16384*(4x4^1)_NN_d
Tuning for shape 1x16384*(4x4^2)_NN_d
Tuning for shape 1x16384*(4x4^3)_NN_d
Tuning for shape 1x16384*(4x4^4)_NN_d
Tuning for shape 1x16384*(4x4^1)_NN_d
Tuning for shape 1x16384*(4x4^2)_NN_d
Tuning for shape 1x16384*(4x4^3)_NN_d
Tuning for shape 1x16384*(4x4^1)_NN_d
Tuning for shape 1x16384*(4x4^2)_NN_d
Tuning for shape 1x16384*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^7  &  1.000 & 1.000 & 1.014 & 0.987
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 7 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16384] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x to produce Y[4, 16384]
Matmul: 4 x 16384 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16384*(4x4^1)_NN_d
Tuning for shape 4x16384*(4x4^2)_NN_d
Tuning for shape 4x16384*(4x4^3)_NN_d
Tuning for shape 4x16384*(4x4^4)_NN_d
Tuning for shape 4x16384*(4x4^5)_NN_d
Tuning for shape 4x16384*(4x4^6)_NN_d
Tuning for shape 4x16384*(4x4^7)_NN_d
Tuning for shape 4x16384*(4x4^1)_NN_d
Tuning for shape 4x16384*(4x4^2)_NN_d
Tuning for shape 4x16384*(4x4^3)_NN_d
Tuning for shape 4x16384*(4x4^4)_NN_d
Tuning for shape 4x16384*(4x4^5)_NN_d
Tuning for shape 4x16384*(4x4^6)_NN_d
Tuning for shape 4x16384*(4x4^1)_NN_d
Tuning for shape 4x16384*(4x4^2)_NN_d
Tuning for shape 4x16384*(4x4^3)_NN_d
Tuning for shape 4x16384*(4x4^4)_NN_d
Tuning for shape 4x16384*(4x4^5)_NN_d
Tuning for shape 4x16384*(4x4^1)_NN_d
Tuning for shape 4x16384*(4x4^2)_NN_d
Tuning for shape 4x16384*(4x4^3)_NN_d
Tuning for shape 4x16384*(4x4^4)_NN_d
Tuning for shape 4x16384*(4x4^1)_NN_d
Tuning for shape 4x16384*(4x4^2)_NN_d
Tuning for shape 4x16384*(4x4^3)_NN_d
Tuning for shape 4x16384*(4x4^1)_NN_d
Tuning for shape 4x16384*(4x4^2)_NN_d
Tuning for shape 4x16384*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^7  &  1.000 & 1.000 & 3.852 & 0.260
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 7 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16384] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x to produce Y[16, 16384]
Matmul: 16 x 16384 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16384*(4x4^1)_NN_d
Tuning for shape 16x16384*(4x4^2)_NN_d
Tuning for shape 16x16384*(4x4^3)_NN_d
Tuning for shape 16x16384*(4x4^4)_NN_d
Tuning for shape 16x16384*(4x4^5)_NN_d
Tuning for shape 16x16384*(4x4^6)_NN_d
Tuning for shape 16x16384*(4x4^7)_NN_d
Tuning for shape 16x16384*(4x4^1)_NN_d
Tuning for shape 16x16384*(4x4^2)_NN_d
Tuning for shape 16x16384*(4x4^3)_NN_d
Tuning for shape 16x16384*(4x4^4)_NN_d
Tuning for shape 16x16384*(4x4^5)_NN_d
Tuning for shape 16x16384*(4x4^6)_NN_d
Tuning for shape 16x16384*(4x4^1)_NN_d
Tuning for shape 16x16384*(4x4^2)_NN_d
Tuning for shape 16x16384*(4x4^3)_NN_d
Tuning for shape 16x16384*(4x4^4)_NN_d
Tuning for shape 16x16384*(4x4^5)_NN_d
Tuning for shape 16x16384*(4x4^1)_NN_d
Tuning for shape 16x16384*(4x4^2)_NN_d
Tuning for shape 16x16384*(4x4^3)_NN_d
Tuning for shape 16x16384*(4x4^4)_NN_d
Tuning for shape 16x16384*(4x4^1)_NN_d
Tuning for shape 16x16384*(4x4^2)_NN_d
Tuning for shape 16x16384*(4x4^3)_NN_d
Tuning for shape 16x16384*(4x4^1)_NN_d
Tuning for shape 16x16384*(4x4^2)_NN_d
Tuning for shape 16x16384*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^7  &  1.000 & 1.000 & 15.829 & 0.063
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 7 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16384] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x to produce Y[64, 16384]
Matmul: 64 x 16384 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16384*(4x4^1)_NN_d
Tuning for shape 64x16384*(4x4^2)_NN_d
Tuning for shape 64x16384*(4x4^3)_NN_d
Tuning for shape 64x16384*(4x4^4)_NN_d
Tuning for shape 64x16384*(4x4^5)_NN_d
Tuning for shape 64x16384*(4x4^6)_NN_d
Tuning for shape 64x16384*(4x4^7)_NN_d
Tuning for shape 64x16384*(4x4^1)_NN_d
Tuning for shape 64x16384*(4x4^2)_NN_d
Tuning for shape 64x16384*(4x4^3)_NN_d
Tuning for shape 64x16384*(4x4^4)_NN_d
Tuning for shape 64x16384*(4x4^5)_NN_d
Tuning for shape 64x16384*(4x4^6)_NN_d
Tuning for shape 64x16384*(4x4^1)_NN_d
Tuning for shape 64x16384*(4x4^2)_NN_d
Tuning for shape 64x16384*(4x4^3)_NN_d
Tuning for shape 64x16384*(4x4^4)_NN_d
Tuning for shape 64x16384*(4x4^5)_NN_d
Tuning for shape 64x16384*(4x4^1)_NN_d
Tuning for shape 64x16384*(4x4^2)_NN_d
Tuning for shape 64x16384*(4x4^3)_NN_d
Tuning for shape 64x16384*(4x4^4)_NN_d
Tuning for shape 64x16384*(4x4^1)_NN_d
Tuning for shape 64x16384*(4x4^2)_NN_d
Tuning for shape 64x16384*(4x4^3)_NN_d
Tuning for shape 64x16384*(4x4^1)_NN_d
Tuning for shape 64x16384*(4x4^2)_NN_d
Tuning for shape 64x16384*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^7  &  1.000 & 1.000 & 60.974 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 7 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16384] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x to produce Y[256, 16384]
Matmul: 256 x 16384 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16384*(4x4^1)_NN_d
Tuning for shape 256x16384*(4x4^2)_NN_d
Tuning for shape 256x16384*(4x4^3)_NN_d
Tuning for shape 256x16384*(4x4^4)_NN_d
Tuning for shape 256x16384*(4x4^5)_NN_d
Tuning for shape 256x16384*(4x4^6)_NN_d
Tuning for shape 256x16384*(4x4^7)_NN_d
Tuning for shape 256x16384*(4x4^1)_NN_d
Tuning for shape 256x16384*(4x4^2)_NN_d
Tuning for shape 256x16384*(4x4^3)_NN_d
Tuning for shape 256x16384*(4x4^4)_NN_d
Tuning for shape 256x16384*(4x4^5)_NN_d
Tuning for shape 256x16384*(4x4^6)_NN_d
Tuning for shape 256x16384*(4x4^1)_NN_d
Tuning for shape 256x16384*(4x4^2)_NN_d
Tuning for shape 256x16384*(4x4^3)_NN_d
Tuning for shape 256x16384*(4x4^4)_NN_d
Tuning for shape 256x16384*(4x4^5)_NN_d
Tuning for shape 256x16384*(4x4^1)_NN_d
Tuning for shape 256x16384*(4x4^2)_NN_d
Tuning for shape 256x16384*(4x4^3)_NN_d
Tuning for shape 256x16384*(4x4^4)_NN_d
Tuning for shape 256x16384*(4x4^1)_NN_d
Tuning for shape 256x16384*(4x4^2)_NN_d
Tuning for shape 256x16384*(4x4^3)_NN_d
Tuning for shape 256x16384*(4x4^1)_NN_d
Tuning for shape 256x16384*(4x4^2)_NN_d
Tuning for shape 256x16384*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^7  &  1.000 & 1.000 & 67.982 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 7 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16384] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x to produce Y[1024, 16384]
Matmul: 1024 x 16384 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16384*(4x4^1)_NN_d
Tuning for shape 1024x16384*(4x4^2)_NN_d
Tuning for shape 1024x16384*(4x4^3)_NN_d
Tuning for shape 1024x16384*(4x4^4)_NN_d
Tuning for shape 1024x16384*(4x4^5)_NN_d
Tuning for shape 1024x16384*(4x4^6)_NN_d
Tuning for shape 1024x16384*(4x4^7)_NN_d
Tuning for shape 1024x16384*(4x4^1)_NN_d
Tuning for shape 1024x16384*(4x4^2)_NN_d
Tuning for shape 1024x16384*(4x4^3)_NN_d
Tuning for shape 1024x16384*(4x4^4)_NN_d
Tuning for shape 1024x16384*(4x4^5)_NN_d
Tuning for shape 1024x16384*(4x4^6)_NN_d
Tuning for shape 1024x16384*(4x4^1)_NN_d
Tuning for shape 1024x16384*(4x4^2)_NN_d
Tuning for shape 1024x16384*(4x4^3)_NN_d
Tuning for shape 1024x16384*(4x4^4)_NN_d
Tuning for shape 1024x16384*(4x4^5)_NN_d
Tuning for shape 1024x16384*(4x4^1)_NN_d
Tuning for shape 1024x16384*(4x4^2)_NN_d
Tuning for shape 1024x16384*(4x4^3)_NN_d
Tuning for shape 1024x16384*(4x4^4)_NN_d
Tuning for shape 1024x16384*(4x4^1)_NN_d
Tuning for shape 1024x16384*(4x4^2)_NN_d
Tuning for shape 1024x16384*(4x4^3)_NN_d
Tuning for shape 1024x16384*(4x4^1)_NN_d
Tuning for shape 1024x16384*(4x4^2)_NN_d
Tuning for shape 1024x16384*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x4^7  &  1.000 & 1.000 & 69.566 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 8 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 65536] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x to produce Y[1, 65536]
Matmul: 1 x 65536 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x65536*(4x4^1)_NN_d
Tuning for shape 1x65536*(4x4^2)_NN_d
Tuning for shape 1x65536*(4x4^3)_NN_d
Tuning for shape 1x65536*(4x4^4)_NN_d
Tuning for shape 1x65536*(4x4^5)_NN_d
Tuning for shape 1x65536*(4x4^6)_NN_d
Tuning for shape 1x65536*(4x4^7)_NN_d
Tuning for shape 1x65536*(4x4^8)_NN_d
Tuning for shape 1x65536*(4x4^1)_NN_d
Tuning for shape 1x65536*(4x4^2)_NN_d
Tuning for shape 1x65536*(4x4^3)_NN_d
Tuning for shape 1x65536*(4x4^4)_NN_d
Tuning for shape 1x65536*(4x4^5)_NN_d
Tuning for shape 1x65536*(4x4^6)_NN_d
Tuning for shape 1x65536*(4x4^7)_NN_d
Tuning for shape 1x65536*(4x4^1)_NN_d
Tuning for shape 1x65536*(4x4^2)_NN_d
Tuning for shape 1x65536*(4x4^3)_NN_d
Tuning for shape 1x65536*(4x4^4)_NN_d
Tuning for shape 1x65536*(4x4^5)_NN_d
Tuning for shape 1x65536*(4x4^6)_NN_d
Tuning for shape 1x65536*(4x4^1)_NN_d
Tuning for shape 1x65536*(4x4^2)_NN_d
Tuning for shape 1x65536*(4x4^3)_NN_d
Tuning for shape 1x65536*(4x4^4)_NN_d
Tuning for shape 1x65536*(4x4^5)_NN_d
Tuning for shape 1x65536*(4x4^1)_NN_d
Tuning for shape 1x65536*(4x4^2)_NN_d
Tuning for shape 1x65536*(4x4^3)_NN_d
Tuning for shape 1x65536*(4x4^4)_NN_d
Tuning for shape 1x65536*(4x4^1)_NN_d
Tuning for shape 1x65536*(4x4^2)_NN_d
Tuning for shape 1x65536*(4x4^3)_NN_d
Tuning for shape 1x65536*(4x4^1)_NN_d
Tuning for shape 1x65536*(4x4^2)_NN_d
Tuning for shape 1x65536*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^8  &  1.000 & 1.000 & 4.279 & 0.234
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 8 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 65536] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x to produce Y[4, 65536]
Matmul: 4 x 65536 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x65536*(4x4^1)_NN_d
Tuning for shape 4x65536*(4x4^2)_NN_d
Tuning for shape 4x65536*(4x4^3)_NN_d
Tuning for shape 4x65536*(4x4^4)_NN_d
Tuning for shape 4x65536*(4x4^5)_NN_d
Tuning for shape 4x65536*(4x4^6)_NN_d
Tuning for shape 4x65536*(4x4^7)_NN_d
Tuning for shape 4x65536*(4x4^8)_NN_d
Tuning for shape 4x65536*(4x4^1)_NN_d
Tuning for shape 4x65536*(4x4^2)_NN_d
Tuning for shape 4x65536*(4x4^3)_NN_d
Tuning for shape 4x65536*(4x4^4)_NN_d
Tuning for shape 4x65536*(4x4^5)_NN_d
Tuning for shape 4x65536*(4x4^6)_NN_d
Tuning for shape 4x65536*(4x4^7)_NN_d
Tuning for shape 4x65536*(4x4^1)_NN_d
Tuning for shape 4x65536*(4x4^2)_NN_d
Tuning for shape 4x65536*(4x4^3)_NN_d
Tuning for shape 4x65536*(4x4^4)_NN_d
Tuning for shape 4x65536*(4x4^5)_NN_d
Tuning for shape 4x65536*(4x4^6)_NN_d
Tuning for shape 4x65536*(4x4^1)_NN_d
Tuning for shape 4x65536*(4x4^2)_NN_d
Tuning for shape 4x65536*(4x4^3)_NN_d
Tuning for shape 4x65536*(4x4^4)_NN_d
Tuning for shape 4x65536*(4x4^5)_NN_d
Tuning for shape 4x65536*(4x4^1)_NN_d
Tuning for shape 4x65536*(4x4^2)_NN_d
Tuning for shape 4x65536*(4x4^3)_NN_d
Tuning for shape 4x65536*(4x4^4)_NN_d
Tuning for shape 4x65536*(4x4^1)_NN_d
Tuning for shape 4x65536*(4x4^2)_NN_d
Tuning for shape 4x65536*(4x4^3)_NN_d
Tuning for shape 4x65536*(4x4^1)_NN_d
Tuning for shape 4x65536*(4x4^2)_NN_d
Tuning for shape 4x65536*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^8  &  1.000 & 1.000 & 16.134 & 0.062
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 8 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 65536] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x to produce Y[16, 65536]
Matmul: 16 x 65536 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x65536*(4x4^1)_NN_d
Tuning for shape 16x65536*(4x4^2)_NN_d
Tuning for shape 16x65536*(4x4^3)_NN_d
Tuning for shape 16x65536*(4x4^4)_NN_d
Tuning for shape 16x65536*(4x4^5)_NN_d
Tuning for shape 16x65536*(4x4^6)_NN_d
Tuning for shape 16x65536*(4x4^7)_NN_d
Tuning for shape 16x65536*(4x4^8)_NN_d
Tuning for shape 16x65536*(4x4^1)_NN_d
Tuning for shape 16x65536*(4x4^2)_NN_d
Tuning for shape 16x65536*(4x4^3)_NN_d
Tuning for shape 16x65536*(4x4^4)_NN_d
Tuning for shape 16x65536*(4x4^5)_NN_d
Tuning for shape 16x65536*(4x4^6)_NN_d
Tuning for shape 16x65536*(4x4^7)_NN_d
Tuning for shape 16x65536*(4x4^1)_NN_d
Tuning for shape 16x65536*(4x4^2)_NN_d
Tuning for shape 16x65536*(4x4^3)_NN_d
Tuning for shape 16x65536*(4x4^4)_NN_d
Tuning for shape 16x65536*(4x4^5)_NN_d
Tuning for shape 16x65536*(4x4^6)_NN_d
Tuning for shape 16x65536*(4x4^1)_NN_d
Tuning for shape 16x65536*(4x4^2)_NN_d
Tuning for shape 16x65536*(4x4^3)_NN_d
Tuning for shape 16x65536*(4x4^4)_NN_d
Tuning for shape 16x65536*(4x4^5)_NN_d
Tuning for shape 16x65536*(4x4^1)_NN_d
Tuning for shape 16x65536*(4x4^2)_NN_d
Tuning for shape 16x65536*(4x4^3)_NN_d
Tuning for shape 16x65536*(4x4^4)_NN_d
Tuning for shape 16x65536*(4x4^1)_NN_d
Tuning for shape 16x65536*(4x4^2)_NN_d
Tuning for shape 16x65536*(4x4^3)_NN_d
Tuning for shape 16x65536*(4x4^1)_NN_d
Tuning for shape 16x65536*(4x4^2)_NN_d
Tuning for shape 16x65536*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^8  &  1.000 & 1.000 & 62.821 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 8 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 65536] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x to produce Y[64, 65536]
Matmul: 64 x 65536 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x65536*(4x4^1)_NN_d
Tuning for shape 64x65536*(4x4^2)_NN_d
Tuning for shape 64x65536*(4x4^3)_NN_d
Tuning for shape 64x65536*(4x4^4)_NN_d
Tuning for shape 64x65536*(4x4^5)_NN_d
Tuning for shape 64x65536*(4x4^6)_NN_d
Tuning for shape 64x65536*(4x4^7)_NN_d
Tuning for shape 64x65536*(4x4^8)_NN_d
Tuning for shape 64x65536*(4x4^1)_NN_d
Tuning for shape 64x65536*(4x4^2)_NN_d
Tuning for shape 64x65536*(4x4^3)_NN_d
Tuning for shape 64x65536*(4x4^4)_NN_d
Tuning for shape 64x65536*(4x4^5)_NN_d
Tuning for shape 64x65536*(4x4^6)_NN_d
Tuning for shape 64x65536*(4x4^7)_NN_d
Tuning for shape 64x65536*(4x4^1)_NN_d
Tuning for shape 64x65536*(4x4^2)_NN_d
Tuning for shape 64x65536*(4x4^3)_NN_d
Tuning for shape 64x65536*(4x4^4)_NN_d
Tuning for shape 64x65536*(4x4^5)_NN_d
Tuning for shape 64x65536*(4x4^6)_NN_d
Tuning for shape 64x65536*(4x4^1)_NN_d
Tuning for shape 64x65536*(4x4^2)_NN_d
Tuning for shape 64x65536*(4x4^3)_NN_d
Tuning for shape 64x65536*(4x4^4)_NN_d
Tuning for shape 64x65536*(4x4^5)_NN_d
Tuning for shape 64x65536*(4x4^1)_NN_d
Tuning for shape 64x65536*(4x4^2)_NN_d
Tuning for shape 64x65536*(4x4^3)_NN_d
Tuning for shape 64x65536*(4x4^4)_NN_d
Tuning for shape 64x65536*(4x4^1)_NN_d
Tuning for shape 64x65536*(4x4^2)_NN_d
Tuning for shape 64x65536*(4x4^3)_NN_d
Tuning for shape 64x65536*(4x4^1)_NN_d
Tuning for shape 64x65536*(4x4^2)_NN_d
Tuning for shape 64x65536*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^8  &  1.000 & 1.000 & 68.514 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 8 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 65536] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x to produce Y[256, 65536]
Matmul: 256 x 65536 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x65536*(4x4^1)_NN_d
Tuning for shape 256x65536*(4x4^2)_NN_d
Tuning for shape 256x65536*(4x4^3)_NN_d
Tuning for shape 256x65536*(4x4^4)_NN_d
Tuning for shape 256x65536*(4x4^5)_NN_d
Tuning for shape 256x65536*(4x4^6)_NN_d
Tuning for shape 256x65536*(4x4^7)_NN_d
Tuning for shape 256x65536*(4x4^8)_NN_d
Tuning for shape 256x65536*(4x4^1)_NN_d
Tuning for shape 256x65536*(4x4^2)_NN_d
Tuning for shape 256x65536*(4x4^3)_NN_d
Tuning for shape 256x65536*(4x4^4)_NN_d
Tuning for shape 256x65536*(4x4^5)_NN_d
Tuning for shape 256x65536*(4x4^6)_NN_d
Tuning for shape 256x65536*(4x4^7)_NN_d
Tuning for shape 256x65536*(4x4^1)_NN_d
Tuning for shape 256x65536*(4x4^2)_NN_d
Tuning for shape 256x65536*(4x4^3)_NN_d
Tuning for shape 256x65536*(4x4^4)_NN_d
Tuning for shape 256x65536*(4x4^5)_NN_d
Tuning for shape 256x65536*(4x4^6)_NN_d
Tuning for shape 256x65536*(4x4^1)_NN_d
Tuning for shape 256x65536*(4x4^2)_NN_d
Tuning for shape 256x65536*(4x4^3)_NN_d
Tuning for shape 256x65536*(4x4^4)_NN_d
Tuning for shape 256x65536*(4x4^5)_NN_d
Tuning for shape 256x65536*(4x4^1)_NN_d
Tuning for shape 256x65536*(4x4^2)_NN_d
Tuning for shape 256x65536*(4x4^3)_NN_d
Tuning for shape 256x65536*(4x4^4)_NN_d
Tuning for shape 256x65536*(4x4^1)_NN_d
Tuning for shape 256x65536*(4x4^2)_NN_d
Tuning for shape 256x65536*(4x4^3)_NN_d
Tuning for shape 256x65536*(4x4^1)_NN_d
Tuning for shape 256x65536*(4x4^2)_NN_d
Tuning for shape 256x65536*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^8  &  1.000 & 1.000 & 65.861 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 8 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 65536] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x to produce Y[1024, 65536]
Matmul: 1024 x 65536 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x65536*(4x4^1)_NN_d
Tuning for shape 1024x65536*(4x4^2)_NN_d
Tuning for shape 1024x65536*(4x4^3)_NN_d
Tuning for shape 1024x65536*(4x4^4)_NN_d
Tuning for shape 1024x65536*(4x4^5)_NN_d
Tuning for shape 1024x65536*(4x4^6)_NN_d
Tuning for shape 1024x65536*(4x4^7)_NN_d
Tuning for shape 1024x65536*(4x4^8)_NN_d
Tuning for shape 1024x65536*(4x4^1)_NN_d
Tuning for shape 1024x65536*(4x4^2)_NN_d
Tuning for shape 1024x65536*(4x4^3)_NN_d
Tuning for shape 1024x65536*(4x4^4)_NN_d
Tuning for shape 1024x65536*(4x4^5)_NN_d
Tuning for shape 1024x65536*(4x4^6)_NN_d
Tuning for shape 1024x65536*(4x4^7)_NN_d
Tuning for shape 1024x65536*(4x4^1)_NN_d
Tuning for shape 1024x65536*(4x4^2)_NN_d
Tuning for shape 1024x65536*(4x4^3)_NN_d
Tuning for shape 1024x65536*(4x4^4)_NN_d
Tuning for shape 1024x65536*(4x4^5)_NN_d
Tuning for shape 1024x65536*(4x4^6)_NN_d
Tuning for shape 1024x65536*(4x4^1)_NN_d
Tuning for shape 1024x65536*(4x4^2)_NN_d
Tuning for shape 1024x65536*(4x4^3)_NN_d
Tuning for shape 1024x65536*(4x4^4)_NN_d
Tuning for shape 1024x65536*(4x4^5)_NN_d
Tuning for shape 1024x65536*(4x4^1)_NN_d
Tuning for shape 1024x65536*(4x4^2)_NN_d
Tuning for shape 1024x65536*(4x4^3)_NN_d
Tuning for shape 1024x65536*(4x4^4)_NN_d
Tuning for shape 1024x65536*(4x4^1)_NN_d
Tuning for shape 1024x65536*(4x4^2)_NN_d
Tuning for shape 1024x65536*(4x4^3)_NN_d
Tuning for shape 1024x65536*(4x4^1)_NN_d
Tuning for shape 1024x65536*(4x4^2)_NN_d
Tuning for shape 1024x65536*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x4^8  &  1.000 & 1.000 & 65.689 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 9 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 262144] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x262144*(4x4^1)_NN_d
Tuning for shape 1x262144*(4x4^2)_NN_d
Tuning for shape 1x262144*(4x4^3)_NN_d
Tuning for shape 1x262144*(4x4^4)_NN_d
Tuning for shape 1x262144*(4x4^5)_NN_d
Tuning for shape 1x262144*(4x4^6)_NN_d
Tuning for shape 1x262144*(4x4^7)_NN_d
Tuning for shape 1x262144*(4x4^8)_NN_d
Tuning for shape 1x262144*(4x4^9)_NN_d
Tuning for shape 1x262144*(4x4^1)_NN_d
Tuning for shape 1x262144*(4x4^2)_NN_d
Tuning for shape 1x262144*(4x4^3)_NN_d
Tuning for shape 1x262144*(4x4^4)_NN_d
Tuning for shape 1x262144*(4x4^5)_NN_d
Tuning for shape 1x262144*(4x4^6)_NN_d
Tuning for shape 1x262144*(4x4^7)_NN_d
Tuning for shape 1x262144*(4x4^8)_NN_d
Tuning for shape 1x262144*(4x4^1)_NN_d
Tuning for shape 1x262144*(4x4^2)_NN_d
Tuning for shape 1x262144*(4x4^3)_NN_d
Tuning for shape 1x262144*(4x4^4)_NN_d
Tuning for shape 1x262144*(4x4^5)_NN_d
Tuning for shape 1x262144*(4x4^6)_NN_d
Tuning for shape 1x262144*(4x4^7)_NN_d
Tuning for shape 1x262144*(4x4^1)_NN_d
Tuning for shape 1x262144*(4x4^2)_NN_d
Tuning for shape 1x262144*(4x4^3)_NN_d
Tuning for shape 1x262144*(4x4^4)_NN_d
Tuning for shape 1x262144*(4x4^5)_NN_d
Tuning for shape 1x262144*(4x4^6)_NN_d
Tuning for shape 1x262144*(4x4^1)_NN_d
Tuning for shape 1x262144*(4x4^2)_NN_d
Tuning for shape 1x262144*(4x4^3)_NN_d
Tuning for shape 1x262144*(4x4^4)_NN_d
Tuning for shape 1x262144*(4x4^5)_NN_d
Tuning for shape 1x262144*(4x4^1)_NN_d
Tuning for shape 1x262144*(4x4^2)_NN_d
Tuning for shape 1x262144*(4x4^3)_NN_d
Tuning for shape 1x262144*(4x4^4)_NN_d
Tuning for shape 1x262144*(4x4^1)_NN_d
Tuning for shape 1x262144*(4x4^2)_NN_d
Tuning for shape 1x262144*(4x4^3)_NN_d
Tuning for shape 1x262144*(4x4^1)_NN_d
Tuning for shape 1x262144*(4x4^2)_NN_d
Tuning for shape 1x262144*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^9  &  1.000 & 1.000 & 17.139 & 0.058
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 9 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 262144] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x262144*(4x4^1)_NN_d
Tuning for shape 4x262144*(4x4^2)_NN_d
Tuning for shape 4x262144*(4x4^3)_NN_d
Tuning for shape 4x262144*(4x4^4)_NN_d
Tuning for shape 4x262144*(4x4^5)_NN_d
Tuning for shape 4x262144*(4x4^6)_NN_d
Tuning for shape 4x262144*(4x4^7)_NN_d
Tuning for shape 4x262144*(4x4^8)_NN_d
Tuning for shape 4x262144*(4x4^9)_NN_d
Tuning for shape 4x262144*(4x4^1)_NN_d
Tuning for shape 4x262144*(4x4^2)_NN_d
Tuning for shape 4x262144*(4x4^3)_NN_d
Tuning for shape 4x262144*(4x4^4)_NN_d
Tuning for shape 4x262144*(4x4^5)_NN_d
Tuning for shape 4x262144*(4x4^6)_NN_d
Tuning for shape 4x262144*(4x4^7)_NN_d
Tuning for shape 4x262144*(4x4^8)_NN_d
Tuning for shape 4x262144*(4x4^1)_NN_d
Tuning for shape 4x262144*(4x4^2)_NN_d
Tuning for shape 4x262144*(4x4^3)_NN_d
Tuning for shape 4x262144*(4x4^4)_NN_d
Tuning for shape 4x262144*(4x4^5)_NN_d
Tuning for shape 4x262144*(4x4^6)_NN_d
Tuning for shape 4x262144*(4x4^7)_NN_d
Tuning for shape 4x262144*(4x4^1)_NN_d
Tuning for shape 4x262144*(4x4^2)_NN_d
Tuning for shape 4x262144*(4x4^3)_NN_d
Tuning for shape 4x262144*(4x4^4)_NN_d
Tuning for shape 4x262144*(4x4^5)_NN_d
Tuning for shape 4x262144*(4x4^6)_NN_d
Tuning for shape 4x262144*(4x4^1)_NN_d
Tuning for shape 4x262144*(4x4^2)_NN_d
Tuning for shape 4x262144*(4x4^3)_NN_d
Tuning for shape 4x262144*(4x4^4)_NN_d
Tuning for shape 4x262144*(4x4^5)_NN_d
Tuning for shape 4x262144*(4x4^1)_NN_d
Tuning for shape 4x262144*(4x4^2)_NN_d
Tuning for shape 4x262144*(4x4^3)_NN_d
Tuning for shape 4x262144*(4x4^4)_NN_d
Tuning for shape 4x262144*(4x4^1)_NN_d
Tuning for shape 4x262144*(4x4^2)_NN_d
Tuning for shape 4x262144*(4x4^3)_NN_d
Tuning for shape 4x262144*(4x4^1)_NN_d
Tuning for shape 4x262144*(4x4^2)_NN_d
Tuning for shape 4x262144*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^9  &  1.000 & 1.000 & 64.737 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 9 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 262144] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x262144*(4x4^1)_NN_d
Tuning for shape 16x262144*(4x4^2)_NN_d
Tuning for shape 16x262144*(4x4^3)_NN_d
Tuning for shape 16x262144*(4x4^4)_NN_d
Tuning for shape 16x262144*(4x4^5)_NN_d
Tuning for shape 16x262144*(4x4^6)_NN_d
Tuning for shape 16x262144*(4x4^7)_NN_d
Tuning for shape 16x262144*(4x4^8)_NN_d
Tuning for shape 16x262144*(4x4^9)_NN_d
Tuning for shape 16x262144*(4x4^1)_NN_d
Tuning for shape 16x262144*(4x4^2)_NN_d
Tuning for shape 16x262144*(4x4^3)_NN_d
Tuning for shape 16x262144*(4x4^4)_NN_d
Tuning for shape 16x262144*(4x4^5)_NN_d
Tuning for shape 16x262144*(4x4^6)_NN_d
Tuning for shape 16x262144*(4x4^7)_NN_d
Tuning for shape 16x262144*(4x4^8)_NN_d
Tuning for shape 16x262144*(4x4^1)_NN_d
Tuning for shape 16x262144*(4x4^2)_NN_d
Tuning for shape 16x262144*(4x4^3)_NN_d
Tuning for shape 16x262144*(4x4^4)_NN_d
Tuning for shape 16x262144*(4x4^5)_NN_d
Tuning for shape 16x262144*(4x4^6)_NN_d
Tuning for shape 16x262144*(4x4^7)_NN_d
Tuning for shape 16x262144*(4x4^1)_NN_d
Tuning for shape 16x262144*(4x4^2)_NN_d
Tuning for shape 16x262144*(4x4^3)_NN_d
Tuning for shape 16x262144*(4x4^4)_NN_d
Tuning for shape 16x262144*(4x4^5)_NN_d
Tuning for shape 16x262144*(4x4^6)_NN_d
Tuning for shape 16x262144*(4x4^1)_NN_d
Tuning for shape 16x262144*(4x4^2)_NN_d
Tuning for shape 16x262144*(4x4^3)_NN_d
Tuning for shape 16x262144*(4x4^4)_NN_d
Tuning for shape 16x262144*(4x4^5)_NN_d
Tuning for shape 16x262144*(4x4^1)_NN_d
Tuning for shape 16x262144*(4x4^2)_NN_d
Tuning for shape 16x262144*(4x4^3)_NN_d
Tuning for shape 16x262144*(4x4^4)_NN_d
Tuning for shape 16x262144*(4x4^1)_NN_d
Tuning for shape 16x262144*(4x4^2)_NN_d
Tuning for shape 16x262144*(4x4^3)_NN_d
Tuning for shape 16x262144*(4x4^1)_NN_d
Tuning for shape 16x262144*(4x4^2)_NN_d
Tuning for shape 16x262144*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^9  &  1.000 & 1.000 & 69.459 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 9 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 262144] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x to produce Y[64, 262144]
Matmul: 64 x 262144 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x262144*(4x4^1)_NN_d
Tuning for shape 64x262144*(4x4^2)_NN_d
Tuning for shape 64x262144*(4x4^3)_NN_d
Tuning for shape 64x262144*(4x4^4)_NN_d
Tuning for shape 64x262144*(4x4^5)_NN_d
Tuning for shape 64x262144*(4x4^6)_NN_d
Tuning for shape 64x262144*(4x4^7)_NN_d
Tuning for shape 64x262144*(4x4^8)_NN_d
Tuning for shape 64x262144*(4x4^9)_NN_d
Tuning for shape 64x262144*(4x4^1)_NN_d
Tuning for shape 64x262144*(4x4^2)_NN_d
Tuning for shape 64x262144*(4x4^3)_NN_d
Tuning for shape 64x262144*(4x4^4)_NN_d
Tuning for shape 64x262144*(4x4^5)_NN_d
Tuning for shape 64x262144*(4x4^6)_NN_d
Tuning for shape 64x262144*(4x4^7)_NN_d
Tuning for shape 64x262144*(4x4^8)_NN_d
Tuning for shape 64x262144*(4x4^1)_NN_d
Tuning for shape 64x262144*(4x4^2)_NN_d
Tuning for shape 64x262144*(4x4^3)_NN_d
Tuning for shape 64x262144*(4x4^4)_NN_d
Tuning for shape 64x262144*(4x4^5)_NN_d
Tuning for shape 64x262144*(4x4^6)_NN_d
Tuning for shape 64x262144*(4x4^7)_NN_d
Tuning for shape 64x262144*(4x4^1)_NN_d
Tuning for shape 64x262144*(4x4^2)_NN_d
Tuning for shape 64x262144*(4x4^3)_NN_d
Tuning for shape 64x262144*(4x4^4)_NN_d
Tuning for shape 64x262144*(4x4^5)_NN_d
Tuning for shape 64x262144*(4x4^6)_NN_d
Tuning for shape 64x262144*(4x4^1)_NN_d
Tuning for shape 64x262144*(4x4^2)_NN_d
Tuning for shape 64x262144*(4x4^3)_NN_d
Tuning for shape 64x262144*(4x4^4)_NN_d
Tuning for shape 64x262144*(4x4^5)_NN_d
Tuning for shape 64x262144*(4x4^1)_NN_d
Tuning for shape 64x262144*(4x4^2)_NN_d
Tuning for shape 64x262144*(4x4^3)_NN_d
Tuning for shape 64x262144*(4x4^4)_NN_d
Tuning for shape 64x262144*(4x4^1)_NN_d
Tuning for shape 64x262144*(4x4^2)_NN_d
Tuning for shape 64x262144*(4x4^3)_NN_d
Tuning for shape 64x262144*(4x4^1)_NN_d
Tuning for shape 64x262144*(4x4^2)_NN_d
Tuning for shape 64x262144*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^9  &  1.000 & 1.000 & 70.040 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 9 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 262144] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x to produce Y[256, 262144]
Matmul: 256 x 262144 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x262144*(4x4^1)_NN_d
Tuning for shape 256x262144*(4x4^2)_NN_d
Tuning for shape 256x262144*(4x4^3)_NN_d
Tuning for shape 256x262144*(4x4^4)_NN_d
Tuning for shape 256x262144*(4x4^5)_NN_d
Tuning for shape 256x262144*(4x4^6)_NN_d
Tuning for shape 256x262144*(4x4^7)_NN_d
Tuning for shape 256x262144*(4x4^8)_NN_d
Tuning for shape 256x262144*(4x4^9)_NN_d
Tuning for shape 256x262144*(4x4^1)_NN_d
Tuning for shape 256x262144*(4x4^2)_NN_d
Tuning for shape 256x262144*(4x4^3)_NN_d
Tuning for shape 256x262144*(4x4^4)_NN_d
Tuning for shape 256x262144*(4x4^5)_NN_d
Tuning for shape 256x262144*(4x4^6)_NN_d
Tuning for shape 256x262144*(4x4^7)_NN_d
Tuning for shape 256x262144*(4x4^8)_NN_d
Tuning for shape 256x262144*(4x4^1)_NN_d
Tuning for shape 256x262144*(4x4^2)_NN_d
Tuning for shape 256x262144*(4x4^3)_NN_d
Tuning for shape 256x262144*(4x4^4)_NN_d
Tuning for shape 256x262144*(4x4^5)_NN_d
Tuning for shape 256x262144*(4x4^6)_NN_d
Tuning for shape 256x262144*(4x4^7)_NN_d
Tuning for shape 256x262144*(4x4^1)_NN_d
Tuning for shape 256x262144*(4x4^2)_NN_d
Tuning for shape 256x262144*(4x4^3)_NN_d
Tuning for shape 256x262144*(4x4^4)_NN_d
Tuning for shape 256x262144*(4x4^5)_NN_d
Tuning for shape 256x262144*(4x4^6)_NN_d
Tuning for shape 256x262144*(4x4^1)_NN_d
Tuning for shape 256x262144*(4x4^2)_NN_d
Tuning for shape 256x262144*(4x4^3)_NN_d
Tuning for shape 256x262144*(4x4^4)_NN_d
Tuning for shape 256x262144*(4x4^5)_NN_d
Tuning for shape 256x262144*(4x4^1)_NN_d
Tuning for shape 256x262144*(4x4^2)_NN_d
Tuning for shape 256x262144*(4x4^3)_NN_d
Tuning for shape 256x262144*(4x4^4)_NN_d
Tuning for shape 256x262144*(4x4^1)_NN_d
Tuning for shape 256x262144*(4x4^2)_NN_d
Tuning for shape 256x262144*(4x4^3)_NN_d
Tuning for shape 256x262144*(4x4^1)_NN_d
Tuning for shape 256x262144*(4x4^2)_NN_d
Tuning for shape 256x262144*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^9  &  1.000 & 1.000 & 65.086 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 9 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 262144] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x to produce Y[1024, 262144]
Matmul: 1024 x 262144 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x262144*(4x4^1)_NN_d
Tuning for shape 1024x262144*(4x4^2)_NN_d
Tuning for shape 1024x262144*(4x4^3)_NN_d
Tuning for shape 1024x262144*(4x4^4)_NN_d
Tuning for shape 1024x262144*(4x4^5)_NN_d
Tuning for shape 1024x262144*(4x4^6)_NN_d
Tuning for shape 1024x262144*(4x4^7)_NN_d
Tuning for shape 1024x262144*(4x4^8)_NN_d
Tuning for shape 1024x262144*(4x4^9)_NN_d
Tuning for shape 1024x262144*(4x4^1)_NN_d
Tuning for shape 1024x262144*(4x4^2)_NN_d
Tuning for shape 1024x262144*(4x4^3)_NN_d
Tuning for shape 1024x262144*(4x4^4)_NN_d
Tuning for shape 1024x262144*(4x4^5)_NN_d
Tuning for shape 1024x262144*(4x4^6)_NN_d
Tuning for shape 1024x262144*(4x4^7)_NN_d
Tuning for shape 1024x262144*(4x4^8)_NN_d
Tuning for shape 1024x262144*(4x4^1)_NN_d
Tuning for shape 1024x262144*(4x4^2)_NN_d
Tuning for shape 1024x262144*(4x4^3)_NN_d
Tuning for shape 1024x262144*(4x4^4)_NN_d
Tuning for shape 1024x262144*(4x4^5)_NN_d
Tuning for shape 1024x262144*(4x4^6)_NN_d
Tuning for shape 1024x262144*(4x4^7)_NN_d
Tuning for shape 1024x262144*(4x4^1)_NN_d
Tuning for shape 1024x262144*(4x4^2)_NN_d
Tuning for shape 1024x262144*(4x4^3)_NN_d
Tuning for shape 1024x262144*(4x4^4)_NN_d
Tuning for shape 1024x262144*(4x4^5)_NN_d
Tuning for shape 1024x262144*(4x4^6)_NN_d
Tuning for shape 1024x262144*(4x4^1)_NN_d
Tuning for shape 1024x262144*(4x4^2)_NN_d
Tuning for shape 1024x262144*(4x4^3)_NN_d
Tuning for shape 1024x262144*(4x4^4)_NN_d
Tuning for shape 1024x262144*(4x4^5)_NN_d
Tuning for shape 1024x262144*(4x4^1)_NN_d
Tuning for shape 1024x262144*(4x4^2)_NN_d
Tuning for shape 1024x262144*(4x4^3)_NN_d
Tuning for shape 1024x262144*(4x4^4)_NN_d
Tuning for shape 1024x262144*(4x4^1)_NN_d
Tuning for shape 1024x262144*(4x4^2)_NN_d
Tuning for shape 1024x262144*(4x4^3)_NN_d
Tuning for shape 1024x262144*(4x4^1)_NN_d
Tuning for shape 1024x262144*(4x4^2)_NN_d
Tuning for shape 1024x262144*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x4^9  &  1.000 & 1.000 & 65.069 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 10 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1048576] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x to produce Y[1, 1048576]
Matmul: 1 x 1048576 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1048576*(4x4^1)_NN_d
Tuning for shape 1x1048576*(4x4^2)_NN_d
Tuning for shape 1x1048576*(4x4^3)_NN_d
Tuning for shape 1x1048576*(4x4^4)_NN_d
Tuning for shape 1x1048576*(4x4^5)_NN_d
Tuning for shape 1x1048576*(4x4^6)_NN_d
Tuning for shape 1x1048576*(4x4^7)_NN_d
Tuning for shape 1x1048576*(4x4^8)_NN_d
Tuning for shape 1x1048576*(4x4^9)_NN_d
Tuning for shape 1x1048576*(4x4^10)_NN_d
Tuning for shape 1x1048576*(4x4^1)_NN_d
Tuning for shape 1x1048576*(4x4^2)_NN_d
Tuning for shape 1x1048576*(4x4^3)_NN_d
Tuning for shape 1x1048576*(4x4^4)_NN_d
Tuning for shape 1x1048576*(4x4^5)_NN_d
Tuning for shape 1x1048576*(4x4^6)_NN_d
Tuning for shape 1x1048576*(4x4^7)_NN_d
Tuning for shape 1x1048576*(4x4^8)_NN_d
Tuning for shape 1x1048576*(4x4^9)_NN_d
Tuning for shape 1x1048576*(4x4^1)_NN_d
Tuning for shape 1x1048576*(4x4^2)_NN_d
Tuning for shape 1x1048576*(4x4^3)_NN_d
Tuning for shape 1x1048576*(4x4^4)_NN_d
Tuning for shape 1x1048576*(4x4^5)_NN_d
Tuning for shape 1x1048576*(4x4^6)_NN_d
Tuning for shape 1x1048576*(4x4^7)_NN_d
Tuning for shape 1x1048576*(4x4^8)_NN_d
Tuning for shape 1x1048576*(4x4^1)_NN_d
Tuning for shape 1x1048576*(4x4^2)_NN_d
Tuning for shape 1x1048576*(4x4^3)_NN_d
Tuning for shape 1x1048576*(4x4^4)_NN_d
Tuning for shape 1x1048576*(4x4^5)_NN_d
Tuning for shape 1x1048576*(4x4^6)_NN_d
Tuning for shape 1x1048576*(4x4^7)_NN_d
Tuning for shape 1x1048576*(4x4^1)_NN_d
Tuning for shape 1x1048576*(4x4^2)_NN_d
Tuning for shape 1x1048576*(4x4^3)_NN_d
Tuning for shape 1x1048576*(4x4^4)_NN_d
Tuning for shape 1x1048576*(4x4^5)_NN_d
Tuning for shape 1x1048576*(4x4^6)_NN_d
Tuning for shape 1x1048576*(4x4^1)_NN_d
Tuning for shape 1x1048576*(4x4^2)_NN_d
Tuning for shape 1x1048576*(4x4^3)_NN_d
Tuning for shape 1x1048576*(4x4^4)_NN_d
Tuning for shape 1x1048576*(4x4^5)_NN_d
Tuning for shape 1x1048576*(4x4^1)_NN_d
Tuning for shape 1x1048576*(4x4^2)_NN_d
Tuning for shape 1x1048576*(4x4^3)_NN_d
Tuning for shape 1x1048576*(4x4^4)_NN_d
Tuning for shape 1x1048576*(4x4^1)_NN_d
Tuning for shape 1x1048576*(4x4^2)_NN_d
Tuning for shape 1x1048576*(4x4^3)_NN_d
Tuning for shape 1x1048576*(4x4^1)_NN_d
Tuning for shape 1x1048576*(4x4^2)_NN_d
Tuning for shape 1x1048576*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^10  &  1.000 & 1.000 & 66.627 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 10 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1048576] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x to produce Y[4, 1048576]
Matmul: 4 x 1048576 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1048576*(4x4^1)_NN_d
Tuning for shape 4x1048576*(4x4^2)_NN_d
Tuning for shape 4x1048576*(4x4^3)_NN_d
Tuning for shape 4x1048576*(4x4^4)_NN_d
Tuning for shape 4x1048576*(4x4^5)_NN_d
Tuning for shape 4x1048576*(4x4^6)_NN_d
Tuning for shape 4x1048576*(4x4^7)_NN_d
Tuning for shape 4x1048576*(4x4^8)_NN_d
Tuning for shape 4x1048576*(4x4^9)_NN_d
Tuning for shape 4x1048576*(4x4^10)_NN_d
Tuning for shape 4x1048576*(4x4^1)_NN_d
Tuning for shape 4x1048576*(4x4^2)_NN_d
Tuning for shape 4x1048576*(4x4^3)_NN_d
Tuning for shape 4x1048576*(4x4^4)_NN_d
Tuning for shape 4x1048576*(4x4^5)_NN_d
Tuning for shape 4x1048576*(4x4^6)_NN_d
Tuning for shape 4x1048576*(4x4^7)_NN_d
Tuning for shape 4x1048576*(4x4^8)_NN_d
Tuning for shape 4x1048576*(4x4^9)_NN_d
Tuning for shape 4x1048576*(4x4^1)_NN_d
Tuning for shape 4x1048576*(4x4^2)_NN_d
Tuning for shape 4x1048576*(4x4^3)_NN_d
Tuning for shape 4x1048576*(4x4^4)_NN_d
Tuning for shape 4x1048576*(4x4^5)_NN_d
Tuning for shape 4x1048576*(4x4^6)_NN_d
Tuning for shape 4x1048576*(4x4^7)_NN_d
Tuning for shape 4x1048576*(4x4^8)_NN_d
Tuning for shape 4x1048576*(4x4^1)_NN_d
Tuning for shape 4x1048576*(4x4^2)_NN_d
Tuning for shape 4x1048576*(4x4^3)_NN_d
Tuning for shape 4x1048576*(4x4^4)_NN_d
Tuning for shape 4x1048576*(4x4^5)_NN_d
Tuning for shape 4x1048576*(4x4^6)_NN_d
Tuning for shape 4x1048576*(4x4^7)_NN_d
Tuning for shape 4x1048576*(4x4^1)_NN_d
Tuning for shape 4x1048576*(4x4^2)_NN_d
Tuning for shape 4x1048576*(4x4^3)_NN_d
Tuning for shape 4x1048576*(4x4^4)_NN_d
Tuning for shape 4x1048576*(4x4^5)_NN_d
Tuning for shape 4x1048576*(4x4^6)_NN_d
Tuning for shape 4x1048576*(4x4^1)_NN_d
Tuning for shape 4x1048576*(4x4^2)_NN_d
Tuning for shape 4x1048576*(4x4^3)_NN_d
Tuning for shape 4x1048576*(4x4^4)_NN_d
Tuning for shape 4x1048576*(4x4^5)_NN_d
Tuning for shape 4x1048576*(4x4^1)_NN_d
Tuning for shape 4x1048576*(4x4^2)_NN_d
Tuning for shape 4x1048576*(4x4^3)_NN_d
Tuning for shape 4x1048576*(4x4^4)_NN_d
Tuning for shape 4x1048576*(4x4^1)_NN_d
Tuning for shape 4x1048576*(4x4^2)_NN_d
Tuning for shape 4x1048576*(4x4^3)_NN_d
Tuning for shape 4x1048576*(4x4^1)_NN_d
Tuning for shape 4x1048576*(4x4^2)_NN_d
Tuning for shape 4x1048576*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^10  &  1.000 & 1.000 & 69.535 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 10 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1048576] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x to produce Y[16, 1048576]
Matmul: 16 x 1048576 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1048576*(4x4^1)_NN_d
Tuning for shape 16x1048576*(4x4^2)_NN_d
Tuning for shape 16x1048576*(4x4^3)_NN_d
Tuning for shape 16x1048576*(4x4^4)_NN_d
Tuning for shape 16x1048576*(4x4^5)_NN_d
Tuning for shape 16x1048576*(4x4^6)_NN_d
Tuning for shape 16x1048576*(4x4^7)_NN_d
Tuning for shape 16x1048576*(4x4^8)_NN_d
Tuning for shape 16x1048576*(4x4^9)_NN_d
Tuning for shape 16x1048576*(4x4^10)_NN_d
Tuning for shape 16x1048576*(4x4^1)_NN_d
Tuning for shape 16x1048576*(4x4^2)_NN_d
Tuning for shape 16x1048576*(4x4^3)_NN_d
Tuning for shape 16x1048576*(4x4^4)_NN_d
Tuning for shape 16x1048576*(4x4^5)_NN_d
Tuning for shape 16x1048576*(4x4^6)_NN_d
Tuning for shape 16x1048576*(4x4^7)_NN_d
Tuning for shape 16x1048576*(4x4^8)_NN_d
Tuning for shape 16x1048576*(4x4^9)_NN_d
Tuning for shape 16x1048576*(4x4^1)_NN_d
Tuning for shape 16x1048576*(4x4^2)_NN_d
Tuning for shape 16x1048576*(4x4^3)_NN_d
Tuning for shape 16x1048576*(4x4^4)_NN_d
Tuning for shape 16x1048576*(4x4^5)_NN_d
Tuning for shape 16x1048576*(4x4^6)_NN_d
Tuning for shape 16x1048576*(4x4^7)_NN_d
Tuning for shape 16x1048576*(4x4^8)_NN_d
Tuning for shape 16x1048576*(4x4^1)_NN_d
Tuning for shape 16x1048576*(4x4^2)_NN_d
Tuning for shape 16x1048576*(4x4^3)_NN_d
Tuning for shape 16x1048576*(4x4^4)_NN_d
Tuning for shape 16x1048576*(4x4^5)_NN_d
Tuning for shape 16x1048576*(4x4^6)_NN_d
Tuning for shape 16x1048576*(4x4^7)_NN_d
Tuning for shape 16x1048576*(4x4^1)_NN_d
Tuning for shape 16x1048576*(4x4^2)_NN_d
Tuning for shape 16x1048576*(4x4^3)_NN_d
Tuning for shape 16x1048576*(4x4^4)_NN_d
Tuning for shape 16x1048576*(4x4^5)_NN_d
Tuning for shape 16x1048576*(4x4^6)_NN_d
Tuning for shape 16x1048576*(4x4^1)_NN_d
Tuning for shape 16x1048576*(4x4^2)_NN_d
Tuning for shape 16x1048576*(4x4^3)_NN_d
Tuning for shape 16x1048576*(4x4^4)_NN_d
Tuning for shape 16x1048576*(4x4^5)_NN_d
Tuning for shape 16x1048576*(4x4^1)_NN_d
Tuning for shape 16x1048576*(4x4^2)_NN_d
Tuning for shape 16x1048576*(4x4^3)_NN_d
Tuning for shape 16x1048576*(4x4^4)_NN_d
Tuning for shape 16x1048576*(4x4^1)_NN_d
Tuning for shape 16x1048576*(4x4^2)_NN_d
Tuning for shape 16x1048576*(4x4^3)_NN_d
Tuning for shape 16x1048576*(4x4^1)_NN_d
Tuning for shape 16x1048576*(4x4^2)_NN_d
Tuning for shape 16x1048576*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^10  &  1.000 & 1.000 & 71.321 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 10 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1048576] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x to produce Y[64, 1048576]
Matmul: 64 x 1048576 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1048576*(4x4^1)_NN_d
Tuning for shape 64x1048576*(4x4^2)_NN_d
Tuning for shape 64x1048576*(4x4^3)_NN_d
Tuning for shape 64x1048576*(4x4^4)_NN_d
Tuning for shape 64x1048576*(4x4^5)_NN_d
Tuning for shape 64x1048576*(4x4^6)_NN_d
Tuning for shape 64x1048576*(4x4^7)_NN_d
Tuning for shape 64x1048576*(4x4^8)_NN_d
Tuning for shape 64x1048576*(4x4^9)_NN_d
Tuning for shape 64x1048576*(4x4^10)_NN_d
Tuning for shape 64x1048576*(4x4^1)_NN_d
Tuning for shape 64x1048576*(4x4^2)_NN_d
Tuning for shape 64x1048576*(4x4^3)_NN_d
Tuning for shape 64x1048576*(4x4^4)_NN_d
Tuning for shape 64x1048576*(4x4^5)_NN_d
Tuning for shape 64x1048576*(4x4^6)_NN_d
Tuning for shape 64x1048576*(4x4^7)_NN_d
Tuning for shape 64x1048576*(4x4^8)_NN_d
Tuning for shape 64x1048576*(4x4^9)_NN_d
Tuning for shape 64x1048576*(4x4^1)_NN_d
Tuning for shape 64x1048576*(4x4^2)_NN_d
Tuning for shape 64x1048576*(4x4^3)_NN_d
Tuning for shape 64x1048576*(4x4^4)_NN_d
Tuning for shape 64x1048576*(4x4^5)_NN_d
Tuning for shape 64x1048576*(4x4^6)_NN_d
Tuning for shape 64x1048576*(4x4^7)_NN_d
Tuning for shape 64x1048576*(4x4^8)_NN_d
Tuning for shape 64x1048576*(4x4^1)_NN_d
Tuning for shape 64x1048576*(4x4^2)_NN_d
Tuning for shape 64x1048576*(4x4^3)_NN_d
Tuning for shape 64x1048576*(4x4^4)_NN_d
Tuning for shape 64x1048576*(4x4^5)_NN_d
Tuning for shape 64x1048576*(4x4^6)_NN_d
Tuning for shape 64x1048576*(4x4^7)_NN_d
Tuning for shape 64x1048576*(4x4^1)_NN_d
Tuning for shape 64x1048576*(4x4^2)_NN_d
Tuning for shape 64x1048576*(4x4^3)_NN_d
Tuning for shape 64x1048576*(4x4^4)_NN_d
Tuning for shape 64x1048576*(4x4^5)_NN_d
Tuning for shape 64x1048576*(4x4^6)_NN_d
Tuning for shape 64x1048576*(4x4^1)_NN_d
Tuning for shape 64x1048576*(4x4^2)_NN_d
Tuning for shape 64x1048576*(4x4^3)_NN_d
Tuning for shape 64x1048576*(4x4^4)_NN_d
Tuning for shape 64x1048576*(4x4^5)_NN_d
Tuning for shape 64x1048576*(4x4^1)_NN_d
Tuning for shape 64x1048576*(4x4^2)_NN_d
Tuning for shape 64x1048576*(4x4^3)_NN_d
Tuning for shape 64x1048576*(4x4^4)_NN_d
Tuning for shape 64x1048576*(4x4^1)_NN_d
Tuning for shape 64x1048576*(4x4^2)_NN_d
Tuning for shape 64x1048576*(4x4^3)_NN_d
Tuning for shape 64x1048576*(4x4^1)_NN_d
Tuning for shape 64x1048576*(4x4^2)_NN_d
Tuning for shape 64x1048576*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^10  &  1.000 & 1.000 & 70.412 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 10 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1048576] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x to produce Y[256, 1048576]
Matmul: 256 x 1048576 x 1048576, Num KP Factors: 10
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1048576*(4x4^1)_NN_d
Tuning for shape 256x1048576*(4x4^2)_NN_d
Tuning for shape 256x1048576*(4x4^3)_NN_d
Tuning for shape 256x1048576*(4x4^4)_NN_d
Tuning for shape 256x1048576*(4x4^5)_NN_d
Tuning for shape 256x1048576*(4x4^6)_NN_d
Tuning for shape 256x1048576*(4x4^7)_NN_d
Tuning for shape 256x1048576*(4x4^8)_NN_d
Tuning for shape 256x1048576*(4x4^9)_NN_d
Tuning for shape 256x1048576*(4x4^10)_NN_d
Tuning for shape 256x1048576*(4x4^1)_NN_d
Tuning for shape 256x1048576*(4x4^2)_NN_d
Tuning for shape 256x1048576*(4x4^3)_NN_d
Tuning for shape 256x1048576*(4x4^4)_NN_d
Tuning for shape 256x1048576*(4x4^5)_NN_d
Tuning for shape 256x1048576*(4x4^6)_NN_d
Tuning for shape 256x1048576*(4x4^7)_NN_d
Tuning for shape 256x1048576*(4x4^8)_NN_d
Tuning for shape 256x1048576*(4x4^9)_NN_d
Tuning for shape 256x1048576*(4x4^1)_NN_d
Tuning for shape 256x1048576*(4x4^2)_NN_d
Tuning for shape 256x1048576*(4x4^3)_NN_d
Tuning for shape 256x1048576*(4x4^4)_NN_d
Tuning for shape 256x1048576*(4x4^5)_NN_d
Tuning for shape 256x1048576*(4x4^6)_NN_d
Tuning for shape 256x1048576*(4x4^7)_NN_d
Tuning for shape 256x1048576*(4x4^8)_NN_d
Tuning for shape 256x1048576*(4x4^1)_NN_d
Tuning for shape 256x1048576*(4x4^2)_NN_d
Tuning for shape 256x1048576*(4x4^3)_NN_d
Tuning for shape 256x1048576*(4x4^4)_NN_d
Tuning for shape 256x1048576*(4x4^5)_NN_d
Tuning for shape 256x1048576*(4x4^6)_NN_d
Tuning for shape 256x1048576*(4x4^7)_NN_d
Tuning for shape 256x1048576*(4x4^1)_NN_d
Tuning for shape 256x1048576*(4x4^2)_NN_d
Tuning for shape 256x1048576*(4x4^3)_NN_d
Tuning for shape 256x1048576*(4x4^4)_NN_d
Tuning for shape 256x1048576*(4x4^5)_NN_d
Tuning for shape 256x1048576*(4x4^6)_NN_d
Tuning for shape 256x1048576*(4x4^1)_NN_d
Tuning for shape 256x1048576*(4x4^2)_NN_d
Tuning for shape 256x1048576*(4x4^3)_NN_d
Tuning for shape 256x1048576*(4x4^4)_NN_d
Tuning for shape 256x1048576*(4x4^5)_NN_d
Tuning for shape 256x1048576*(4x4^1)_NN_d
Tuning for shape 256x1048576*(4x4^2)_NN_d
Tuning for shape 256x1048576*(4x4^3)_NN_d
Tuning for shape 256x1048576*(4x4^4)_NN_d
Tuning for shape 256x1048576*(4x4^1)_NN_d
Tuning for shape 256x1048576*(4x4^2)_NN_d
Tuning for shape 256x1048576*(4x4^3)_NN_d
Tuning for shape 256x1048576*(4x4^1)_NN_d
Tuning for shape 256x1048576*(4x4^2)_NN_d
Tuning for shape 256x1048576*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x4^10  &  1.000 & 1.000 & 66.089 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 11 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4194304] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x to produce Y[1, 4194304]
Matmul: 1 x 4194304 x 4194304, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^3)_NN_d
Tuning for shape 1x4194304*(4x4^4)_NN_d
Tuning for shape 1x4194304*(4x4^5)_NN_d
Tuning for shape 1x4194304*(4x4^6)_NN_d
Tuning for shape 1x4194304*(4x4^7)_NN_d
Tuning for shape 1x4194304*(4x4^8)_NN_d
Tuning for shape 1x4194304*(4x4^9)_NN_d
Tuning for shape 1x4194304*(4x4^10)_NN_d
Tuning for shape 1x4194304*(4x4^11)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^3)_NN_d
Tuning for shape 1x4194304*(4x4^4)_NN_d
Tuning for shape 1x4194304*(4x4^5)_NN_d
Tuning for shape 1x4194304*(4x4^6)_NN_d
Tuning for shape 1x4194304*(4x4^7)_NN_d
Tuning for shape 1x4194304*(4x4^8)_NN_d
Tuning for shape 1x4194304*(4x4^9)_NN_d
Tuning for shape 1x4194304*(4x4^10)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^3)_NN_d
Tuning for shape 1x4194304*(4x4^4)_NN_d
Tuning for shape 1x4194304*(4x4^5)_NN_d
Tuning for shape 1x4194304*(4x4^6)_NN_d
Tuning for shape 1x4194304*(4x4^7)_NN_d
Tuning for shape 1x4194304*(4x4^8)_NN_d
Tuning for shape 1x4194304*(4x4^9)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^3)_NN_d
Tuning for shape 1x4194304*(4x4^4)_NN_d
Tuning for shape 1x4194304*(4x4^5)_NN_d
Tuning for shape 1x4194304*(4x4^6)_NN_d
Tuning for shape 1x4194304*(4x4^7)_NN_d
Tuning for shape 1x4194304*(4x4^8)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^3)_NN_d
Tuning for shape 1x4194304*(4x4^4)_NN_d
Tuning for shape 1x4194304*(4x4^5)_NN_d
Tuning for shape 1x4194304*(4x4^6)_NN_d
Tuning for shape 1x4194304*(4x4^7)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^3)_NN_d
Tuning for shape 1x4194304*(4x4^4)_NN_d
Tuning for shape 1x4194304*(4x4^5)_NN_d
Tuning for shape 1x4194304*(4x4^6)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^3)_NN_d
Tuning for shape 1x4194304*(4x4^4)_NN_d
Tuning for shape 1x4194304*(4x4^5)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^3)_NN_d
Tuning for shape 1x4194304*(4x4^4)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^3)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Tuning for shape 1x4194304*(4x4^2)_NN_d
Tuning for shape 1x4194304*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^11  &  1.000 & 1.000 & 70.841 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 11 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4194304] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x to produce Y[4, 4194304]
Matmul: 4 x 4194304 x 4194304, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^3)_NN_d
Tuning for shape 4x4194304*(4x4^4)_NN_d
Tuning for shape 4x4194304*(4x4^5)_NN_d
Tuning for shape 4x4194304*(4x4^6)_NN_d
Tuning for shape 4x4194304*(4x4^7)_NN_d
Tuning for shape 4x4194304*(4x4^8)_NN_d
Tuning for shape 4x4194304*(4x4^9)_NN_d
Tuning for shape 4x4194304*(4x4^10)_NN_d
Tuning for shape 4x4194304*(4x4^11)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^3)_NN_d
Tuning for shape 4x4194304*(4x4^4)_NN_d
Tuning for shape 4x4194304*(4x4^5)_NN_d
Tuning for shape 4x4194304*(4x4^6)_NN_d
Tuning for shape 4x4194304*(4x4^7)_NN_d
Tuning for shape 4x4194304*(4x4^8)_NN_d
Tuning for shape 4x4194304*(4x4^9)_NN_d
Tuning for shape 4x4194304*(4x4^10)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^3)_NN_d
Tuning for shape 4x4194304*(4x4^4)_NN_d
Tuning for shape 4x4194304*(4x4^5)_NN_d
Tuning for shape 4x4194304*(4x4^6)_NN_d
Tuning for shape 4x4194304*(4x4^7)_NN_d
Tuning for shape 4x4194304*(4x4^8)_NN_d
Tuning for shape 4x4194304*(4x4^9)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^3)_NN_d
Tuning for shape 4x4194304*(4x4^4)_NN_d
Tuning for shape 4x4194304*(4x4^5)_NN_d
Tuning for shape 4x4194304*(4x4^6)_NN_d
Tuning for shape 4x4194304*(4x4^7)_NN_d
Tuning for shape 4x4194304*(4x4^8)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^3)_NN_d
Tuning for shape 4x4194304*(4x4^4)_NN_d
Tuning for shape 4x4194304*(4x4^5)_NN_d
Tuning for shape 4x4194304*(4x4^6)_NN_d
Tuning for shape 4x4194304*(4x4^7)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^3)_NN_d
Tuning for shape 4x4194304*(4x4^4)_NN_d
Tuning for shape 4x4194304*(4x4^5)_NN_d
Tuning for shape 4x4194304*(4x4^6)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^3)_NN_d
Tuning for shape 4x4194304*(4x4^4)_NN_d
Tuning for shape 4x4194304*(4x4^5)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^3)_NN_d
Tuning for shape 4x4194304*(4x4^4)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^3)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Tuning for shape 4x4194304*(4x4^2)_NN_d
Tuning for shape 4x4194304*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^11  &  1.000 & 1.000 & 71.405 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 11 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4194304] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x to produce Y[16, 4194304]
Matmul: 16 x 4194304 x 4194304, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^3)_NN_d
Tuning for shape 16x4194304*(4x4^4)_NN_d
Tuning for shape 16x4194304*(4x4^5)_NN_d
Tuning for shape 16x4194304*(4x4^6)_NN_d
Tuning for shape 16x4194304*(4x4^7)_NN_d
Tuning for shape 16x4194304*(4x4^8)_NN_d
Tuning for shape 16x4194304*(4x4^9)_NN_d
Tuning for shape 16x4194304*(4x4^10)_NN_d
Tuning for shape 16x4194304*(4x4^11)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^3)_NN_d
Tuning for shape 16x4194304*(4x4^4)_NN_d
Tuning for shape 16x4194304*(4x4^5)_NN_d
Tuning for shape 16x4194304*(4x4^6)_NN_d
Tuning for shape 16x4194304*(4x4^7)_NN_d
Tuning for shape 16x4194304*(4x4^8)_NN_d
Tuning for shape 16x4194304*(4x4^9)_NN_d
Tuning for shape 16x4194304*(4x4^10)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^3)_NN_d
Tuning for shape 16x4194304*(4x4^4)_NN_d
Tuning for shape 16x4194304*(4x4^5)_NN_d
Tuning for shape 16x4194304*(4x4^6)_NN_d
Tuning for shape 16x4194304*(4x4^7)_NN_d
Tuning for shape 16x4194304*(4x4^8)_NN_d
Tuning for shape 16x4194304*(4x4^9)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^3)_NN_d
Tuning for shape 16x4194304*(4x4^4)_NN_d
Tuning for shape 16x4194304*(4x4^5)_NN_d
Tuning for shape 16x4194304*(4x4^6)_NN_d
Tuning for shape 16x4194304*(4x4^7)_NN_d
Tuning for shape 16x4194304*(4x4^8)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^3)_NN_d
Tuning for shape 16x4194304*(4x4^4)_NN_d
Tuning for shape 16x4194304*(4x4^5)_NN_d
Tuning for shape 16x4194304*(4x4^6)_NN_d
Tuning for shape 16x4194304*(4x4^7)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^3)_NN_d
Tuning for shape 16x4194304*(4x4^4)_NN_d
Tuning for shape 16x4194304*(4x4^5)_NN_d
Tuning for shape 16x4194304*(4x4^6)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^3)_NN_d
Tuning for shape 16x4194304*(4x4^4)_NN_d
Tuning for shape 16x4194304*(4x4^5)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^3)_NN_d
Tuning for shape 16x4194304*(4x4^4)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^3)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Tuning for shape 16x4194304*(4x4^2)_NN_d
Tuning for shape 16x4194304*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^11  &  1.000 & 1.000 & 71.616 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 11 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4194304] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x to produce Y[64, 4194304]
Matmul: 64 x 4194304 x 4194304, Num KP Factors: 11
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^3)_NN_d
Tuning for shape 64x4194304*(4x4^4)_NN_d
Tuning for shape 64x4194304*(4x4^5)_NN_d
Tuning for shape 64x4194304*(4x4^6)_NN_d
Tuning for shape 64x4194304*(4x4^7)_NN_d
Tuning for shape 64x4194304*(4x4^8)_NN_d
Tuning for shape 64x4194304*(4x4^9)_NN_d
Tuning for shape 64x4194304*(4x4^10)_NN_d
Tuning for shape 64x4194304*(4x4^11)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^3)_NN_d
Tuning for shape 64x4194304*(4x4^4)_NN_d
Tuning for shape 64x4194304*(4x4^5)_NN_d
Tuning for shape 64x4194304*(4x4^6)_NN_d
Tuning for shape 64x4194304*(4x4^7)_NN_d
Tuning for shape 64x4194304*(4x4^8)_NN_d
Tuning for shape 64x4194304*(4x4^9)_NN_d
Tuning for shape 64x4194304*(4x4^10)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^3)_NN_d
Tuning for shape 64x4194304*(4x4^4)_NN_d
Tuning for shape 64x4194304*(4x4^5)_NN_d
Tuning for shape 64x4194304*(4x4^6)_NN_d
Tuning for shape 64x4194304*(4x4^7)_NN_d
Tuning for shape 64x4194304*(4x4^8)_NN_d
Tuning for shape 64x4194304*(4x4^9)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^3)_NN_d
Tuning for shape 64x4194304*(4x4^4)_NN_d
Tuning for shape 64x4194304*(4x4^5)_NN_d
Tuning for shape 64x4194304*(4x4^6)_NN_d
Tuning for shape 64x4194304*(4x4^7)_NN_d
Tuning for shape 64x4194304*(4x4^8)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^3)_NN_d
Tuning for shape 64x4194304*(4x4^4)_NN_d
Tuning for shape 64x4194304*(4x4^5)_NN_d
Tuning for shape 64x4194304*(4x4^6)_NN_d
Tuning for shape 64x4194304*(4x4^7)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^3)_NN_d
Tuning for shape 64x4194304*(4x4^4)_NN_d
Tuning for shape 64x4194304*(4x4^5)_NN_d
Tuning for shape 64x4194304*(4x4^6)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^3)_NN_d
Tuning for shape 64x4194304*(4x4^4)_NN_d
Tuning for shape 64x4194304*(4x4^5)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^3)_NN_d
Tuning for shape 64x4194304*(4x4^4)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^3)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Tuning for shape 64x4194304*(4x4^2)_NN_d
Tuning for shape 64x4194304*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x4^11  &  1.000 & 1.000 & 70.845 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 12 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16777216] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x F_11 [4, 4] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 16777216, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^4)_NN_d
Tuning for shape 1x16777216*(4x4^5)_NN_d
Tuning for shape 1x16777216*(4x4^6)_NN_d
Tuning for shape 1x16777216*(4x4^7)_NN_d
Tuning for shape 1x16777216*(4x4^8)_NN_d
Tuning for shape 1x16777216*(4x4^9)_NN_d
Tuning for shape 1x16777216*(4x4^10)_NN_d
Tuning for shape 1x16777216*(4x4^11)_NN_d
Tuning for shape 1x16777216*(4x4^12)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^4)_NN_d
Tuning for shape 1x16777216*(4x4^5)_NN_d
Tuning for shape 1x16777216*(4x4^6)_NN_d
Tuning for shape 1x16777216*(4x4^7)_NN_d
Tuning for shape 1x16777216*(4x4^8)_NN_d
Tuning for shape 1x16777216*(4x4^9)_NN_d
Tuning for shape 1x16777216*(4x4^10)_NN_d
Tuning for shape 1x16777216*(4x4^11)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^4)_NN_d
Tuning for shape 1x16777216*(4x4^5)_NN_d
Tuning for shape 1x16777216*(4x4^6)_NN_d
Tuning for shape 1x16777216*(4x4^7)_NN_d
Tuning for shape 1x16777216*(4x4^8)_NN_d
Tuning for shape 1x16777216*(4x4^9)_NN_d
Tuning for shape 1x16777216*(4x4^10)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^4)_NN_d
Tuning for shape 1x16777216*(4x4^5)_NN_d
Tuning for shape 1x16777216*(4x4^6)_NN_d
Tuning for shape 1x16777216*(4x4^7)_NN_d
Tuning for shape 1x16777216*(4x4^8)_NN_d
Tuning for shape 1x16777216*(4x4^9)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^4)_NN_d
Tuning for shape 1x16777216*(4x4^5)_NN_d
Tuning for shape 1x16777216*(4x4^6)_NN_d
Tuning for shape 1x16777216*(4x4^7)_NN_d
Tuning for shape 1x16777216*(4x4^8)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^4)_NN_d
Tuning for shape 1x16777216*(4x4^5)_NN_d
Tuning for shape 1x16777216*(4x4^6)_NN_d
Tuning for shape 1x16777216*(4x4^7)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^4)_NN_d
Tuning for shape 1x16777216*(4x4^5)_NN_d
Tuning for shape 1x16777216*(4x4^6)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^4)_NN_d
Tuning for shape 1x16777216*(4x4^5)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^4)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^3)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Tuning for shape 1x16777216*(4x4^2)_NN_d
Tuning for shape 1x16777216*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^12  &  1.000 & 1.000 & 72.503 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 12 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16777216] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x F_11 [4, 4] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 16777216, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^4)_NN_d
Tuning for shape 4x16777216*(4x4^5)_NN_d
Tuning for shape 4x16777216*(4x4^6)_NN_d
Tuning for shape 4x16777216*(4x4^7)_NN_d
Tuning for shape 4x16777216*(4x4^8)_NN_d
Tuning for shape 4x16777216*(4x4^9)_NN_d
Tuning for shape 4x16777216*(4x4^10)_NN_d
Tuning for shape 4x16777216*(4x4^11)_NN_d
Tuning for shape 4x16777216*(4x4^12)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^4)_NN_d
Tuning for shape 4x16777216*(4x4^5)_NN_d
Tuning for shape 4x16777216*(4x4^6)_NN_d
Tuning for shape 4x16777216*(4x4^7)_NN_d
Tuning for shape 4x16777216*(4x4^8)_NN_d
Tuning for shape 4x16777216*(4x4^9)_NN_d
Tuning for shape 4x16777216*(4x4^10)_NN_d
Tuning for shape 4x16777216*(4x4^11)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^4)_NN_d
Tuning for shape 4x16777216*(4x4^5)_NN_d
Tuning for shape 4x16777216*(4x4^6)_NN_d
Tuning for shape 4x16777216*(4x4^7)_NN_d
Tuning for shape 4x16777216*(4x4^8)_NN_d
Tuning for shape 4x16777216*(4x4^9)_NN_d
Tuning for shape 4x16777216*(4x4^10)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^4)_NN_d
Tuning for shape 4x16777216*(4x4^5)_NN_d
Tuning for shape 4x16777216*(4x4^6)_NN_d
Tuning for shape 4x16777216*(4x4^7)_NN_d
Tuning for shape 4x16777216*(4x4^8)_NN_d
Tuning for shape 4x16777216*(4x4^9)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^4)_NN_d
Tuning for shape 4x16777216*(4x4^5)_NN_d
Tuning for shape 4x16777216*(4x4^6)_NN_d
Tuning for shape 4x16777216*(4x4^7)_NN_d
Tuning for shape 4x16777216*(4x4^8)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^4)_NN_d
Tuning for shape 4x16777216*(4x4^5)_NN_d
Tuning for shape 4x16777216*(4x4^6)_NN_d
Tuning for shape 4x16777216*(4x4^7)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^4)_NN_d
Tuning for shape 4x16777216*(4x4^5)_NN_d
Tuning for shape 4x16777216*(4x4^6)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^4)_NN_d
Tuning for shape 4x16777216*(4x4^5)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^4)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^3)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Tuning for shape 4x16777216*(4x4^2)_NN_d
Tuning for shape 4x16777216*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^12  &  1.000 & 1.000 & 71.632 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 12 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16777216] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x F_11 [4, 4] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 16777216, Num KP Factors: 12
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^4)_NN_d
Tuning for shape 16x16777216*(4x4^5)_NN_d
Tuning for shape 16x16777216*(4x4^6)_NN_d
Tuning for shape 16x16777216*(4x4^7)_NN_d
Tuning for shape 16x16777216*(4x4^8)_NN_d
Tuning for shape 16x16777216*(4x4^9)_NN_d
Tuning for shape 16x16777216*(4x4^10)_NN_d
Tuning for shape 16x16777216*(4x4^11)_NN_d
Tuning for shape 16x16777216*(4x4^12)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^4)_NN_d
Tuning for shape 16x16777216*(4x4^5)_NN_d
Tuning for shape 16x16777216*(4x4^6)_NN_d
Tuning for shape 16x16777216*(4x4^7)_NN_d
Tuning for shape 16x16777216*(4x4^8)_NN_d
Tuning for shape 16x16777216*(4x4^9)_NN_d
Tuning for shape 16x16777216*(4x4^10)_NN_d
Tuning for shape 16x16777216*(4x4^11)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^4)_NN_d
Tuning for shape 16x16777216*(4x4^5)_NN_d
Tuning for shape 16x16777216*(4x4^6)_NN_d
Tuning for shape 16x16777216*(4x4^7)_NN_d
Tuning for shape 16x16777216*(4x4^8)_NN_d
Tuning for shape 16x16777216*(4x4^9)_NN_d
Tuning for shape 16x16777216*(4x4^10)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^4)_NN_d
Tuning for shape 16x16777216*(4x4^5)_NN_d
Tuning for shape 16x16777216*(4x4^6)_NN_d
Tuning for shape 16x16777216*(4x4^7)_NN_d
Tuning for shape 16x16777216*(4x4^8)_NN_d
Tuning for shape 16x16777216*(4x4^9)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^4)_NN_d
Tuning for shape 16x16777216*(4x4^5)_NN_d
Tuning for shape 16x16777216*(4x4^6)_NN_d
Tuning for shape 16x16777216*(4x4^7)_NN_d
Tuning for shape 16x16777216*(4x4^8)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^4)_NN_d
Tuning for shape 16x16777216*(4x4^5)_NN_d
Tuning for shape 16x16777216*(4x4^6)_NN_d
Tuning for shape 16x16777216*(4x4^7)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^4)_NN_d
Tuning for shape 16x16777216*(4x4^5)_NN_d
Tuning for shape 16x16777216*(4x4^6)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^4)_NN_d
Tuning for shape 16x16777216*(4x4^5)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^4)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^3)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Tuning for shape 16x16777216*(4x4^2)_NN_d
Tuning for shape 16x16777216*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x4^12  &  1.000 & 1.000 & 71.945 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 13 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 67108864] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x F_11 [4, 4] x F_12 [4, 4] x to produce Y[1, 67108864]
Matmul: 1 x 67108864 x 67108864, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^5)_NN_d
Tuning for shape 1x67108864*(4x4^6)_NN_d
Tuning for shape 1x67108864*(4x4^7)_NN_d
Tuning for shape 1x67108864*(4x4^8)_NN_d
Tuning for shape 1x67108864*(4x4^9)_NN_d
Tuning for shape 1x67108864*(4x4^10)_NN_d
Tuning for shape 1x67108864*(4x4^11)_NN_d
Tuning for shape 1x67108864*(4x4^12)_NN_d
Tuning for shape 1x67108864*(4x4^13)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^5)_NN_d
Tuning for shape 1x67108864*(4x4^6)_NN_d
Tuning for shape 1x67108864*(4x4^7)_NN_d
Tuning for shape 1x67108864*(4x4^8)_NN_d
Tuning for shape 1x67108864*(4x4^9)_NN_d
Tuning for shape 1x67108864*(4x4^10)_NN_d
Tuning for shape 1x67108864*(4x4^11)_NN_d
Tuning for shape 1x67108864*(4x4^12)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^5)_NN_d
Tuning for shape 1x67108864*(4x4^6)_NN_d
Tuning for shape 1x67108864*(4x4^7)_NN_d
Tuning for shape 1x67108864*(4x4^8)_NN_d
Tuning for shape 1x67108864*(4x4^9)_NN_d
Tuning for shape 1x67108864*(4x4^10)_NN_d
Tuning for shape 1x67108864*(4x4^11)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^5)_NN_d
Tuning for shape 1x67108864*(4x4^6)_NN_d
Tuning for shape 1x67108864*(4x4^7)_NN_d
Tuning for shape 1x67108864*(4x4^8)_NN_d
Tuning for shape 1x67108864*(4x4^9)_NN_d
Tuning for shape 1x67108864*(4x4^10)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^5)_NN_d
Tuning for shape 1x67108864*(4x4^6)_NN_d
Tuning for shape 1x67108864*(4x4^7)_NN_d
Tuning for shape 1x67108864*(4x4^8)_NN_d
Tuning for shape 1x67108864*(4x4^9)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^5)_NN_d
Tuning for shape 1x67108864*(4x4^6)_NN_d
Tuning for shape 1x67108864*(4x4^7)_NN_d
Tuning for shape 1x67108864*(4x4^8)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^5)_NN_d
Tuning for shape 1x67108864*(4x4^6)_NN_d
Tuning for shape 1x67108864*(4x4^7)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^5)_NN_d
Tuning for shape 1x67108864*(4x4^6)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^5)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^4)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^3)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Tuning for shape 1x67108864*(4x4^2)_NN_d
Tuning for shape 1x67108864*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^13  &  1.000 & 1.000 & 72.670 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 13 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 67108864] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x F_11 [4, 4] x F_12 [4, 4] x to produce Y[4, 67108864]
Matmul: 4 x 67108864 x 67108864, Num KP Factors: 13
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^5)_NN_d
Tuning for shape 4x67108864*(4x4^6)_NN_d
Tuning for shape 4x67108864*(4x4^7)_NN_d
Tuning for shape 4x67108864*(4x4^8)_NN_d
Tuning for shape 4x67108864*(4x4^9)_NN_d
Tuning for shape 4x67108864*(4x4^10)_NN_d
Tuning for shape 4x67108864*(4x4^11)_NN_d
Tuning for shape 4x67108864*(4x4^12)_NN_d
Tuning for shape 4x67108864*(4x4^13)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^5)_NN_d
Tuning for shape 4x67108864*(4x4^6)_NN_d
Tuning for shape 4x67108864*(4x4^7)_NN_d
Tuning for shape 4x67108864*(4x4^8)_NN_d
Tuning for shape 4x67108864*(4x4^9)_NN_d
Tuning for shape 4x67108864*(4x4^10)_NN_d
Tuning for shape 4x67108864*(4x4^11)_NN_d
Tuning for shape 4x67108864*(4x4^12)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^5)_NN_d
Tuning for shape 4x67108864*(4x4^6)_NN_d
Tuning for shape 4x67108864*(4x4^7)_NN_d
Tuning for shape 4x67108864*(4x4^8)_NN_d
Tuning for shape 4x67108864*(4x4^9)_NN_d
Tuning for shape 4x67108864*(4x4^10)_NN_d
Tuning for shape 4x67108864*(4x4^11)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^5)_NN_d
Tuning for shape 4x67108864*(4x4^6)_NN_d
Tuning for shape 4x67108864*(4x4^7)_NN_d
Tuning for shape 4x67108864*(4x4^8)_NN_d
Tuning for shape 4x67108864*(4x4^9)_NN_d
Tuning for shape 4x67108864*(4x4^10)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^5)_NN_d
Tuning for shape 4x67108864*(4x4^6)_NN_d
Tuning for shape 4x67108864*(4x4^7)_NN_d
Tuning for shape 4x67108864*(4x4^8)_NN_d
Tuning for shape 4x67108864*(4x4^9)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^5)_NN_d
Tuning for shape 4x67108864*(4x4^6)_NN_d
Tuning for shape 4x67108864*(4x4^7)_NN_d
Tuning for shape 4x67108864*(4x4^8)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^5)_NN_d
Tuning for shape 4x67108864*(4x4^6)_NN_d
Tuning for shape 4x67108864*(4x4^7)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^5)_NN_d
Tuning for shape 4x67108864*(4x4^6)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^5)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^4)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^3)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Tuning for shape 4x67108864*(4x4^2)_NN_d
Tuning for shape 4x67108864*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x4^13  &  1.000 & 1.000 & 71.965 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 14 -p 4 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 268435456] with F_0 [4, 4] x F_1 [4, 4] x F_2 [4, 4] x F_3 [4, 4] x F_4 [4, 4] x F_5 [4, 4] x F_6 [4, 4] x F_7 [4, 4] x F_8 [4, 4] x F_9 [4, 4] x F_10 [4, 4] x F_11 [4, 4] x F_12 [4, 4] x F_13 [4, 4] x to produce Y[1, 268435456]
Matmul: 1 x 268435456 x 268435456, Num KP Factors: 14
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^6)_NN_d
Tuning for shape 1x268435456*(4x4^7)_NN_d
Tuning for shape 1x268435456*(4x4^8)_NN_d
Tuning for shape 1x268435456*(4x4^9)_NN_d
Tuning for shape 1x268435456*(4x4^10)_NN_d
Tuning for shape 1x268435456*(4x4^11)_NN_d
Tuning for shape 1x268435456*(4x4^12)_NN_d
Tuning for shape 1x268435456*(4x4^13)_NN_d
Tuning for shape 1x268435456*(4x4^14)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^6)_NN_d
Tuning for shape 1x268435456*(4x4^7)_NN_d
Tuning for shape 1x268435456*(4x4^8)_NN_d
Tuning for shape 1x268435456*(4x4^9)_NN_d
Tuning for shape 1x268435456*(4x4^10)_NN_d
Tuning for shape 1x268435456*(4x4^11)_NN_d
Tuning for shape 1x268435456*(4x4^12)_NN_d
Tuning for shape 1x268435456*(4x4^13)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^6)_NN_d
Tuning for shape 1x268435456*(4x4^7)_NN_d
Tuning for shape 1x268435456*(4x4^8)_NN_d
Tuning for shape 1x268435456*(4x4^9)_NN_d
Tuning for shape 1x268435456*(4x4^10)_NN_d
Tuning for shape 1x268435456*(4x4^11)_NN_d
Tuning for shape 1x268435456*(4x4^12)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^6)_NN_d
Tuning for shape 1x268435456*(4x4^7)_NN_d
Tuning for shape 1x268435456*(4x4^8)_NN_d
Tuning for shape 1x268435456*(4x4^9)_NN_d
Tuning for shape 1x268435456*(4x4^10)_NN_d
Tuning for shape 1x268435456*(4x4^11)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^6)_NN_d
Tuning for shape 1x268435456*(4x4^7)_NN_d
Tuning for shape 1x268435456*(4x4^8)_NN_d
Tuning for shape 1x268435456*(4x4^9)_NN_d
Tuning for shape 1x268435456*(4x4^10)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^6)_NN_d
Tuning for shape 1x268435456*(4x4^7)_NN_d
Tuning for shape 1x268435456*(4x4^8)_NN_d
Tuning for shape 1x268435456*(4x4^9)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^6)_NN_d
Tuning for shape 1x268435456*(4x4^7)_NN_d
Tuning for shape 1x268435456*(4x4^8)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^6)_NN_d
Tuning for shape 1x268435456*(4x4^7)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^6)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^5)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^4)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^3)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Tuning for shape 1x268435456*(4x4^2)_NN_d
Tuning for shape 1x268435456*(4x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x4^14  &  1.000 & 1.000 & 72.971 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [4, 8] x to produce Y[1, 8]
Matmul: 1 x 8 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x8^1  &  1.000 & 1.000 & 0.000 & 4103.221
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [4, 8] x to produce Y[4, 8]
Matmul: 4 x 8 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x8^1  &  1.000 & 1.000 & 0.001 & 1196.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [4, 8] x to produce Y[16, 8]
Matmul: 16 x 8 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x8^1  &  1.000 & 1.000 & 0.003 & 300.840
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [4, 8] x to produce Y[64, 8]
Matmul: 64 x 8 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x8^1  &  1.000 & 1.000 & 0.014 & 72.800
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [4, 8] x to produce Y[256, 8]
Matmul: 256 x 8 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x8^1  &  1.000 & 1.000 & 0.053 & 18.763
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [4, 8] x to produce Y[1024, 8]
Matmul: 1024 x 8 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x8^1  &  1.000 & 1.000 & 0.207 & 4.827
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [4, 8] x F_1 [4, 8] x to produce Y[1, 64]
Matmul: 1 x 64 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32*(4x8^1)_NN_d
Tuning for shape 1x16*(4x8^2)_NN_d
Tuning for shape 1x16*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x8^2  &  1.000 & 1.000 & 0.002 & 541.393
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [4, 8] x F_1 [4, 8] x to produce Y[4, 64]
Matmul: 4 x 64 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32*(4x8^1)_NN_d
Tuning for shape 4x16*(4x8^2)_NN_d
Tuning for shape 4x16*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x8^2  &  1.000 & 1.000 & 0.007 & 144.456
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [4, 8] x F_1 [4, 8] x to produce Y[16, 64]
Matmul: 16 x 64 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32*(4x8^1)_NN_d
Tuning for shape 16x16*(4x8^2)_NN_d
Tuning for shape 16x16*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x8^2  &  1.000 & 1.000 & 0.027 & 37.519
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [4, 8] x F_1 [4, 8] x to produce Y[64, 64]
Matmul: 64 x 64 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32*(4x8^1)_NN_d
Tuning for shape 64x16*(4x8^2)_NN_d
Tuning for shape 64x16*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x8^2  &  1.000 & 1.000 & 0.109 & 9.189
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [4, 8] x F_1 [4, 8] x to produce Y[256, 64]
Matmul: 256 x 64 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32*(4x8^1)_NN_d
Tuning for shape 256x16*(4x8^2)_NN_d
Tuning for shape 256x16*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x8^2  &  1.000 & 1.000 & 0.429 & 2.330
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [4, 8] x F_1 [4, 8] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32*(4x8^1)_NN_d
Tuning for shape 1024x16*(4x8^2)_NN_d
Tuning for shape 1024x16*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x8^2  &  1.000 & 1.000 & 1.754 & 0.570
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x to produce Y[1, 512]
Matmul: 1 x 512 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(4x8^1)_NN_d
Tuning for shape 1x128*(4x8^2)_NN_d
Tuning for shape 1x64*(4x8^3)_NN_d
Tuning for shape 1x128*(4x8^1)_NN_d
Tuning for shape 1x64*(4x8^2)_NN_d
Tuning for shape 1x64*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x8^3  &  1.000 & 1.000 & 0.014 & 72.399
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x to produce Y[4, 512]
Matmul: 4 x 512 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(4x8^1)_NN_d
Tuning for shape 4x128*(4x8^2)_NN_d
Tuning for shape 4x64*(4x8^3)_NN_d
Tuning for shape 4x128*(4x8^1)_NN_d
Tuning for shape 4x64*(4x8^2)_NN_d
Tuning for shape 4x64*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x8^3  &  1.000 & 1.000 & 0.052 & 19.317
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x to produce Y[16, 512]
Matmul: 16 x 512 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(4x8^1)_NN_d
Tuning for shape 16x128*(4x8^2)_NN_d
Tuning for shape 16x64*(4x8^3)_NN_d
Tuning for shape 16x128*(4x8^1)_NN_d
Tuning for shape 16x64*(4x8^2)_NN_d
Tuning for shape 16x64*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x8^3  &  1.000 & 1.000 & 0.207 & 4.825
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x to produce Y[64, 512]
Matmul: 64 x 512 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(4x8^1)_NN_d
Tuning for shape 64x128*(4x8^2)_NN_d
Tuning for shape 64x64*(4x8^3)_NN_d
Tuning for shape 64x128*(4x8^1)_NN_d
Tuning for shape 64x64*(4x8^2)_NN_d
Tuning for shape 64x64*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x8^3  &  1.000 & 1.000 & 0.844 & 1.185
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x to produce Y[256, 512]
Matmul: 256 x 512 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(4x8^1)_NN_d
Tuning for shape 256x128*(4x8^2)_NN_d
Tuning for shape 256x64*(4x8^3)_NN_d
Tuning for shape 256x128*(4x8^1)_NN_d
Tuning for shape 256x64*(4x8^2)_NN_d
Tuning for shape 256x64*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x8^3  &  1.000 & 1.000 & 3.255 & 0.307
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x to produce Y[1024, 512]
Matmul: 1024 x 512 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x256*(4x8^1)_NN_d
Tuning for shape 1024x128*(4x8^2)_NN_d
Tuning for shape 1024x64*(4x8^3)_NN_d
Tuning for shape 1024x128*(4x8^1)_NN_d
Tuning for shape 1024x64*(4x8^2)_NN_d
Tuning for shape 1024x64*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x8^3  &  1.000 & 1.000 & 13.639 & 0.073
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2048*(4x8^1)_NN_d
Tuning for shape 1x1024*(4x8^2)_NN_d
Tuning for shape 1x512*(4x8^3)_NN_d
Tuning for shape 1x256*(4x8^4)_NN_d
Tuning for shape 1x1024*(4x8^1)_NN_d
Tuning for shape 1x512*(4x8^2)_NN_d
Tuning for shape 1x256*(4x8^3)_NN_d
Tuning for shape 1x512*(4x8^1)_NN_d
Tuning for shape 1x256*(4x8^2)_NN_d
Tuning for shape 1x256*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x8^4  &  1.000 & 1.000 & 0.100 & 10.039
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2048*(4x8^1)_NN_d
Tuning for shape 4x1024*(4x8^2)_NN_d
Tuning for shape 4x512*(4x8^3)_NN_d
Tuning for shape 4x256*(4x8^4)_NN_d
Tuning for shape 4x1024*(4x8^1)_NN_d
Tuning for shape 4x512*(4x8^2)_NN_d
Tuning for shape 4x256*(4x8^3)_NN_d
Tuning for shape 4x512*(4x8^1)_NN_d
Tuning for shape 4x256*(4x8^2)_NN_d
Tuning for shape 4x256*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x8^4  &  1.000 & 1.000 & 0.377 & 2.654
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2048*(4x8^1)_NN_d
Tuning for shape 16x1024*(4x8^2)_NN_d
Tuning for shape 16x512*(4x8^3)_NN_d
Tuning for shape 16x256*(4x8^4)_NN_d
Tuning for shape 16x1024*(4x8^1)_NN_d
Tuning for shape 16x512*(4x8^2)_NN_d
Tuning for shape 16x256*(4x8^3)_NN_d
Tuning for shape 16x512*(4x8^1)_NN_d
Tuning for shape 16x256*(4x8^2)_NN_d
Tuning for shape 16x256*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x8^4  &  1.000 & 1.000 & 1.451 & 0.689
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2048*(4x8^1)_NN_d
Tuning for shape 64x1024*(4x8^2)_NN_d
Tuning for shape 64x512*(4x8^3)_NN_d
Tuning for shape 64x256*(4x8^4)_NN_d
Tuning for shape 64x1024*(4x8^1)_NN_d
Tuning for shape 64x512*(4x8^2)_NN_d
Tuning for shape 64x256*(4x8^3)_NN_d
Tuning for shape 64x512*(4x8^1)_NN_d
Tuning for shape 64x256*(4x8^2)_NN_d
Tuning for shape 64x256*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x8^4  &  1.000 & 1.000 & 6.031 & 0.166
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2048*(4x8^1)_NN_d
Tuning for shape 256x1024*(4x8^2)_NN_d
Tuning for shape 256x512*(4x8^3)_NN_d
Tuning for shape 256x256*(4x8^4)_NN_d
Tuning for shape 256x1024*(4x8^1)_NN_d
Tuning for shape 256x512*(4x8^2)_NN_d
Tuning for shape 256x256*(4x8^3)_NN_d
Tuning for shape 256x512*(4x8^1)_NN_d
Tuning for shape 256x256*(4x8^2)_NN_d
Tuning for shape 256x256*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x8^4  &  1.000 & 1.000 & 23.010 & 0.043
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 256] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2048*(4x8^1)_NN_d
Tuning for shape 1024x1024*(4x8^2)_NN_d
Tuning for shape 1024x512*(4x8^3)_NN_d
Tuning for shape 1024x256*(4x8^4)_NN_d
Tuning for shape 1024x1024*(4x8^1)_NN_d
Tuning for shape 1024x512*(4x8^2)_NN_d
Tuning for shape 1024x256*(4x8^3)_NN_d
Tuning for shape 1024x512*(4x8^1)_NN_d
Tuning for shape 1024x256*(4x8^2)_NN_d
Tuning for shape 1024x256*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x8^4  &  1.000 & 1.000 & 82.436 & 0.012
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1024] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x to produce Y[1, 32768]
Matmul: 1 x 32768 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16384*(4x8^1)_NN_d
Tuning for shape 1x8192*(4x8^2)_NN_d
Tuning for shape 1x4096*(4x8^3)_NN_d
Tuning for shape 1x2048*(4x8^4)_NN_d
Tuning for shape 1x1024*(4x8^5)_NN_d
Tuning for shape 1x8192*(4x8^1)_NN_d
Tuning for shape 1x4096*(4x8^2)_NN_d
Tuning for shape 1x2048*(4x8^3)_NN_d
Tuning for shape 1x1024*(4x8^4)_NN_d
Tuning for shape 1x4096*(4x8^1)_NN_d
Tuning for shape 1x2048*(4x8^2)_NN_d
Tuning for shape 1x1024*(4x8^3)_NN_d
Tuning for shape 1x2048*(4x8^1)_NN_d
Tuning for shape 1x1024*(4x8^2)_NN_d
Tuning for shape 1x1024*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x8^5  &  1.000 & 1.000 & 0.726 & 1.378
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1024] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x to produce Y[4, 32768]
Matmul: 4 x 32768 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16384*(4x8^1)_NN_d
Tuning for shape 4x8192*(4x8^2)_NN_d
Tuning for shape 4x4096*(4x8^3)_NN_d
Tuning for shape 4x2048*(4x8^4)_NN_d
Tuning for shape 4x1024*(4x8^5)_NN_d
Tuning for shape 4x8192*(4x8^1)_NN_d
Tuning for shape 4x4096*(4x8^2)_NN_d
Tuning for shape 4x2048*(4x8^3)_NN_d
Tuning for shape 4x1024*(4x8^4)_NN_d
Tuning for shape 4x4096*(4x8^1)_NN_d
Tuning for shape 4x2048*(4x8^2)_NN_d
Tuning for shape 4x1024*(4x8^3)_NN_d
Tuning for shape 4x2048*(4x8^1)_NN_d
Tuning for shape 4x1024*(4x8^2)_NN_d
Tuning for shape 4x1024*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x8^5  &  1.000 & 1.000 & 2.672 & 0.374
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1024] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x to produce Y[16, 32768]
Matmul: 16 x 32768 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16384*(4x8^1)_NN_d
Tuning for shape 16x8192*(4x8^2)_NN_d
Tuning for shape 16x4096*(4x8^3)_NN_d
Tuning for shape 16x2048*(4x8^4)_NN_d
Tuning for shape 16x1024*(4x8^5)_NN_d
Tuning for shape 16x8192*(4x8^1)_NN_d
Tuning for shape 16x4096*(4x8^2)_NN_d
Tuning for shape 16x2048*(4x8^3)_NN_d
Tuning for shape 16x1024*(4x8^4)_NN_d
Tuning for shape 16x4096*(4x8^1)_NN_d
Tuning for shape 16x2048*(4x8^2)_NN_d
Tuning for shape 16x1024*(4x8^3)_NN_d
Tuning for shape 16x2048*(4x8^1)_NN_d
Tuning for shape 16x1024*(4x8^2)_NN_d
Tuning for shape 16x1024*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x8^5  &  1.000 & 1.000 & 10.724 & 0.093
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1024] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x to produce Y[64, 32768]
Matmul: 64 x 32768 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16384*(4x8^1)_NN_d
Tuning for shape 64x8192*(4x8^2)_NN_d
Tuning for shape 64x4096*(4x8^3)_NN_d
Tuning for shape 64x2048*(4x8^4)_NN_d
Tuning for shape 64x1024*(4x8^5)_NN_d
Tuning for shape 64x8192*(4x8^1)_NN_d
Tuning for shape 64x4096*(4x8^2)_NN_d
Tuning for shape 64x2048*(4x8^3)_NN_d
Tuning for shape 64x1024*(4x8^4)_NN_d
Tuning for shape 64x4096*(4x8^1)_NN_d
Tuning for shape 64x2048*(4x8^2)_NN_d
Tuning for shape 64x1024*(4x8^3)_NN_d
Tuning for shape 64x2048*(4x8^1)_NN_d
Tuning for shape 64x1024*(4x8^2)_NN_d
Tuning for shape 64x1024*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x8^5  &  1.000 & 1.000 & 40.363 & 0.025
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1024] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x to produce Y[256, 32768]
Matmul: 256 x 32768 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16384*(4x8^1)_NN_d
Tuning for shape 256x8192*(4x8^2)_NN_d
Tuning for shape 256x4096*(4x8^3)_NN_d
Tuning for shape 256x2048*(4x8^4)_NN_d
Tuning for shape 256x1024*(4x8^5)_NN_d
Tuning for shape 256x8192*(4x8^1)_NN_d
Tuning for shape 256x4096*(4x8^2)_NN_d
Tuning for shape 256x2048*(4x8^3)_NN_d
Tuning for shape 256x1024*(4x8^4)_NN_d
Tuning for shape 256x4096*(4x8^1)_NN_d
Tuning for shape 256x2048*(4x8^2)_NN_d
Tuning for shape 256x1024*(4x8^3)_NN_d
Tuning for shape 256x2048*(4x8^1)_NN_d
Tuning for shape 256x1024*(4x8^2)_NN_d
Tuning for shape 256x1024*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x8^5  &  1.000 & 1.000 & 106.475 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 5 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 1024] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x to produce Y[1024, 32768]
Matmul: 1024 x 32768 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16384*(4x8^1)_NN_d
Tuning for shape 1024x8192*(4x8^2)_NN_d
Tuning for shape 1024x4096*(4x8^3)_NN_d
Tuning for shape 1024x2048*(4x8^4)_NN_d
Tuning for shape 1024x1024*(4x8^5)_NN_d
Tuning for shape 1024x8192*(4x8^1)_NN_d
Tuning for shape 1024x4096*(4x8^2)_NN_d
Tuning for shape 1024x2048*(4x8^3)_NN_d
Tuning for shape 1024x1024*(4x8^4)_NN_d
Tuning for shape 1024x4096*(4x8^1)_NN_d
Tuning for shape 1024x2048*(4x8^2)_NN_d
Tuning for shape 1024x1024*(4x8^3)_NN_d
Tuning for shape 1024x2048*(4x8^1)_NN_d
Tuning for shape 1024x1024*(4x8^2)_NN_d
Tuning for shape 1024x1024*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x8^5  &  1.000 & 1.000 & 112.286 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x131072*(4x8^1)_NN_d
Tuning for shape 1x65536*(4x8^2)_NN_d
Tuning for shape 1x32768*(4x8^3)_NN_d
Tuning for shape 1x16384*(4x8^4)_NN_d
Tuning for shape 1x8192*(4x8^5)_NN_d
Tuning for shape 1x4096*(4x8^6)_NN_d
Tuning for shape 1x65536*(4x8^1)_NN_d
Tuning for shape 1x32768*(4x8^2)_NN_d
Tuning for shape 1x16384*(4x8^3)_NN_d
Tuning for shape 1x8192*(4x8^4)_NN_d
Tuning for shape 1x4096*(4x8^5)_NN_d
Tuning for shape 1x32768*(4x8^1)_NN_d
Tuning for shape 1x16384*(4x8^2)_NN_d
Tuning for shape 1x8192*(4x8^3)_NN_d
Tuning for shape 1x4096*(4x8^4)_NN_d
Tuning for shape 1x16384*(4x8^1)_NN_d
Tuning for shape 1x8192*(4x8^2)_NN_d
Tuning for shape 1x4096*(4x8^3)_NN_d
Tuning for shape 1x8192*(4x8^1)_NN_d
Tuning for shape 1x4096*(4x8^2)_NN_d
Tuning for shape 1x4096*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x8^6  &  1.000 & 1.000 & 5.117 & 0.195
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x131072*(4x8^1)_NN_d
Tuning for shape 4x65536*(4x8^2)_NN_d
Tuning for shape 4x32768*(4x8^3)_NN_d
Tuning for shape 4x16384*(4x8^4)_NN_d
Tuning for shape 4x8192*(4x8^5)_NN_d
Tuning for shape 4x4096*(4x8^6)_NN_d
Tuning for shape 4x65536*(4x8^1)_NN_d
Tuning for shape 4x32768*(4x8^2)_NN_d
Tuning for shape 4x16384*(4x8^3)_NN_d
Tuning for shape 4x8192*(4x8^4)_NN_d
Tuning for shape 4x4096*(4x8^5)_NN_d
Tuning for shape 4x32768*(4x8^1)_NN_d
Tuning for shape 4x16384*(4x8^2)_NN_d
Tuning for shape 4x8192*(4x8^3)_NN_d
Tuning for shape 4x4096*(4x8^4)_NN_d
Tuning for shape 4x16384*(4x8^1)_NN_d
Tuning for shape 4x8192*(4x8^2)_NN_d
Tuning for shape 4x4096*(4x8^3)_NN_d
Tuning for shape 4x8192*(4x8^1)_NN_d
Tuning for shape 4x4096*(4x8^2)_NN_d
Tuning for shape 4x4096*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x8^6  &  1.000 & 1.000 & 19.707 & 0.051
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x131072*(4x8^1)_NN_d
Tuning for shape 16x65536*(4x8^2)_NN_d
Tuning for shape 16x32768*(4x8^3)_NN_d
Tuning for shape 16x16384*(4x8^4)_NN_d
Tuning for shape 16x8192*(4x8^5)_NN_d
Tuning for shape 16x4096*(4x8^6)_NN_d
Tuning for shape 16x65536*(4x8^1)_NN_d
Tuning for shape 16x32768*(4x8^2)_NN_d
Tuning for shape 16x16384*(4x8^3)_NN_d
Tuning for shape 16x8192*(4x8^4)_NN_d
Tuning for shape 16x4096*(4x8^5)_NN_d
Tuning for shape 16x32768*(4x8^1)_NN_d
Tuning for shape 16x16384*(4x8^2)_NN_d
Tuning for shape 16x8192*(4x8^3)_NN_d
Tuning for shape 16x4096*(4x8^4)_NN_d
Tuning for shape 16x16384*(4x8^1)_NN_d
Tuning for shape 16x8192*(4x8^2)_NN_d
Tuning for shape 16x4096*(4x8^3)_NN_d
Tuning for shape 16x8192*(4x8^1)_NN_d
Tuning for shape 16x4096*(4x8^2)_NN_d
Tuning for shape 16x4096*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x8^6  &  1.000 & 1.000 & 74.045 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 6 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x to produce Y[64, 262144]
Matmul: 64 x 262144 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x131072*(4x8^1)_NN_d
Tuning for shape 64x65536*(4x8^2)_NN_d
Tuning for shape 64x32768*(4x8^3)_NN_d
Tuning for shape 64x16384*(4x8^4)_NN_d
Tuning for shape 64x8192*(4x8^5)_NN_d
Tuning for shape 64x4096*(4x8^6)_NN_d
Tuning for shape 64x65536*(4x8^1)_NN_d
Tuning for shape 64x32768*(4x8^2)_NN_d
Tuning for shape 64x16384*(4x8^3)_NN_d
Tuning for shape 64x8192*(4x8^4)_NN_d
Tuning for shape 64x4096*(4x8^5)_NN_d
Tuning for shape 64x32768*(4x8^1)_NN_d
Tuning for shape 64x16384*(4x8^2)_NN_d
Tuning for shape 64x8192*(4x8^3)_NN_d
Tuning for shape 64x4096*(4x8^4)_NN_d
Tuning for shape 64x16384*(4x8^1)_NN_d
Tuning for shape 64x8192*(4x8^2)_NN_d
Tuning for shape 64x4096*(4x8^3)_NN_d
Tuning for shape 64x8192*(4x8^1)_NN_d
Tuning for shape 64x4096*(4x8^2)_NN_d
Tuning for shape 64x4096*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x8^6  &  1.000 & 1.000 & 116.750 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 6 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x to produce Y[256, 262144]
Matmul: 256 x 262144 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x131072*(4x8^1)_NN_d
Tuning for shape 256x65536*(4x8^2)_NN_d
Tuning for shape 256x32768*(4x8^3)_NN_d
Tuning for shape 256x16384*(4x8^4)_NN_d
Tuning for shape 256x8192*(4x8^5)_NN_d
Tuning for shape 256x4096*(4x8^6)_NN_d
Tuning for shape 256x65536*(4x8^1)_NN_d
Tuning for shape 256x32768*(4x8^2)_NN_d
Tuning for shape 256x16384*(4x8^3)_NN_d
Tuning for shape 256x8192*(4x8^4)_NN_d
Tuning for shape 256x4096*(4x8^5)_NN_d
Tuning for shape 256x32768*(4x8^1)_NN_d
Tuning for shape 256x16384*(4x8^2)_NN_d
Tuning for shape 256x8192*(4x8^3)_NN_d
Tuning for shape 256x4096*(4x8^4)_NN_d
Tuning for shape 256x16384*(4x8^1)_NN_d
Tuning for shape 256x8192*(4x8^2)_NN_d
Tuning for shape 256x4096*(4x8^3)_NN_d
Tuning for shape 256x8192*(4x8^1)_NN_d
Tuning for shape 256x4096*(4x8^2)_NN_d
Tuning for shape 256x4096*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x8^6  &  1.000 & 1.000 & 120.344 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 6 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x to produce Y[1024, 262144]
Matmul: 1024 x 262144 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x131072*(4x8^1)_NN_d
Tuning for shape 1024x65536*(4x8^2)_NN_d
Tuning for shape 1024x32768*(4x8^3)_NN_d
Tuning for shape 1024x16384*(4x8^4)_NN_d
Tuning for shape 1024x8192*(4x8^5)_NN_d
Tuning for shape 1024x4096*(4x8^6)_NN_d
Tuning for shape 1024x65536*(4x8^1)_NN_d
Tuning for shape 1024x32768*(4x8^2)_NN_d
Tuning for shape 1024x16384*(4x8^3)_NN_d
Tuning for shape 1024x8192*(4x8^4)_NN_d
Tuning for shape 1024x4096*(4x8^5)_NN_d
Tuning for shape 1024x32768*(4x8^1)_NN_d
Tuning for shape 1024x16384*(4x8^2)_NN_d
Tuning for shape 1024x8192*(4x8^3)_NN_d
Tuning for shape 1024x4096*(4x8^4)_NN_d
Tuning for shape 1024x16384*(4x8^1)_NN_d
Tuning for shape 1024x8192*(4x8^2)_NN_d
Tuning for shape 1024x4096*(4x8^3)_NN_d
Tuning for shape 1024x8192*(4x8^1)_NN_d
Tuning for shape 1024x4096*(4x8^2)_NN_d
Tuning for shape 1024x4096*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x8^6  &  1.000 & 1.000 & 121.761 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16384] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x to produce Y[1, 2097152]
Matmul: 1 x 2097152 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1048576*(4x8^1)_NN_d
Tuning for shape 1x524288*(4x8^2)_NN_d
Tuning for shape 1x262144*(4x8^3)_NN_d
Tuning for shape 1x131072*(4x8^4)_NN_d
Tuning for shape 1x65536*(4x8^5)_NN_d
Tuning for shape 1x32768*(4x8^6)_NN_d
Tuning for shape 1x16384*(4x8^7)_NN_d
Tuning for shape 1x524288*(4x8^1)_NN_d
Tuning for shape 1x262144*(4x8^2)_NN_d
Tuning for shape 1x131072*(4x8^3)_NN_d
Tuning for shape 1x65536*(4x8^4)_NN_d
Tuning for shape 1x32768*(4x8^5)_NN_d
Tuning for shape 1x16384*(4x8^6)_NN_d
Tuning for shape 1x262144*(4x8^1)_NN_d
Tuning for shape 1x131072*(4x8^2)_NN_d
Tuning for shape 1x65536*(4x8^3)_NN_d
Tuning for shape 1x32768*(4x8^4)_NN_d
Tuning for shape 1x16384*(4x8^5)_NN_d
Tuning for shape 1x131072*(4x8^1)_NN_d
Tuning for shape 1x65536*(4x8^2)_NN_d
Tuning for shape 1x32768*(4x8^3)_NN_d
Tuning for shape 1x16384*(4x8^4)_NN_d
Tuning for shape 1x65536*(4x8^1)_NN_d
Tuning for shape 1x32768*(4x8^2)_NN_d
Tuning for shape 1x16384*(4x8^3)_NN_d
Tuning for shape 1x32768*(4x8^1)_NN_d
Tuning for shape 1x16384*(4x8^2)_NN_d
Tuning for shape 1x16384*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x8^7  &  1.000 & 1.000 & 35.728 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 7 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16384] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x to produce Y[4, 2097152]
Matmul: 4 x 2097152 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1048576*(4x8^1)_NN_d
Tuning for shape 4x524288*(4x8^2)_NN_d
Tuning for shape 4x262144*(4x8^3)_NN_d
Tuning for shape 4x131072*(4x8^4)_NN_d
Tuning for shape 4x65536*(4x8^5)_NN_d
Tuning for shape 4x32768*(4x8^6)_NN_d
Tuning for shape 4x16384*(4x8^7)_NN_d
Tuning for shape 4x524288*(4x8^1)_NN_d
Tuning for shape 4x262144*(4x8^2)_NN_d
Tuning for shape 4x131072*(4x8^3)_NN_d
Tuning for shape 4x65536*(4x8^4)_NN_d
Tuning for shape 4x32768*(4x8^5)_NN_d
Tuning for shape 4x16384*(4x8^6)_NN_d
Tuning for shape 4x262144*(4x8^1)_NN_d
Tuning for shape 4x131072*(4x8^2)_NN_d
Tuning for shape 4x65536*(4x8^3)_NN_d
Tuning for shape 4x32768*(4x8^4)_NN_d
Tuning for shape 4x16384*(4x8^5)_NN_d
Tuning for shape 4x131072*(4x8^1)_NN_d
Tuning for shape 4x65536*(4x8^2)_NN_d
Tuning for shape 4x32768*(4x8^3)_NN_d
Tuning for shape 4x16384*(4x8^4)_NN_d
Tuning for shape 4x65536*(4x8^1)_NN_d
Tuning for shape 4x32768*(4x8^2)_NN_d
Tuning for shape 4x16384*(4x8^3)_NN_d
Tuning for shape 4x32768*(4x8^1)_NN_d
Tuning for shape 4x16384*(4x8^2)_NN_d
Tuning for shape 4x16384*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x8^7  &  1.000 & 1.000 & 113.031 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 7 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16384] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x to produce Y[16, 2097152]
Matmul: 16 x 2097152 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1048576*(4x8^1)_NN_d
Tuning for shape 16x524288*(4x8^2)_NN_d
Tuning for shape 16x262144*(4x8^3)_NN_d
Tuning for shape 16x131072*(4x8^4)_NN_d
Tuning for shape 16x65536*(4x8^5)_NN_d
Tuning for shape 16x32768*(4x8^6)_NN_d
Tuning for shape 16x16384*(4x8^7)_NN_d
Tuning for shape 16x524288*(4x8^1)_NN_d
Tuning for shape 16x262144*(4x8^2)_NN_d
Tuning for shape 16x131072*(4x8^3)_NN_d
Tuning for shape 16x65536*(4x8^4)_NN_d
Tuning for shape 16x32768*(4x8^5)_NN_d
Tuning for shape 16x16384*(4x8^6)_NN_d
Tuning for shape 16x262144*(4x8^1)_NN_d
Tuning for shape 16x131072*(4x8^2)_NN_d
Tuning for shape 16x65536*(4x8^3)_NN_d
Tuning for shape 16x32768*(4x8^4)_NN_d
Tuning for shape 16x16384*(4x8^5)_NN_d
Tuning for shape 16x131072*(4x8^1)_NN_d
Tuning for shape 16x65536*(4x8^2)_NN_d
Tuning for shape 16x32768*(4x8^3)_NN_d
Tuning for shape 16x16384*(4x8^4)_NN_d
Tuning for shape 16x65536*(4x8^1)_NN_d
Tuning for shape 16x32768*(4x8^2)_NN_d
Tuning for shape 16x16384*(4x8^3)_NN_d
Tuning for shape 16x32768*(4x8^1)_NN_d
Tuning for shape 16x16384*(4x8^2)_NN_d
Tuning for shape 16x16384*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x8^7  &  1.000 & 1.000 & 120.612 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 7 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16384] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x to produce Y[64, 2097152]
Matmul: 64 x 2097152 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1048576*(4x8^1)_NN_d
Tuning for shape 64x524288*(4x8^2)_NN_d
Tuning for shape 64x262144*(4x8^3)_NN_d
Tuning for shape 64x131072*(4x8^4)_NN_d
Tuning for shape 64x65536*(4x8^5)_NN_d
Tuning for shape 64x32768*(4x8^6)_NN_d
Tuning for shape 64x16384*(4x8^7)_NN_d
Tuning for shape 64x524288*(4x8^1)_NN_d
Tuning for shape 64x262144*(4x8^2)_NN_d
Tuning for shape 64x131072*(4x8^3)_NN_d
Tuning for shape 64x65536*(4x8^4)_NN_d
Tuning for shape 64x32768*(4x8^5)_NN_d
Tuning for shape 64x16384*(4x8^6)_NN_d
Tuning for shape 64x262144*(4x8^1)_NN_d
Tuning for shape 64x131072*(4x8^2)_NN_d
Tuning for shape 64x65536*(4x8^3)_NN_d
Tuning for shape 64x32768*(4x8^4)_NN_d
Tuning for shape 64x16384*(4x8^5)_NN_d
Tuning for shape 64x131072*(4x8^1)_NN_d
Tuning for shape 64x65536*(4x8^2)_NN_d
Tuning for shape 64x32768*(4x8^3)_NN_d
Tuning for shape 64x16384*(4x8^4)_NN_d
Tuning for shape 64x65536*(4x8^1)_NN_d
Tuning for shape 64x32768*(4x8^2)_NN_d
Tuning for shape 64x16384*(4x8^3)_NN_d
Tuning for shape 64x32768*(4x8^1)_NN_d
Tuning for shape 64x16384*(4x8^2)_NN_d
Tuning for shape 64x16384*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x8^7  &  1.000 & 1.000 & 121.488 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 7 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16384] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x to produce Y[256, 2097152]
Matmul: 256 x 2097152 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1048576*(4x8^1)_NN_d
Tuning for shape 256x524288*(4x8^2)_NN_d
Tuning for shape 256x262144*(4x8^3)_NN_d
Tuning for shape 256x131072*(4x8^4)_NN_d
Tuning for shape 256x65536*(4x8^5)_NN_d
Tuning for shape 256x32768*(4x8^6)_NN_d
Tuning for shape 256x16384*(4x8^7)_NN_d
Tuning for shape 256x524288*(4x8^1)_NN_d
Tuning for shape 256x262144*(4x8^2)_NN_d
Tuning for shape 256x131072*(4x8^3)_NN_d
Tuning for shape 256x65536*(4x8^4)_NN_d
Tuning for shape 256x32768*(4x8^5)_NN_d
Tuning for shape 256x16384*(4x8^6)_NN_d
Tuning for shape 256x262144*(4x8^1)_NN_d
Tuning for shape 256x131072*(4x8^2)_NN_d
Tuning for shape 256x65536*(4x8^3)_NN_d
Tuning for shape 256x32768*(4x8^4)_NN_d
Tuning for shape 256x16384*(4x8^5)_NN_d
Tuning for shape 256x131072*(4x8^1)_NN_d
Tuning for shape 256x65536*(4x8^2)_NN_d
Tuning for shape 256x32768*(4x8^3)_NN_d
Tuning for shape 256x16384*(4x8^4)_NN_d
Tuning for shape 256x65536*(4x8^1)_NN_d
Tuning for shape 256x32768*(4x8^2)_NN_d
Tuning for shape 256x16384*(4x8^3)_NN_d
Tuning for shape 256x32768*(4x8^1)_NN_d
Tuning for shape 256x16384*(4x8^2)_NN_d
Tuning for shape 256x16384*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x8^7  &  1.000 & 1.000 & -1.000 & -1.000
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 8 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 65536] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x F_7 [4, 8] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8388608*(4x8^1)_NN_d
Tuning for shape 1x4194304*(4x8^2)_NN_d
Tuning for shape 1x2097152*(4x8^3)_NN_d
Tuning for shape 1x1048576*(4x8^4)_NN_d
Tuning for shape 1x524288*(4x8^5)_NN_d
Tuning for shape 1x262144*(4x8^6)_NN_d
Tuning for shape 1x131072*(4x8^7)_NN_d
Tuning for shape 1x65536*(4x8^8)_NN_d
Tuning for shape 1x4194304*(4x8^1)_NN_d
Tuning for shape 1x2097152*(4x8^2)_NN_d
Tuning for shape 1x1048576*(4x8^3)_NN_d
Tuning for shape 1x524288*(4x8^4)_NN_d
Tuning for shape 1x262144*(4x8^5)_NN_d
Tuning for shape 1x131072*(4x8^6)_NN_d
Tuning for shape 1x65536*(4x8^7)_NN_d
Tuning for shape 1x2097152*(4x8^1)_NN_d
Tuning for shape 1x1048576*(4x8^2)_NN_d
Tuning for shape 1x524288*(4x8^3)_NN_d
Tuning for shape 1x262144*(4x8^4)_NN_d
Tuning for shape 1x131072*(4x8^5)_NN_d
Tuning for shape 1x65536*(4x8^6)_NN_d
Tuning for shape 1x1048576*(4x8^1)_NN_d
Tuning for shape 1x524288*(4x8^2)_NN_d
Tuning for shape 1x262144*(4x8^3)_NN_d
Tuning for shape 1x131072*(4x8^4)_NN_d
Tuning for shape 1x65536*(4x8^5)_NN_d
Tuning for shape 1x524288*(4x8^1)_NN_d
Tuning for shape 1x262144*(4x8^2)_NN_d
Tuning for shape 1x131072*(4x8^3)_NN_d
Tuning for shape 1x65536*(4x8^4)_NN_d
Tuning for shape 1x262144*(4x8^1)_NN_d
Tuning for shape 1x131072*(4x8^2)_NN_d
Tuning for shape 1x65536*(4x8^3)_NN_d
Tuning for shape 1x131072*(4x8^1)_NN_d
Tuning for shape 1x65536*(4x8^2)_NN_d
Tuning for shape 1x65536*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x8^8  &  1.000 & 1.000 & 117.383 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 8 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 65536] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x F_7 [4, 8] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8388608*(4x8^1)_NN_d
Tuning for shape 4x4194304*(4x8^2)_NN_d
Tuning for shape 4x2097152*(4x8^3)_NN_d
Tuning for shape 4x1048576*(4x8^4)_NN_d
Tuning for shape 4x524288*(4x8^5)_NN_d
Tuning for shape 4x262144*(4x8^6)_NN_d
Tuning for shape 4x131072*(4x8^7)_NN_d
Tuning for shape 4x65536*(4x8^8)_NN_d
Tuning for shape 4x4194304*(4x8^1)_NN_d
Tuning for shape 4x2097152*(4x8^2)_NN_d
Tuning for shape 4x1048576*(4x8^3)_NN_d
Tuning for shape 4x524288*(4x8^4)_NN_d
Tuning for shape 4x262144*(4x8^5)_NN_d
Tuning for shape 4x131072*(4x8^6)_NN_d
Tuning for shape 4x65536*(4x8^7)_NN_d
Tuning for shape 4x2097152*(4x8^1)_NN_d
Tuning for shape 4x1048576*(4x8^2)_NN_d
Tuning for shape 4x524288*(4x8^3)_NN_d
Tuning for shape 4x262144*(4x8^4)_NN_d
Tuning for shape 4x131072*(4x8^5)_NN_d
Tuning for shape 4x65536*(4x8^6)_NN_d
Tuning for shape 4x1048576*(4x8^1)_NN_d
Tuning for shape 4x524288*(4x8^2)_NN_d
Tuning for shape 4x262144*(4x8^3)_NN_d
Tuning for shape 4x131072*(4x8^4)_NN_d
Tuning for shape 4x65536*(4x8^5)_NN_d
Tuning for shape 4x524288*(4x8^1)_NN_d
Tuning for shape 4x262144*(4x8^2)_NN_d
Tuning for shape 4x131072*(4x8^3)_NN_d
Tuning for shape 4x65536*(4x8^4)_NN_d
Tuning for shape 4x262144*(4x8^1)_NN_d
Tuning for shape 4x131072*(4x8^2)_NN_d
Tuning for shape 4x65536*(4x8^3)_NN_d
Tuning for shape 4x131072*(4x8^1)_NN_d
Tuning for shape 4x65536*(4x8^2)_NN_d
Tuning for shape 4x65536*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x8^8  &  1.000 & 1.000 & 121.468 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 8 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 65536] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x F_7 [4, 8] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 65536, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8388608*(4x8^1)_NN_d
Tuning for shape 16x4194304*(4x8^2)_NN_d
Tuning for shape 16x2097152*(4x8^3)_NN_d
Tuning for shape 16x1048576*(4x8^4)_NN_d
Tuning for shape 16x524288*(4x8^5)_NN_d
Tuning for shape 16x262144*(4x8^6)_NN_d
Tuning for shape 16x131072*(4x8^7)_NN_d
Tuning for shape 16x65536*(4x8^8)_NN_d
Tuning for shape 16x4194304*(4x8^1)_NN_d
Tuning for shape 16x2097152*(4x8^2)_NN_d
Tuning for shape 16x1048576*(4x8^3)_NN_d
Tuning for shape 16x524288*(4x8^4)_NN_d
Tuning for shape 16x262144*(4x8^5)_NN_d
Tuning for shape 16x131072*(4x8^6)_NN_d
Tuning for shape 16x65536*(4x8^7)_NN_d
Tuning for shape 16x2097152*(4x8^1)_NN_d
Tuning for shape 16x1048576*(4x8^2)_NN_d
Tuning for shape 16x524288*(4x8^3)_NN_d
Tuning for shape 16x262144*(4x8^4)_NN_d
Tuning for shape 16x131072*(4x8^5)_NN_d
Tuning for shape 16x65536*(4x8^6)_NN_d
Tuning for shape 16x1048576*(4x8^1)_NN_d
Tuning for shape 16x524288*(4x8^2)_NN_d
Tuning for shape 16x262144*(4x8^3)_NN_d
Tuning for shape 16x131072*(4x8^4)_NN_d
Tuning for shape 16x65536*(4x8^5)_NN_d
Tuning for shape 16x524288*(4x8^1)_NN_d
Tuning for shape 16x262144*(4x8^2)_NN_d
Tuning for shape 16x131072*(4x8^3)_NN_d
Tuning for shape 16x65536*(4x8^4)_NN_d
Tuning for shape 16x262144*(4x8^1)_NN_d
Tuning for shape 16x131072*(4x8^2)_NN_d
Tuning for shape 16x65536*(4x8^3)_NN_d
Tuning for shape 16x131072*(4x8^1)_NN_d
Tuning for shape 16x65536*(4x8^2)_NN_d
Tuning for shape 16x65536*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x8^8  &  1.000 & 1.000 & 122.397 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 9 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 262144] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x F_7 [4, 8] x F_8 [4, 8] x to produce Y[1, 134217728]
Matmul: 1 x 134217728 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x67108864*(4x8^1)_NN_d
Tuning for shape 1x33554432*(4x8^2)_NN_d
Tuning for shape 1x16777216*(4x8^3)_NN_d
Tuning for shape 1x8388608*(4x8^4)_NN_d
Tuning for shape 1x4194304*(4x8^5)_NN_d
Tuning for shape 1x2097152*(4x8^6)_NN_d
Tuning for shape 1x1048576*(4x8^7)_NN_d
Tuning for shape 1x524288*(4x8^8)_NN_d
Tuning for shape 1x262144*(4x8^9)_NN_d
Tuning for shape 1x33554432*(4x8^1)_NN_d
Tuning for shape 1x16777216*(4x8^2)_NN_d
Tuning for shape 1x8388608*(4x8^3)_NN_d
Tuning for shape 1x4194304*(4x8^4)_NN_d
Tuning for shape 1x2097152*(4x8^5)_NN_d
Tuning for shape 1x1048576*(4x8^6)_NN_d
Tuning for shape 1x524288*(4x8^7)_NN_d
Tuning for shape 1x262144*(4x8^8)_NN_d
Tuning for shape 1x16777216*(4x8^1)_NN_d
Tuning for shape 1x8388608*(4x8^2)_NN_d
Tuning for shape 1x4194304*(4x8^3)_NN_d
Tuning for shape 1x2097152*(4x8^4)_NN_d
Tuning for shape 1x1048576*(4x8^5)_NN_d
Tuning for shape 1x524288*(4x8^6)_NN_d
Tuning for shape 1x262144*(4x8^7)_NN_d
Tuning for shape 1x8388608*(4x8^1)_NN_d
Tuning for shape 1x4194304*(4x8^2)_NN_d
Tuning for shape 1x2097152*(4x8^3)_NN_d
Tuning for shape 1x1048576*(4x8^4)_NN_d
Tuning for shape 1x524288*(4x8^5)_NN_d
Tuning for shape 1x262144*(4x8^6)_NN_d
Tuning for shape 1x4194304*(4x8^1)_NN_d
Tuning for shape 1x2097152*(4x8^2)_NN_d
Tuning for shape 1x1048576*(4x8^3)_NN_d
Tuning for shape 1x524288*(4x8^4)_NN_d
Tuning for shape 1x262144*(4x8^5)_NN_d
Tuning for shape 1x2097152*(4x8^1)_NN_d
Tuning for shape 1x1048576*(4x8^2)_NN_d
Tuning for shape 1x524288*(4x8^3)_NN_d
Tuning for shape 1x262144*(4x8^4)_NN_d
Tuning for shape 1x1048576*(4x8^1)_NN_d
Tuning for shape 1x524288*(4x8^2)_NN_d
Tuning for shape 1x262144*(4x8^3)_NN_d
Tuning for shape 1x524288*(4x8^1)_NN_d
Tuning for shape 1x262144*(4x8^2)_NN_d
Tuning for shape 1x262144*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x8^9  &  1.000 & 1.000 & 121.953 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 9 -p 4 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 262144] with F_0 [4, 8] x F_1 [4, 8] x F_2 [4, 8] x F_3 [4, 8] x F_4 [4, 8] x F_5 [4, 8] x F_6 [4, 8] x F_7 [4, 8] x F_8 [4, 8] x to produce Y[4, 134217728]
Matmul: 4 x 134217728 x 262144, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x67108864*(4x8^1)_NN_d
Tuning for shape 4x33554432*(4x8^2)_NN_d
Tuning for shape 4x16777216*(4x8^3)_NN_d
Tuning for shape 4x8388608*(4x8^4)_NN_d
Tuning for shape 4x4194304*(4x8^5)_NN_d
Tuning for shape 4x2097152*(4x8^6)_NN_d
Tuning for shape 4x1048576*(4x8^7)_NN_d
Tuning for shape 4x524288*(4x8^8)_NN_d
Tuning for shape 4x262144*(4x8^9)_NN_d
Tuning for shape 4x33554432*(4x8^1)_NN_d
Tuning for shape 4x16777216*(4x8^2)_NN_d
Tuning for shape 4x8388608*(4x8^3)_NN_d
Tuning for shape 4x4194304*(4x8^4)_NN_d
Tuning for shape 4x2097152*(4x8^5)_NN_d
Tuning for shape 4x1048576*(4x8^6)_NN_d
Tuning for shape 4x524288*(4x8^7)_NN_d
Tuning for shape 4x262144*(4x8^8)_NN_d
Tuning for shape 4x16777216*(4x8^1)_NN_d
Tuning for shape 4x8388608*(4x8^2)_NN_d
Tuning for shape 4x4194304*(4x8^3)_NN_d
Tuning for shape 4x2097152*(4x8^4)_NN_d
Tuning for shape 4x1048576*(4x8^5)_NN_d
Tuning for shape 4x524288*(4x8^6)_NN_d
Tuning for shape 4x262144*(4x8^7)_NN_d
Tuning for shape 4x8388608*(4x8^1)_NN_d
Tuning for shape 4x4194304*(4x8^2)_NN_d
Tuning for shape 4x2097152*(4x8^3)_NN_d
Tuning for shape 4x1048576*(4x8^4)_NN_d
Tuning for shape 4x524288*(4x8^5)_NN_d
Tuning for shape 4x262144*(4x8^6)_NN_d
Tuning for shape 4x4194304*(4x8^1)_NN_d
Tuning for shape 4x2097152*(4x8^2)_NN_d
Tuning for shape 4x1048576*(4x8^3)_NN_d
Tuning for shape 4x524288*(4x8^4)_NN_d
Tuning for shape 4x262144*(4x8^5)_NN_d
Tuning for shape 4x2097152*(4x8^1)_NN_d
Tuning for shape 4x1048576*(4x8^2)_NN_d
Tuning for shape 4x524288*(4x8^3)_NN_d
Tuning for shape 4x262144*(4x8^4)_NN_d
Tuning for shape 4x1048576*(4x8^1)_NN_d
Tuning for shape 4x524288*(4x8^2)_NN_d
Tuning for shape 4x262144*(4x8^3)_NN_d
Tuning for shape 4x524288*(4x8^1)_NN_d
Tuning for shape 4x262144*(4x8^2)_NN_d
Tuning for shape 4x262144*(4x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x8^9  &  1.000 & 1.000 & -1.000 & -1.000
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [4, 16] x to produce Y[1, 16]
Matmul: 1 x 16 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x16^1  &  1.000 & 1.000 & 0.000 & 2058.689
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [4, 16] x to produce Y[4, 16]
Matmul: 4 x 16 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x16^1  &  1.000 & 1.000 & 0.001 & 731.112
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [4, 16] x to produce Y[16, 16]
Matmul: 16 x 16 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x16^1  &  1.000 & 1.000 & 0.005 & 214.606
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [4, 16] x to produce Y[64, 16]
Matmul: 64 x 16 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x16^1  &  1.000 & 1.000 & 0.027 & 37.577
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [4, 16] x to produce Y[256, 16]
Matmul: 256 x 16 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x16^1  &  1.000 & 1.000 & 0.109 & 9.210
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [4, 16] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x16^1  &  1.000 & 1.000 & 0.427 & 2.342
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [4, 16] x F_1 [4, 16] x to produce Y[1, 256]
Matmul: 1 x 256 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x64*(4x16^1)_NN_d
Tuning for shape 1x16*(4x16^2)_NN_d
Tuning for shape 1x16*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x16^2  &  1.000 & 1.000 & 0.006 & 160.993
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [4, 16] x F_1 [4, 16] x to produce Y[4, 256]
Matmul: 4 x 256 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x64*(4x16^1)_NN_d
Tuning for shape 4x16*(4x16^2)_NN_d
Tuning for shape 4x16*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x16^2  &  1.000 & 1.000 & 0.018 & 57.033
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [4, 16] x F_1 [4, 16] x to produce Y[16, 256]
Matmul: 16 x 256 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x64*(4x16^1)_NN_d
Tuning for shape 16x16*(4x16^2)_NN_d
Tuning for shape 16x16*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x16^2  &  1.000 & 1.000 & 0.088 & 11.356
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [4, 16] x F_1 [4, 16] x to produce Y[64, 256]
Matmul: 64 x 256 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x64*(4x16^1)_NN_d
Tuning for shape 64x16*(4x16^2)_NN_d
Tuning for shape 64x16*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x16^2  &  1.000 & 1.000 & 0.360 & 2.778
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [4, 16] x F_1 [4, 16] x to produce Y[256, 256]
Matmul: 256 x 256 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x64*(4x16^1)_NN_d
Tuning for shape 256x16*(4x16^2)_NN_d
Tuning for shape 256x16*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x16^2  &  1.000 & 1.000 & 1.209 & 0.827
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [4, 16] x F_1 [4, 16] x to produce Y[1024, 256]
Matmul: 1024 x 256 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x64*(4x16^1)_NN_d
Tuning for shape 1024x16*(4x16^2)_NN_d
Tuning for shape 1024x16*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x16^2  &  1.000 & 1.000 & 5.829 & 0.172
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(4x16^1)_NN_d
Tuning for shape 1x256*(4x16^2)_NN_d
Tuning for shape 1x64*(4x16^3)_NN_d
Tuning for shape 1x256*(4x16^1)_NN_d
Tuning for shape 1x64*(4x16^2)_NN_d
Tuning for shape 1x64*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x16^3  &  1.000 & 1.000 & 0.083 & 12.003
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1024*(4x16^1)_NN_d
Tuning for shape 4x256*(4x16^2)_NN_d
Tuning for shape 4x64*(4x16^3)_NN_d
Tuning for shape 4x256*(4x16^1)_NN_d
Tuning for shape 4x64*(4x16^2)_NN_d
Tuning for shape 4x64*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x16^3  &  1.000 & 1.000 & 0.305 & 3.281
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1024*(4x16^1)_NN_d
Tuning for shape 16x256*(4x16^2)_NN_d
Tuning for shape 16x64*(4x16^3)_NN_d
Tuning for shape 16x256*(4x16^1)_NN_d
Tuning for shape 16x64*(4x16^2)_NN_d
Tuning for shape 16x64*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x16^3  &  1.000 & 1.000 & 0.901 & 1.110
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1024*(4x16^1)_NN_d
Tuning for shape 64x256*(4x16^2)_NN_d
Tuning for shape 64x64*(4x16^3)_NN_d
Tuning for shape 64x256*(4x16^1)_NN_d
Tuning for shape 64x64*(4x16^2)_NN_d
Tuning for shape 64x64*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x16^3  &  1.000 & 1.000 & 5.080 & 0.197
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1024*(4x16^1)_NN_d
Tuning for shape 256x256*(4x16^2)_NN_d
Tuning for shape 256x64*(4x16^3)_NN_d
Tuning for shape 256x256*(4x16^1)_NN_d
Tuning for shape 256x64*(4x16^2)_NN_d
Tuning for shape 256x64*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x16^3  &  1.000 & 1.000 & 19.209 & 0.052
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x1024*(4x16^1)_NN_d
Tuning for shape 1024x256*(4x16^2)_NN_d
Tuning for shape 1024x64*(4x16^3)_NN_d
Tuning for shape 1024x256*(4x16^1)_NN_d
Tuning for shape 1024x64*(4x16^2)_NN_d
Tuning for shape 1024x64*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x16^3  &  1.000 & 1.000 & 73.264 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x to produce Y[1, 65536]
Matmul: 1 x 65536 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16384*(4x16^1)_NN_d
Tuning for shape 1x4096*(4x16^2)_NN_d
Tuning for shape 1x1024*(4x16^3)_NN_d
Tuning for shape 1x256*(4x16^4)_NN_d
Tuning for shape 1x4096*(4x16^1)_NN_d
Tuning for shape 1x1024*(4x16^2)_NN_d
Tuning for shape 1x256*(4x16^3)_NN_d
Tuning for shape 1x1024*(4x16^1)_NN_d
Tuning for shape 1x256*(4x16^2)_NN_d
Tuning for shape 1x256*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x16^4  &  1.000 & 1.000 & 1.140 & 0.877
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x to produce Y[4, 65536]
Matmul: 4 x 65536 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16384*(4x16^1)_NN_d
Tuning for shape 4x4096*(4x16^2)_NN_d
Tuning for shape 4x1024*(4x16^3)_NN_d
Tuning for shape 4x256*(4x16^4)_NN_d
Tuning for shape 4x4096*(4x16^1)_NN_d
Tuning for shape 4x1024*(4x16^2)_NN_d
Tuning for shape 4x256*(4x16^3)_NN_d
Tuning for shape 4x1024*(4x16^1)_NN_d
Tuning for shape 4x256*(4x16^2)_NN_d
Tuning for shape 4x256*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x16^4  &  1.000 & 1.000 & 4.276 & 0.234
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x to produce Y[16, 65536]
Matmul: 16 x 65536 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16384*(4x16^1)_NN_d
Tuning for shape 16x4096*(4x16^2)_NN_d
Tuning for shape 16x1024*(4x16^3)_NN_d
Tuning for shape 16x256*(4x16^4)_NN_d
Tuning for shape 16x4096*(4x16^1)_NN_d
Tuning for shape 16x1024*(4x16^2)_NN_d
Tuning for shape 16x256*(4x16^3)_NN_d
Tuning for shape 16x1024*(4x16^1)_NN_d
Tuning for shape 16x256*(4x16^2)_NN_d
Tuning for shape 16x256*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x16^4  &  1.000 & 1.000 & 15.590 & 0.064
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x to produce Y[64, 65536]
Matmul: 64 x 65536 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16384*(4x16^1)_NN_d
Tuning for shape 64x4096*(4x16^2)_NN_d
Tuning for shape 64x1024*(4x16^3)_NN_d
Tuning for shape 64x256*(4x16^4)_NN_d
Tuning for shape 64x4096*(4x16^1)_NN_d
Tuning for shape 64x1024*(4x16^2)_NN_d
Tuning for shape 64x256*(4x16^3)_NN_d
Tuning for shape 64x1024*(4x16^1)_NN_d
Tuning for shape 64x256*(4x16^2)_NN_d
Tuning for shape 64x256*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x16^4  &  1.000 & 1.000 & 63.775 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x to produce Y[256, 65536]
Matmul: 256 x 65536 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16384*(4x16^1)_NN_d
Tuning for shape 256x4096*(4x16^2)_NN_d
Tuning for shape 256x1024*(4x16^3)_NN_d
Tuning for shape 256x256*(4x16^4)_NN_d
Tuning for shape 256x4096*(4x16^1)_NN_d
Tuning for shape 256x1024*(4x16^2)_NN_d
Tuning for shape 256x256*(4x16^3)_NN_d
Tuning for shape 256x1024*(4x16^1)_NN_d
Tuning for shape 256x256*(4x16^2)_NN_d
Tuning for shape 256x256*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x16^4  &  1.000 & 1.000 & 125.513 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 256] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x to produce Y[1024, 65536]
Matmul: 1024 x 65536 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16384*(4x16^1)_NN_d
Tuning for shape 1024x4096*(4x16^2)_NN_d
Tuning for shape 1024x1024*(4x16^3)_NN_d
Tuning for shape 1024x256*(4x16^4)_NN_d
Tuning for shape 1024x4096*(4x16^1)_NN_d
Tuning for shape 1024x1024*(4x16^2)_NN_d
Tuning for shape 1024x256*(4x16^3)_NN_d
Tuning for shape 1024x1024*(4x16^1)_NN_d
Tuning for shape 1024x256*(4x16^2)_NN_d
Tuning for shape 1024x256*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x16^4  &  1.000 & 1.000 & 132.785 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1024] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x F_4 [4, 16] x to produce Y[1, 1048576]
Matmul: 1 x 1048576 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x262144*(4x16^1)_NN_d
Tuning for shape 1x65536*(4x16^2)_NN_d
Tuning for shape 1x16384*(4x16^3)_NN_d
Tuning for shape 1x4096*(4x16^4)_NN_d
Tuning for shape 1x1024*(4x16^5)_NN_d
Tuning for shape 1x65536*(4x16^1)_NN_d
Tuning for shape 1x16384*(4x16^2)_NN_d
Tuning for shape 1x4096*(4x16^3)_NN_d
Tuning for shape 1x1024*(4x16^4)_NN_d
Tuning for shape 1x16384*(4x16^1)_NN_d
Tuning for shape 1x4096*(4x16^2)_NN_d
Tuning for shape 1x1024*(4x16^3)_NN_d
Tuning for shape 1x4096*(4x16^1)_NN_d
Tuning for shape 1x1024*(4x16^2)_NN_d
Tuning for shape 1x1024*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x16^5  &  1.000 & 1.000 & 15.184 & 0.066
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1024] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x F_4 [4, 16] x to produce Y[4, 1048576]
Matmul: 4 x 1048576 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x262144*(4x16^1)_NN_d
Tuning for shape 4x65536*(4x16^2)_NN_d
Tuning for shape 4x16384*(4x16^3)_NN_d
Tuning for shape 4x4096*(4x16^4)_NN_d
Tuning for shape 4x1024*(4x16^5)_NN_d
Tuning for shape 4x65536*(4x16^1)_NN_d
Tuning for shape 4x16384*(4x16^2)_NN_d
Tuning for shape 4x4096*(4x16^3)_NN_d
Tuning for shape 4x1024*(4x16^4)_NN_d
Tuning for shape 4x16384*(4x16^1)_NN_d
Tuning for shape 4x4096*(4x16^2)_NN_d
Tuning for shape 4x1024*(4x16^3)_NN_d
Tuning for shape 4x4096*(4x16^1)_NN_d
Tuning for shape 4x1024*(4x16^2)_NN_d
Tuning for shape 4x1024*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x16^5  &  1.000 & 1.000 & 55.850 & 0.018
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1024] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x F_4 [4, 16] x to produce Y[16, 1048576]
Matmul: 16 x 1048576 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x262144*(4x16^1)_NN_d
Tuning for shape 16x65536*(4x16^2)_NN_d
Tuning for shape 16x16384*(4x16^3)_NN_d
Tuning for shape 16x4096*(4x16^4)_NN_d
Tuning for shape 16x1024*(4x16^5)_NN_d
Tuning for shape 16x65536*(4x16^1)_NN_d
Tuning for shape 16x16384*(4x16^2)_NN_d
Tuning for shape 16x4096*(4x16^3)_NN_d
Tuning for shape 16x1024*(4x16^4)_NN_d
Tuning for shape 16x16384*(4x16^1)_NN_d
Tuning for shape 16x4096*(4x16^2)_NN_d
Tuning for shape 16x1024*(4x16^3)_NN_d
Tuning for shape 16x4096*(4x16^1)_NN_d
Tuning for shape 16x1024*(4x16^2)_NN_d
Tuning for shape 16x1024*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x16^5  &  1.000 & 1.000 & 126.429 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1024] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x F_4 [4, 16] x to produce Y[64, 1048576]
Matmul: 64 x 1048576 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x262144*(4x16^1)_NN_d
Tuning for shape 64x65536*(4x16^2)_NN_d
Tuning for shape 64x16384*(4x16^3)_NN_d
Tuning for shape 64x4096*(4x16^4)_NN_d
Tuning for shape 64x1024*(4x16^5)_NN_d
Tuning for shape 64x65536*(4x16^1)_NN_d
Tuning for shape 64x16384*(4x16^2)_NN_d
Tuning for shape 64x4096*(4x16^3)_NN_d
Tuning for shape 64x1024*(4x16^4)_NN_d
Tuning for shape 64x16384*(4x16^1)_NN_d
Tuning for shape 64x4096*(4x16^2)_NN_d
Tuning for shape 64x1024*(4x16^3)_NN_d
Tuning for shape 64x4096*(4x16^1)_NN_d
Tuning for shape 64x1024*(4x16^2)_NN_d
Tuning for shape 64x1024*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x16^5  &  1.000 & 1.000 & 134.418 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1024] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x F_4 [4, 16] x to produce Y[256, 1048576]
Matmul: 256 x 1048576 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x262144*(4x16^1)_NN_d
Tuning for shape 256x65536*(4x16^2)_NN_d
Tuning for shape 256x16384*(4x16^3)_NN_d
Tuning for shape 256x4096*(4x16^4)_NN_d
Tuning for shape 256x1024*(4x16^5)_NN_d
Tuning for shape 256x65536*(4x16^1)_NN_d
Tuning for shape 256x16384*(4x16^2)_NN_d
Tuning for shape 256x4096*(4x16^3)_NN_d
Tuning for shape 256x1024*(4x16^4)_NN_d
Tuning for shape 256x16384*(4x16^1)_NN_d
Tuning for shape 256x4096*(4x16^2)_NN_d
Tuning for shape 256x1024*(4x16^3)_NN_d
Tuning for shape 256x4096*(4x16^1)_NN_d
Tuning for shape 256x1024*(4x16^2)_NN_d
Tuning for shape 256x1024*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x16^5  &  1.000 & 1.000 & 134.720 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x F_4 [4, 16] x F_5 [4, 16] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4194304*(4x16^1)_NN_d
Tuning for shape 1x1048576*(4x16^2)_NN_d
Tuning for shape 1x262144*(4x16^3)_NN_d
Tuning for shape 1x65536*(4x16^4)_NN_d
Tuning for shape 1x16384*(4x16^5)_NN_d
Tuning for shape 1x4096*(4x16^6)_NN_d
Tuning for shape 1x1048576*(4x16^1)_NN_d
Tuning for shape 1x262144*(4x16^2)_NN_d
Tuning for shape 1x65536*(4x16^3)_NN_d
Tuning for shape 1x16384*(4x16^4)_NN_d
Tuning for shape 1x4096*(4x16^5)_NN_d
Tuning for shape 1x262144*(4x16^1)_NN_d
Tuning for shape 1x65536*(4x16^2)_NN_d
Tuning for shape 1x16384*(4x16^3)_NN_d
Tuning for shape 1x4096*(4x16^4)_NN_d
Tuning for shape 1x65536*(4x16^1)_NN_d
Tuning for shape 1x16384*(4x16^2)_NN_d
Tuning for shape 1x4096*(4x16^3)_NN_d
Tuning for shape 1x16384*(4x16^1)_NN_d
Tuning for shape 1x4096*(4x16^2)_NN_d
Tuning for shape 1x4096*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x16^6  &  1.000 & 1.000 & 126.971 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x F_4 [4, 16] x F_5 [4, 16] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4194304*(4x16^1)_NN_d
Tuning for shape 4x1048576*(4x16^2)_NN_d
Tuning for shape 4x262144*(4x16^3)_NN_d
Tuning for shape 4x65536*(4x16^4)_NN_d
Tuning for shape 4x16384*(4x16^5)_NN_d
Tuning for shape 4x4096*(4x16^6)_NN_d
Tuning for shape 4x1048576*(4x16^1)_NN_d
Tuning for shape 4x262144*(4x16^2)_NN_d
Tuning for shape 4x65536*(4x16^3)_NN_d
Tuning for shape 4x16384*(4x16^4)_NN_d
Tuning for shape 4x4096*(4x16^5)_NN_d
Tuning for shape 4x262144*(4x16^1)_NN_d
Tuning for shape 4x65536*(4x16^2)_NN_d
Tuning for shape 4x16384*(4x16^3)_NN_d
Tuning for shape 4x4096*(4x16^4)_NN_d
Tuning for shape 4x65536*(4x16^1)_NN_d
Tuning for shape 4x16384*(4x16^2)_NN_d
Tuning for shape 4x4096*(4x16^3)_NN_d
Tuning for shape 4x16384*(4x16^1)_NN_d
Tuning for shape 4x4096*(4x16^2)_NN_d
Tuning for shape 4x4096*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x16^6  &  1.000 & 1.000 & 134.461 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x F_4 [4, 16] x F_5 [4, 16] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 4096, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4194304*(4x16^1)_NN_d
Tuning for shape 16x1048576*(4x16^2)_NN_d
Tuning for shape 16x262144*(4x16^3)_NN_d
Tuning for shape 16x65536*(4x16^4)_NN_d
Tuning for shape 16x16384*(4x16^5)_NN_d
Tuning for shape 16x4096*(4x16^6)_NN_d
Tuning for shape 16x1048576*(4x16^1)_NN_d
Tuning for shape 16x262144*(4x16^2)_NN_d
Tuning for shape 16x65536*(4x16^3)_NN_d
Tuning for shape 16x16384*(4x16^4)_NN_d
Tuning for shape 16x4096*(4x16^5)_NN_d
Tuning for shape 16x262144*(4x16^1)_NN_d
Tuning for shape 16x65536*(4x16^2)_NN_d
Tuning for shape 16x16384*(4x16^3)_NN_d
Tuning for shape 16x4096*(4x16^4)_NN_d
Tuning for shape 16x65536*(4x16^1)_NN_d
Tuning for shape 16x16384*(4x16^2)_NN_d
Tuning for shape 16x4096*(4x16^3)_NN_d
Tuning for shape 16x16384*(4x16^1)_NN_d
Tuning for shape 16x4096*(4x16^2)_NN_d
Tuning for shape 16x4096*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x16^6  &  1.000 & 1.000 & 134.536 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 4 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16384] with F_0 [4, 16] x F_1 [4, 16] x F_2 [4, 16] x F_3 [4, 16] x F_4 [4, 16] x F_5 [4, 16] x F_6 [4, 16] x to produce Y[1, 268435456]
Matmul: 1 x 268435456 x 16384, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x67108864*(4x16^1)_NN_d
Tuning for shape 1x16777216*(4x16^2)_NN_d
Tuning for shape 1x4194304*(4x16^3)_NN_d
Tuning for shape 1x1048576*(4x16^4)_NN_d
Tuning for shape 1x262144*(4x16^5)_NN_d
Tuning for shape 1x65536*(4x16^6)_NN_d
Tuning for shape 1x16384*(4x16^7)_NN_d
Tuning for shape 1x16777216*(4x16^1)_NN_d
Tuning for shape 1x4194304*(4x16^2)_NN_d
Tuning for shape 1x1048576*(4x16^3)_NN_d
Tuning for shape 1x262144*(4x16^4)_NN_d
Tuning for shape 1x65536*(4x16^5)_NN_d
Tuning for shape 1x16384*(4x16^6)_NN_d
Tuning for shape 1x4194304*(4x16^1)_NN_d
Tuning for shape 1x1048576*(4x16^2)_NN_d
Tuning for shape 1x262144*(4x16^3)_NN_d
Tuning for shape 1x65536*(4x16^4)_NN_d
Tuning for shape 1x16384*(4x16^5)_NN_d
Tuning for shape 1x1048576*(4x16^1)_NN_d
Tuning for shape 1x262144*(4x16^2)_NN_d
Tuning for shape 1x65536*(4x16^3)_NN_d
Tuning for shape 1x16384*(4x16^4)_NN_d
Tuning for shape 1x262144*(4x16^1)_NN_d
Tuning for shape 1x65536*(4x16^2)_NN_d
Tuning for shape 1x16384*(4x16^3)_NN_d
Tuning for shape 1x65536*(4x16^1)_NN_d
Tuning for shape 1x16384*(4x16^2)_NN_d
Tuning for shape 1x16384*(4x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x16^7  &  1.000 & 1.000 & 134.759 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [4, 32] x to produce Y[1, 32]
Matmul: 1 x 32 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x32^1  &  1.000 & 1.000 & 0.001 & 1003.221
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [4, 32] x to produce Y[4, 32]
Matmul: 4 x 32 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x32^1  &  1.000 & 1.000 & 0.003 & 290.573
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [4, 32] x to produce Y[16, 32]
Matmul: 16 x 32 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x32^1  &  1.000 & 1.000 & 0.013 & 75.164
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [4, 32] x to produce Y[64, 32]
Matmul: 64 x 32 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x32^1  &  1.000 & 1.000 & 0.050 & 19.928
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [4, 32] x to produce Y[256, 32]
Matmul: 256 x 32 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x32^1  &  1.000 & 1.000 & 0.207 & 4.837
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [4, 32] x to produce Y[1024, 32]
Matmul: 1024 x 32 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x32^1  &  1.000 & 1.000 & 0.880 & 1.137
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [4, 32] x F_1 [4, 32] x to produce Y[1, 1024]
Matmul: 1 x 1024 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(4x32^1)_NN_d
Tuning for shape 1x16*(4x32^2)_NN_d
Tuning for shape 1x16*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x32^2  &  1.000 & 1.000 & 0.022 & 45.273
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [4, 32] x F_1 [4, 32] x to produce Y[4, 1024]
Matmul: 4 x 1024 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(4x32^1)_NN_d
Tuning for shape 4x16*(4x32^2)_NN_d
Tuning for shape 4x16*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x32^2  &  1.000 & 1.000 & 0.082 & 12.124
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [4, 32] x F_1 [4, 32] x to produce Y[16, 1024]
Matmul: 16 x 1024 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(4x32^1)_NN_d
Tuning for shape 16x16*(4x32^2)_NN_d
Tuning for shape 16x16*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x32^2  &  1.000 & 1.000 & 0.330 & 3.033
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [4, 32] x F_1 [4, 32] x to produce Y[64, 1024]
Matmul: 64 x 1024 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(4x32^1)_NN_d
Tuning for shape 64x16*(4x32^2)_NN_d
Tuning for shape 64x16*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x32^2  &  1.000 & 1.000 & 1.349 & 0.741
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [4, 32] x F_1 [4, 32] x to produce Y[256, 1024]
Matmul: 256 x 1024 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(4x32^1)_NN_d
Tuning for shape 256x16*(4x32^2)_NN_d
Tuning for shape 256x16*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x32^2  &  1.000 & 1.000 & 5.392 & 0.185
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [4, 32] x F_1 [4, 32] x to produce Y[1024, 1024]
Matmul: 1024 x 1024 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(4x32^1)_NN_d
Tuning for shape 1024x16*(4x32^2)_NN_d
Tuning for shape 1024x16*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x32^2  &  1.000 & 1.000 & 20.448 & 0.049
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x to produce Y[1, 32768]
Matmul: 1 x 32768 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4096*(4x32^1)_NN_d
Tuning for shape 1x512*(4x32^2)_NN_d
Tuning for shape 1x64*(4x32^3)_NN_d
Tuning for shape 1x512*(4x32^1)_NN_d
Tuning for shape 1x64*(4x32^2)_NN_d
Tuning for shape 1x64*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x32^3  &  1.000 & 1.000 & 0.586 & 1.706
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x to produce Y[4, 32768]
Matmul: 4 x 32768 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4096*(4x32^1)_NN_d
Tuning for shape 4x512*(4x32^2)_NN_d
Tuning for shape 4x64*(4x32^3)_NN_d
Tuning for shape 4x512*(4x32^1)_NN_d
Tuning for shape 4x64*(4x32^2)_NN_d
Tuning for shape 4x64*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x32^3  &  1.000 & 1.000 & 2.146 & 0.466
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x to produce Y[16, 32768]
Matmul: 16 x 32768 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4096*(4x32^1)_NN_d
Tuning for shape 16x512*(4x32^2)_NN_d
Tuning for shape 16x64*(4x32^3)_NN_d
Tuning for shape 16x512*(4x32^1)_NN_d
Tuning for shape 16x64*(4x32^2)_NN_d
Tuning for shape 16x64*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x32^3  &  1.000 & 1.000 & 8.753 & 0.114
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x to produce Y[64, 32768]
Matmul: 64 x 32768 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4096*(4x32^1)_NN_d
Tuning for shape 64x512*(4x32^2)_NN_d
Tuning for shape 64x64*(4x32^3)_NN_d
Tuning for shape 64x512*(4x32^1)_NN_d
Tuning for shape 64x64*(4x32^2)_NN_d
Tuning for shape 64x64*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x32^3  &  1.000 & 1.000 & 34.885 & 0.029
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x to produce Y[256, 32768]
Matmul: 256 x 32768 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4096*(4x32^1)_NN_d
Tuning for shape 256x512*(4x32^2)_NN_d
Tuning for shape 256x64*(4x32^3)_NN_d
Tuning for shape 256x512*(4x32^1)_NN_d
Tuning for shape 256x64*(4x32^2)_NN_d
Tuning for shape 256x64*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x32^3  &  1.000 & 1.000 & 137.979 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x to produce Y[1024, 32768]
Matmul: 1024 x 32768 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4096*(4x32^1)_NN_d
Tuning for shape 1024x512*(4x32^2)_NN_d
Tuning for shape 1024x64*(4x32^3)_NN_d
Tuning for shape 1024x512*(4x32^1)_NN_d
Tuning for shape 1024x64*(4x32^2)_NN_d
Tuning for shape 1024x64*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x32^3  &  1.000 & 1.000 & 239.370 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x F_3 [4, 32] x to produce Y[1, 1048576]
Matmul: 1 x 1048576 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x131072*(4x32^1)_NN_d
Tuning for shape 1x16384*(4x32^2)_NN_d
Tuning for shape 1x2048*(4x32^3)_NN_d
Tuning for shape 1x256*(4x32^4)_NN_d
Tuning for shape 1x16384*(4x32^1)_NN_d
Tuning for shape 1x2048*(4x32^2)_NN_d
Tuning for shape 1x256*(4x32^3)_NN_d
Tuning for shape 1x2048*(4x32^1)_NN_d
Tuning for shape 1x256*(4x32^2)_NN_d
Tuning for shape 1x256*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x32^4  &  1.000 & 1.000 & 16.029 & 0.062
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x F_3 [4, 32] x to produce Y[4, 1048576]
Matmul: 4 x 1048576 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x131072*(4x32^1)_NN_d
Tuning for shape 4x16384*(4x32^2)_NN_d
Tuning for shape 4x2048*(4x32^3)_NN_d
Tuning for shape 4x256*(4x32^4)_NN_d
Tuning for shape 4x16384*(4x32^1)_NN_d
Tuning for shape 4x2048*(4x32^2)_NN_d
Tuning for shape 4x256*(4x32^3)_NN_d
Tuning for shape 4x2048*(4x32^1)_NN_d
Tuning for shape 4x256*(4x32^2)_NN_d
Tuning for shape 4x256*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x32^4  &  1.000 & 1.000 & 59.604 & 0.017
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x F_3 [4, 32] x to produce Y[16, 1048576]
Matmul: 16 x 1048576 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x131072*(4x32^1)_NN_d
Tuning for shape 16x16384*(4x32^2)_NN_d
Tuning for shape 16x2048*(4x32^3)_NN_d
Tuning for shape 16x256*(4x32^4)_NN_d
Tuning for shape 16x16384*(4x32^1)_NN_d
Tuning for shape 16x2048*(4x32^2)_NN_d
Tuning for shape 16x256*(4x32^3)_NN_d
Tuning for shape 16x2048*(4x32^1)_NN_d
Tuning for shape 16x256*(4x32^2)_NN_d
Tuning for shape 16x256*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x32^4  &  1.000 & 1.000 & 224.865 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x F_3 [4, 32] x to produce Y[64, 1048576]
Matmul: 64 x 1048576 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x131072*(4x32^1)_NN_d
Tuning for shape 64x16384*(4x32^2)_NN_d
Tuning for shape 64x2048*(4x32^3)_NN_d
Tuning for shape 64x256*(4x32^4)_NN_d
Tuning for shape 64x16384*(4x32^1)_NN_d
Tuning for shape 64x2048*(4x32^2)_NN_d
Tuning for shape 64x256*(4x32^3)_NN_d
Tuning for shape 64x2048*(4x32^1)_NN_d
Tuning for shape 64x256*(4x32^2)_NN_d
Tuning for shape 64x256*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x32^4  &  1.000 & 1.000 & 244.451 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x F_3 [4, 32] x to produce Y[256, 1048576]
Matmul: 256 x 1048576 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x131072*(4x32^1)_NN_d
Tuning for shape 256x16384*(4x32^2)_NN_d
Tuning for shape 256x2048*(4x32^3)_NN_d
Tuning for shape 256x256*(4x32^4)_NN_d
Tuning for shape 256x16384*(4x32^1)_NN_d
Tuning for shape 256x2048*(4x32^2)_NN_d
Tuning for shape 256x256*(4x32^3)_NN_d
Tuning for shape 256x2048*(4x32^1)_NN_d
Tuning for shape 256x256*(4x32^2)_NN_d
Tuning for shape 256x256*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x32^4  &  1.000 & 1.000 & 249.067 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1024] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x F_3 [4, 32] x F_4 [4, 32] x to produce Y[1, 33554432]
Matmul: 1 x 33554432 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4194304*(4x32^1)_NN_d
Tuning for shape 1x524288*(4x32^2)_NN_d
Tuning for shape 1x65536*(4x32^3)_NN_d
Tuning for shape 1x8192*(4x32^4)_NN_d
Tuning for shape 1x1024*(4x32^5)_NN_d
Tuning for shape 1x524288*(4x32^1)_NN_d
Tuning for shape 1x65536*(4x32^2)_NN_d
Tuning for shape 1x8192*(4x32^3)_NN_d
Tuning for shape 1x1024*(4x32^4)_NN_d
Tuning for shape 1x65536*(4x32^1)_NN_d
Tuning for shape 1x8192*(4x32^2)_NN_d
Tuning for shape 1x1024*(4x32^3)_NN_d
Tuning for shape 1x8192*(4x32^1)_NN_d
Tuning for shape 1x1024*(4x32^2)_NN_d
Tuning for shape 1x1024*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x32^5  &  1.000 & 1.000 & 200.933 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1024] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x F_3 [4, 32] x F_4 [4, 32] x to produce Y[4, 33554432]
Matmul: 4 x 33554432 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4194304*(4x32^1)_NN_d
Tuning for shape 4x524288*(4x32^2)_NN_d
Tuning for shape 4x65536*(4x32^3)_NN_d
Tuning for shape 4x8192*(4x32^4)_NN_d
Tuning for shape 4x1024*(4x32^5)_NN_d
Tuning for shape 4x524288*(4x32^1)_NN_d
Tuning for shape 4x65536*(4x32^2)_NN_d
Tuning for shape 4x8192*(4x32^3)_NN_d
Tuning for shape 4x1024*(4x32^4)_NN_d
Tuning for shape 4x65536*(4x32^1)_NN_d
Tuning for shape 4x8192*(4x32^2)_NN_d
Tuning for shape 4x1024*(4x32^3)_NN_d
Tuning for shape 4x8192*(4x32^1)_NN_d
Tuning for shape 4x1024*(4x32^2)_NN_d
Tuning for shape 4x1024*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x32^5  &  1.000 & 1.000 & 232.216 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 4 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1024] with F_0 [4, 32] x F_1 [4, 32] x F_2 [4, 32] x F_3 [4, 32] x F_4 [4, 32] x to produce Y[16, 33554432]
Matmul: 16 x 33554432 x 1024, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4194304*(4x32^1)_NN_d
Tuning for shape 16x524288*(4x32^2)_NN_d
Tuning for shape 16x65536*(4x32^3)_NN_d
Tuning for shape 16x8192*(4x32^4)_NN_d
Tuning for shape 16x1024*(4x32^5)_NN_d
Tuning for shape 16x524288*(4x32^1)_NN_d
Tuning for shape 16x65536*(4x32^2)_NN_d
Tuning for shape 16x8192*(4x32^3)_NN_d
Tuning for shape 16x1024*(4x32^4)_NN_d
Tuning for shape 16x65536*(4x32^1)_NN_d
Tuning for shape 16x8192*(4x32^2)_NN_d
Tuning for shape 16x1024*(4x32^3)_NN_d
Tuning for shape 16x8192*(4x32^1)_NN_d
Tuning for shape 16x1024*(4x32^2)_NN_d
Tuning for shape 16x1024*(4x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x32^5  &  1.000 & 1.000 & 246.810 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [4, 64] x to produce Y[1, 64]
Matmul: 1 x 64 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x64^1  &  1.000 & 1.000 & 0.002 & 502.029
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [4, 64] x to produce Y[4, 64]
Matmul: 4 x 64 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x64^1  &  1.000 & 1.000 & 0.006 & 157.370
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [4, 64] x to produce Y[16, 64]
Matmul: 16 x 64 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x64^1  &  1.000 & 1.000 & 0.026 & 38.976
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [4, 64] x to produce Y[64, 64]
Matmul: 64 x 64 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x64^1  &  1.000 & 1.000 & 0.108 & 9.221
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [4, 64] x to produce Y[256, 64]
Matmul: 256 x 64 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x64^1  &  1.000 & 1.000 & 0.430 & 2.328
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [4, 64] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x64^1  &  1.000 & 1.000 & 1.732 & 0.577
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [4, 64] x F_1 [4, 64] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(4x64^1)_NN_d
Tuning for shape 1x16*(4x64^2)_NN_d
Tuning for shape 1x16*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x64^2  &  1.000 & 1.000 & 0.082 & 12.205
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [4, 64] x F_1 [4, 64] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(4x64^1)_NN_d
Tuning for shape 4x16*(4x64^2)_NN_d
Tuning for shape 4x16*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x64^2  &  1.000 & 1.000 & 0.312 & 3.207
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [4, 64] x F_1 [4, 64] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(4x64^1)_NN_d
Tuning for shape 16x16*(4x64^2)_NN_d
Tuning for shape 16x16*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x64^2  &  1.000 & 1.000 & 1.221 & 0.819
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [4, 64] x F_1 [4, 64] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(4x64^1)_NN_d
Tuning for shape 64x16*(4x64^2)_NN_d
Tuning for shape 64x16*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x64^2  &  1.000 & 1.000 & 5.043 & 0.198
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [4, 64] x F_1 [4, 64] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(4x64^1)_NN_d
Tuning for shape 256x16*(4x64^2)_NN_d
Tuning for shape 256x16*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x64^2  &  1.000 & 1.000 & 19.921 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [4, 64] x F_1 [4, 64] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x256*(4x64^1)_NN_d
Tuning for shape 1024x16*(4x64^2)_NN_d
Tuning for shape 1024x16*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x64^2  &  1.000 & 1.000 & 78.364 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [4, 64] x F_1 [4, 64] x F_2 [4, 64] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16384*(4x64^1)_NN_d
Tuning for shape 1x1024*(4x64^2)_NN_d
Tuning for shape 1x64*(4x64^3)_NN_d
Tuning for shape 1x1024*(4x64^1)_NN_d
Tuning for shape 1x64*(4x64^2)_NN_d
Tuning for shape 1x64*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x64^3  &  1.000 & 1.000 & 4.400 & 0.227
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [4, 64] x F_1 [4, 64] x F_2 [4, 64] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16384*(4x64^1)_NN_d
Tuning for shape 4x1024*(4x64^2)_NN_d
Tuning for shape 4x64*(4x64^3)_NN_d
Tuning for shape 4x1024*(4x64^1)_NN_d
Tuning for shape 4x64*(4x64^2)_NN_d
Tuning for shape 4x64*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x64^3  &  1.000 & 1.000 & 16.204 & 0.062
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [4, 64] x F_1 [4, 64] x F_2 [4, 64] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16384*(4x64^1)_NN_d
Tuning for shape 16x1024*(4x64^2)_NN_d
Tuning for shape 16x64*(4x64^3)_NN_d
Tuning for shape 16x1024*(4x64^1)_NN_d
Tuning for shape 16x64*(4x64^2)_NN_d
Tuning for shape 16x64*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x64^3  &  1.000 & 1.000 & 59.725 & 0.017
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [4, 64] x F_1 [4, 64] x F_2 [4, 64] x to produce Y[64, 262144]
Matmul: 64 x 262144 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16384*(4x64^1)_NN_d
Tuning for shape 64x1024*(4x64^2)_NN_d
Tuning for shape 64x64*(4x64^3)_NN_d
Tuning for shape 64x1024*(4x64^1)_NN_d
Tuning for shape 64x64*(4x64^2)_NN_d
Tuning for shape 64x64*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x64^3  &  1.000 & 1.000 & 238.228 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [4, 64] x F_1 [4, 64] x F_2 [4, 64] x to produce Y[256, 262144]
Matmul: 256 x 262144 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16384*(4x64^1)_NN_d
Tuning for shape 256x1024*(4x64^2)_NN_d
Tuning for shape 256x64*(4x64^3)_NN_d
Tuning for shape 256x1024*(4x64^1)_NN_d
Tuning for shape 256x64*(4x64^2)_NN_d
Tuning for shape 256x64*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x64^3  &  1.000 & 1.000 & 254.942 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [4, 64] x F_1 [4, 64] x F_2 [4, 64] x to produce Y[1024, 262144]
Matmul: 1024 x 262144 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16384*(4x64^1)_NN_d
Tuning for shape 1024x1024*(4x64^2)_NN_d
Tuning for shape 1024x64*(4x64^3)_NN_d
Tuning for shape 1024x1024*(4x64^1)_NN_d
Tuning for shape 1024x64*(4x64^2)_NN_d
Tuning for shape 1024x64*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x64^3  &  1.000 & 1.000 & 260.232 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [4, 64] x F_1 [4, 64] x F_2 [4, 64] x F_3 [4, 64] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1048576*(4x64^1)_NN_d
Tuning for shape 1x65536*(4x64^2)_NN_d
Tuning for shape 1x4096*(4x64^3)_NN_d
Tuning for shape 1x256*(4x64^4)_NN_d
Tuning for shape 1x65536*(4x64^1)_NN_d
Tuning for shape 1x4096*(4x64^2)_NN_d
Tuning for shape 1x256*(4x64^3)_NN_d
Tuning for shape 1x4096*(4x64^1)_NN_d
Tuning for shape 1x256*(4x64^2)_NN_d
Tuning for shape 1x256*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x64^4  &  1.000 & 1.000 & 173.079 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [4, 64] x F_1 [4, 64] x F_2 [4, 64] x F_3 [4, 64] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1048576*(4x64^1)_NN_d
Tuning for shape 4x65536*(4x64^2)_NN_d
Tuning for shape 4x4096*(4x64^3)_NN_d
Tuning for shape 4x256*(4x64^4)_NN_d
Tuning for shape 4x65536*(4x64^1)_NN_d
Tuning for shape 4x4096*(4x64^2)_NN_d
Tuning for shape 4x256*(4x64^3)_NN_d
Tuning for shape 4x4096*(4x64^1)_NN_d
Tuning for shape 4x256*(4x64^2)_NN_d
Tuning for shape 4x256*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x64^4  &  1.000 & 1.000 & 219.803 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 4 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [4, 64] x F_1 [4, 64] x F_2 [4, 64] x F_3 [4, 64] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1048576*(4x64^1)_NN_d
Tuning for shape 16x65536*(4x64^2)_NN_d
Tuning for shape 16x4096*(4x64^3)_NN_d
Tuning for shape 16x256*(4x64^4)_NN_d
Tuning for shape 16x65536*(4x64^1)_NN_d
Tuning for shape 16x4096*(4x64^2)_NN_d
Tuning for shape 16x256*(4x64^3)_NN_d
Tuning for shape 16x4096*(4x64^1)_NN_d
Tuning for shape 16x256*(4x64^2)_NN_d
Tuning for shape 16x256*(4x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x64^4  &  1.000 & 1.000 & 259.720 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4] with F_0 [4, 128] x to produce Y[1, 128]
Matmul: 1 x 128 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x128^1  &  1.000 & 1.000 & 0.004 & 249.711
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4] with F_0 [4, 128] x to produce Y[4, 128]
Matmul: 4 x 128 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x128^1  &  1.000 & 1.000 & 0.013 & 77.309
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4] with F_0 [4, 128] x to produce Y[16, 128]
Matmul: 16 x 128 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x128^1  &  1.000 & 1.000 & 0.051 & 19.433
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4] with F_0 [4, 128] x to produce Y[64, 128]
Matmul: 64 x 128 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x128^1  &  1.000 & 1.000 & 0.213 & 4.684
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4] with F_0 [4, 128] x to produce Y[256, 128]
Matmul: 256 x 128 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x128^1  &  1.000 & 1.000 & 0.859 & 1.165
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4] with F_0 [4, 128] x to produce Y[1024, 128]
Matmul: 1024 x 128 x 4, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x128^1  &  1.000 & 1.000 & 3.404 & 0.294
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [4, 128] x F_1 [4, 128] x to produce Y[1, 16384]
Matmul: 1 x 16384 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(4x128^1)_NN_d
Tuning for shape 1x16*(4x128^2)_NN_d
Tuning for shape 1x16*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x128^2  &  1.000 & 1.000 & 0.326 & 3.070
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [4, 128] x F_1 [4, 128] x to produce Y[4, 16384]
Matmul: 4 x 16384 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(4x128^1)_NN_d
Tuning for shape 4x16*(4x128^2)_NN_d
Tuning for shape 4x16*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x128^2  &  1.000 & 1.000 & 1.207 & 0.829
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [4, 128] x F_1 [4, 128] x to produce Y[16, 16384]
Matmul: 16 x 16384 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(4x128^1)_NN_d
Tuning for shape 16x16*(4x128^2)_NN_d
Tuning for shape 16x16*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x128^2  &  1.000 & 1.000 & 4.693 & 0.213
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [4, 128] x F_1 [4, 128] x to produce Y[64, 16384]
Matmul: 64 x 16384 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x512*(4x128^1)_NN_d
Tuning for shape 64x16*(4x128^2)_NN_d
Tuning for shape 64x16*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x128^2  &  1.000 & 1.000 & 19.191 & 0.052
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [4, 128] x F_1 [4, 128] x to produce Y[256, 16384]
Matmul: 256 x 16384 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x512*(4x128^1)_NN_d
Tuning for shape 256x16*(4x128^2)_NN_d
Tuning for shape 256x16*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x128^2  &  1.000 & 1.000 & 75.676 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [4, 128] x F_1 [4, 128] x to produce Y[1024, 16384]
Matmul: 1024 x 16384 x 16, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x512*(4x128^1)_NN_d
Tuning for shape 1024x16*(4x128^2)_NN_d
Tuning for shape 1024x16*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_4x128^2  &  1.000 & 1.000 & 246.945 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [4, 128] x F_1 [4, 128] x F_2 [4, 128] x to produce Y[1, 2097152]
Matmul: 1 x 2097152 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x65536*(4x128^1)_NN_d
Tuning for shape 1x2048*(4x128^2)_NN_d
Tuning for shape 1x64*(4x128^3)_NN_d
Tuning for shape 1x2048*(4x128^1)_NN_d
Tuning for shape 1x64*(4x128^2)_NN_d
Tuning for shape 1x64*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x128^3  &  1.000 & 1.000 & 32.766 & 0.031
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [4, 128] x F_1 [4, 128] x F_2 [4, 128] x to produce Y[4, 2097152]
Matmul: 4 x 2097152 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x65536*(4x128^1)_NN_d
Tuning for shape 4x2048*(4x128^2)_NN_d
Tuning for shape 4x64*(4x128^3)_NN_d
Tuning for shape 4x2048*(4x128^1)_NN_d
Tuning for shape 4x64*(4x128^2)_NN_d
Tuning for shape 4x64*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_4x128^3  &  1.000 & 1.000 & 121.575 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [4, 128] x F_1 [4, 128] x F_2 [4, 128] x to produce Y[16, 2097152]
Matmul: 16 x 2097152 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x65536*(4x128^1)_NN_d
Tuning for shape 16x2048*(4x128^2)_NN_d
Tuning for shape 16x64*(4x128^3)_NN_d
Tuning for shape 16x2048*(4x128^1)_NN_d
Tuning for shape 16x64*(4x128^2)_NN_d
Tuning for shape 16x64*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_4x128^3  &  1.000 & 1.000 & 246.455 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [4, 128] x F_1 [4, 128] x F_2 [4, 128] x to produce Y[64, 2097152]
Matmul: 64 x 2097152 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x65536*(4x128^1)_NN_d
Tuning for shape 64x2048*(4x128^2)_NN_d
Tuning for shape 64x64*(4x128^3)_NN_d
Tuning for shape 64x2048*(4x128^1)_NN_d
Tuning for shape 64x64*(4x128^2)_NN_d
Tuning for shape 64x64*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_4x128^3  &  1.000 & 1.000 & 266.344 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [4, 128] x F_1 [4, 128] x F_2 [4, 128] x to produce Y[256, 2097152]
Matmul: 256 x 2097152 x 64, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x65536*(4x128^1)_NN_d
Tuning for shape 256x2048*(4x128^2)_NN_d
Tuning for shape 256x64*(4x128^3)_NN_d
Tuning for shape 256x2048*(4x128^1)_NN_d
Tuning for shape 256x64*(4x128^2)_NN_d
Tuning for shape 256x64*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_4x128^3  &  1.000 & 1.000 & 266.614 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 4 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [4, 128] x F_1 [4, 128] x F_2 [4, 128] x F_3 [4, 128] x to produce Y[1, 268435456]
Matmul: 1 x 268435456 x 256, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8388608*(4x128^1)_NN_d
Tuning for shape 1x262144*(4x128^2)_NN_d
Tuning for shape 1x8192*(4x128^3)_NN_d
Tuning for shape 1x256*(4x128^4)_NN_d
Tuning for shape 1x262144*(4x128^1)_NN_d
Tuning for shape 1x8192*(4x128^2)_NN_d
Tuning for shape 1x256*(4x128^3)_NN_d
Tuning for shape 1x8192*(4x128^1)_NN_d
Tuning for shape 1x256*(4x128^2)_NN_d
Tuning for shape 1x256*(4x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_4x128^4  &  1.000 & 1.000 & 76.956 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [8, 2] x to produce Y[1, 2]
Matmul: 1 x 2 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x2^1  &  1.000 & 1.000 & 0.000 & 7815.287
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [8, 2] x to produce Y[4, 2]
Matmul: 4 x 2 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x2^1  &  1.000 & 1.000 & 0.000 & 2396.759
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [8, 2] x to produce Y[16, 2]
Matmul: 16 x 2 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x2^1  &  1.000 & 1.000 & 0.002 & 593.020
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [8, 2] x to produce Y[64, 2]
Matmul: 64 x 2 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x2^1  &  1.000 & 1.000 & 0.007 & 151.020
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [8, 2] x to produce Y[256, 2]
Matmul: 256 x 2 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x2^1  &  1.000 & 1.000 & 0.026 & 38.155
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [8, 2] x to produce Y[1024, 2]
Matmul: 1024 x 2 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x2^1  &  1.000 & 1.000 & 0.106 & 9.443
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [8, 2] x F_1 [8, 2] x to produce Y[1, 4]
Matmul: 1 x 4 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16*(8x2^1)_NN_d
Tuning for shape 1x64*(8x2^2)_NN_d
Tuning for shape 1x64*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x2^2  &  1.000 & 1.000 & 0.001 & 1295.842
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [8, 2] x F_1 [8, 2] x to produce Y[4, 4]
Matmul: 4 x 4 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16*(8x2^1)_NN_d
Tuning for shape 4x64*(8x2^2)_NN_d
Tuning for shape 4x64*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x2^2  &  1.000 & 1.000 & 0.003 & 360.431
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [8, 2] x F_1 [8, 2] x to produce Y[16, 4]
Matmul: 16 x 4 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16*(8x2^1)_NN_d
Tuning for shape 16x64*(8x2^2)_NN_d
Tuning for shape 16x64*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x2^2  &  1.000 & 1.000 & 0.011 & 91.454
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [8, 2] x F_1 [8, 2] x to produce Y[64, 4]
Matmul: 64 x 4 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16*(8x2^1)_NN_d
Tuning for shape 64x64*(8x2^2)_NN_d
Tuning for shape 64x64*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x2^2  &  1.000 & 1.000 & 0.046 & 21.837
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [8, 2] x F_1 [8, 2] x to produce Y[256, 4]
Matmul: 256 x 4 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16*(8x2^1)_NN_d
Tuning for shape 256x64*(8x2^2)_NN_d
Tuning for shape 256x64*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x2^2  &  1.000 & 1.000 & 0.169 & 5.909
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [8, 2] x F_1 [8, 2] x to produce Y[1024, 4]
Matmul: 1024 x 4 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16*(8x2^1)_NN_d
Tuning for shape 1024x64*(8x2^2)_NN_d
Tuning for shape 1024x64*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x2^2  &  1.000 & 1.000 & 0.716 & 1.396
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x to produce Y[1, 8]
Matmul: 1 x 8 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32*(8x2^1)_NN_d
Tuning for shape 1x128*(8x2^2)_NN_d
Tuning for shape 1x512*(8x2^3)_NN_d
Tuning for shape 1x128*(8x2^1)_NN_d
Tuning for shape 1x512*(8x2^2)_NN_d
Tuning for shape 1x512*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x2^3  &  1.000 & 1.000 & 0.005 & 191.507
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x to produce Y[4, 8]
Matmul: 4 x 8 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32*(8x2^1)_NN_d
Tuning for shape 4x128*(8x2^2)_NN_d
Tuning for shape 4x512*(8x2^3)_NN_d
Tuning for shape 4x128*(8x2^1)_NN_d
Tuning for shape 4x512*(8x2^2)_NN_d
Tuning for shape 4x512*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x2^3  &  1.000 & 1.000 & 0.019 & 51.947
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 512] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x to produce Y[16, 8]
Matmul: 16 x 8 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32*(8x2^1)_NN_d
Tuning for shape 16x128*(8x2^2)_NN_d
Tuning for shape 16x512*(8x2^3)_NN_d
Tuning for shape 16x128*(8x2^1)_NN_d
Tuning for shape 16x512*(8x2^2)_NN_d
Tuning for shape 16x512*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x2^3  &  1.000 & 1.000 & 0.078 & 12.883
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 512] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x to produce Y[64, 8]
Matmul: 64 x 8 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32*(8x2^1)_NN_d
Tuning for shape 64x128*(8x2^2)_NN_d
Tuning for shape 64x512*(8x2^3)_NN_d
Tuning for shape 64x128*(8x2^1)_NN_d
Tuning for shape 64x512*(8x2^2)_NN_d
Tuning for shape 64x512*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x2^3  &  1.000 & 1.000 & 0.312 & 3.206
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 512] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x to produce Y[256, 8]
Matmul: 256 x 8 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32*(8x2^1)_NN_d
Tuning for shape 256x128*(8x2^2)_NN_d
Tuning for shape 256x512*(8x2^3)_NN_d
Tuning for shape 256x128*(8x2^1)_NN_d
Tuning for shape 256x512*(8x2^2)_NN_d
Tuning for shape 256x512*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x2^3  &  1.000 & 1.000 & 1.274 & 0.785
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 512] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x to produce Y[1024, 8]
Matmul: 1024 x 8 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32*(8x2^1)_NN_d
Tuning for shape 1024x128*(8x2^2)_NN_d
Tuning for shape 1024x512*(8x2^3)_NN_d
Tuning for shape 1024x128*(8x2^1)_NN_d
Tuning for shape 1024x512*(8x2^2)_NN_d
Tuning for shape 1024x512*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x2^3  &  1.000 & 1.000 & 4.966 & 0.201
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x to produce Y[1, 16]
Matmul: 1 x 16 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x64*(8x2^1)_NN_d
Tuning for shape 1x256*(8x2^2)_NN_d
Tuning for shape 1x1024*(8x2^3)_NN_d
Tuning for shape 1x4096*(8x2^4)_NN_d
Tuning for shape 1x256*(8x2^1)_NN_d
Tuning for shape 1x1024*(8x2^2)_NN_d
Tuning for shape 1x4096*(8x2^3)_NN_d
Tuning for shape 1x1024*(8x2^1)_NN_d
Tuning for shape 1x4096*(8x2^2)_NN_d
Tuning for shape 1x4096*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x2^4  &  1.000 & 1.000 & 0.035 & 28.725
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x to produce Y[4, 16]
Matmul: 4 x 16 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x64*(8x2^1)_NN_d
Tuning for shape 4x256*(8x2^2)_NN_d
Tuning for shape 4x1024*(8x2^3)_NN_d
Tuning for shape 4x4096*(8x2^4)_NN_d
Tuning for shape 4x256*(8x2^1)_NN_d
Tuning for shape 4x1024*(8x2^2)_NN_d
Tuning for shape 4x4096*(8x2^3)_NN_d
Tuning for shape 4x1024*(8x2^1)_NN_d
Tuning for shape 4x4096*(8x2^2)_NN_d
Tuning for shape 4x4096*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x2^4  &  1.000 & 1.000 & 0.134 & 7.437
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x to produce Y[16, 16]
Matmul: 16 x 16 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x64*(8x2^1)_NN_d
Tuning for shape 16x256*(8x2^2)_NN_d
Tuning for shape 16x1024*(8x2^3)_NN_d
Tuning for shape 16x4096*(8x2^4)_NN_d
Tuning for shape 16x256*(8x2^1)_NN_d
Tuning for shape 16x1024*(8x2^2)_NN_d
Tuning for shape 16x4096*(8x2^3)_NN_d
Tuning for shape 16x1024*(8x2^1)_NN_d
Tuning for shape 16x4096*(8x2^2)_NN_d
Tuning for shape 16x4096*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x2^4  &  1.000 & 1.000 & 0.493 & 2.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x to produce Y[64, 16]
Matmul: 64 x 16 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x64*(8x2^1)_NN_d
Tuning for shape 64x256*(8x2^2)_NN_d
Tuning for shape 64x1024*(8x2^3)_NN_d
Tuning for shape 64x4096*(8x2^4)_NN_d
Tuning for shape 64x256*(8x2^1)_NN_d
Tuning for shape 64x1024*(8x2^2)_NN_d
Tuning for shape 64x4096*(8x2^3)_NN_d
Tuning for shape 64x1024*(8x2^1)_NN_d
Tuning for shape 64x4096*(8x2^2)_NN_d
Tuning for shape 64x4096*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x2^4  &  1.000 & 1.000 & 2.183 & 0.458
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x to produce Y[256, 16]
Matmul: 256 x 16 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x64*(8x2^1)_NN_d
Tuning for shape 256x256*(8x2^2)_NN_d
Tuning for shape 256x1024*(8x2^3)_NN_d
Tuning for shape 256x4096*(8x2^4)_NN_d
Tuning for shape 256x256*(8x2^1)_NN_d
Tuning for shape 256x1024*(8x2^2)_NN_d
Tuning for shape 256x4096*(8x2^3)_NN_d
Tuning for shape 256x1024*(8x2^1)_NN_d
Tuning for shape 256x4096*(8x2^2)_NN_d
Tuning for shape 256x4096*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x2^4  &  1.000 & 1.000 & 8.608 & 0.116
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x64*(8x2^1)_NN_d
Tuning for shape 1024x256*(8x2^2)_NN_d
Tuning for shape 1024x1024*(8x2^3)_NN_d
Tuning for shape 1024x4096*(8x2^4)_NN_d
Tuning for shape 1024x256*(8x2^1)_NN_d
Tuning for shape 1024x1024*(8x2^2)_NN_d
Tuning for shape 1024x4096*(8x2^3)_NN_d
Tuning for shape 1024x1024*(8x2^1)_NN_d
Tuning for shape 1024x4096*(8x2^2)_NN_d
Tuning for shape 1024x4096*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x2^4  &  1.000 & 1.000 & 33.763 & 0.030
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32768] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x to produce Y[1, 32]
Matmul: 1 x 32 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(8x2^1)_NN_d
Tuning for shape 1x512*(8x2^2)_NN_d
Tuning for shape 1x2048*(8x2^3)_NN_d
Tuning for shape 1x8192*(8x2^4)_NN_d
Tuning for shape 1x32768*(8x2^5)_NN_d
Tuning for shape 1x512*(8x2^1)_NN_d
Tuning for shape 1x2048*(8x2^2)_NN_d
Tuning for shape 1x8192*(8x2^3)_NN_d
Tuning for shape 1x32768*(8x2^4)_NN_d
Tuning for shape 1x2048*(8x2^1)_NN_d
Tuning for shape 1x8192*(8x2^2)_NN_d
Tuning for shape 1x32768*(8x2^3)_NN_d
Tuning for shape 1x8192*(8x2^1)_NN_d
Tuning for shape 1x32768*(8x2^2)_NN_d
Tuning for shape 1x32768*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x2^5  &  1.000 & 1.000 & 0.243 & 4.119
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32768] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x to produce Y[4, 32]
Matmul: 4 x 32 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(8x2^1)_NN_d
Tuning for shape 4x512*(8x2^2)_NN_d
Tuning for shape 4x2048*(8x2^3)_NN_d
Tuning for shape 4x8192*(8x2^4)_NN_d
Tuning for shape 4x32768*(8x2^5)_NN_d
Tuning for shape 4x512*(8x2^1)_NN_d
Tuning for shape 4x2048*(8x2^2)_NN_d
Tuning for shape 4x8192*(8x2^3)_NN_d
Tuning for shape 4x32768*(8x2^4)_NN_d
Tuning for shape 4x2048*(8x2^1)_NN_d
Tuning for shape 4x8192*(8x2^2)_NN_d
Tuning for shape 4x32768*(8x2^3)_NN_d
Tuning for shape 4x8192*(8x2^1)_NN_d
Tuning for shape 4x32768*(8x2^2)_NN_d
Tuning for shape 4x32768*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x2^5  &  1.000 & 1.000 & 0.939 & 1.065
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32768] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x to produce Y[16, 32]
Matmul: 16 x 32 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(8x2^1)_NN_d
Tuning for shape 16x512*(8x2^2)_NN_d
Tuning for shape 16x2048*(8x2^3)_NN_d
Tuning for shape 16x8192*(8x2^4)_NN_d
Tuning for shape 16x32768*(8x2^5)_NN_d
Tuning for shape 16x512*(8x2^1)_NN_d
Tuning for shape 16x2048*(8x2^2)_NN_d
Tuning for shape 16x8192*(8x2^3)_NN_d
Tuning for shape 16x32768*(8x2^4)_NN_d
Tuning for shape 16x2048*(8x2^1)_NN_d
Tuning for shape 16x8192*(8x2^2)_NN_d
Tuning for shape 16x32768*(8x2^3)_NN_d
Tuning for shape 16x8192*(8x2^1)_NN_d
Tuning for shape 16x32768*(8x2^2)_NN_d
Tuning for shape 16x32768*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x2^5  &  1.000 & 1.000 & 3.701 & 0.270
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 32768] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x to produce Y[64, 32]
Matmul: 64 x 32 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(8x2^1)_NN_d
Tuning for shape 64x512*(8x2^2)_NN_d
Tuning for shape 64x2048*(8x2^3)_NN_d
Tuning for shape 64x8192*(8x2^4)_NN_d
Tuning for shape 64x32768*(8x2^5)_NN_d
Tuning for shape 64x512*(8x2^1)_NN_d
Tuning for shape 64x2048*(8x2^2)_NN_d
Tuning for shape 64x8192*(8x2^3)_NN_d
Tuning for shape 64x32768*(8x2^4)_NN_d
Tuning for shape 64x2048*(8x2^1)_NN_d
Tuning for shape 64x8192*(8x2^2)_NN_d
Tuning for shape 64x32768*(8x2^3)_NN_d
Tuning for shape 64x8192*(8x2^1)_NN_d
Tuning for shape 64x32768*(8x2^2)_NN_d
Tuning for shape 64x32768*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x2^5  &  1.000 & 1.000 & 14.741 & 0.068
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 32768] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x to produce Y[256, 32]
Matmul: 256 x 32 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(8x2^1)_NN_d
Tuning for shape 256x512*(8x2^2)_NN_d
Tuning for shape 256x2048*(8x2^3)_NN_d
Tuning for shape 256x8192*(8x2^4)_NN_d
Tuning for shape 256x32768*(8x2^5)_NN_d
Tuning for shape 256x512*(8x2^1)_NN_d
Tuning for shape 256x2048*(8x2^2)_NN_d
Tuning for shape 256x8192*(8x2^3)_NN_d
Tuning for shape 256x32768*(8x2^4)_NN_d
Tuning for shape 256x2048*(8x2^1)_NN_d
Tuning for shape 256x8192*(8x2^2)_NN_d
Tuning for shape 256x32768*(8x2^3)_NN_d
Tuning for shape 256x8192*(8x2^1)_NN_d
Tuning for shape 256x32768*(8x2^2)_NN_d
Tuning for shape 256x32768*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x2^5  &  1.000 & 1.000 & 44.200 & 0.023
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 5 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 32768] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x to produce Y[1024, 32]
Matmul: 1024 x 32 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(8x2^1)_NN_d
Tuning for shape 1024x512*(8x2^2)_NN_d
Tuning for shape 1024x2048*(8x2^3)_NN_d
Tuning for shape 1024x8192*(8x2^4)_NN_d
Tuning for shape 1024x32768*(8x2^5)_NN_d
Tuning for shape 1024x512*(8x2^1)_NN_d
Tuning for shape 1024x2048*(8x2^2)_NN_d
Tuning for shape 1024x8192*(8x2^3)_NN_d
Tuning for shape 1024x32768*(8x2^4)_NN_d
Tuning for shape 1024x2048*(8x2^1)_NN_d
Tuning for shape 1024x8192*(8x2^2)_NN_d
Tuning for shape 1024x32768*(8x2^3)_NN_d
Tuning for shape 1024x8192*(8x2^1)_NN_d
Tuning for shape 1024x32768*(8x2^2)_NN_d
Tuning for shape 1024x32768*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x2^5  &  1.000 & 1.000 & 42.588 & 0.023
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 262144] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x to produce Y[1, 64]
Matmul: 1 x 64 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(8x2^1)_NN_d
Tuning for shape 1x1024*(8x2^2)_NN_d
Tuning for shape 1x4096*(8x2^3)_NN_d
Tuning for shape 1x16384*(8x2^4)_NN_d
Tuning for shape 1x65536*(8x2^5)_NN_d
Tuning for shape 1x262144*(8x2^6)_NN_d
Tuning for shape 1x1024*(8x2^1)_NN_d
Tuning for shape 1x4096*(8x2^2)_NN_d
Tuning for shape 1x16384*(8x2^3)_NN_d
Tuning for shape 1x65536*(8x2^4)_NN_d
Tuning for shape 1x262144*(8x2^5)_NN_d
Tuning for shape 1x4096*(8x2^1)_NN_d
Tuning for shape 1x16384*(8x2^2)_NN_d
Tuning for shape 1x65536*(8x2^3)_NN_d
Tuning for shape 1x262144*(8x2^4)_NN_d
Tuning for shape 1x16384*(8x2^1)_NN_d
Tuning for shape 1x65536*(8x2^2)_NN_d
Tuning for shape 1x262144*(8x2^3)_NN_d
Tuning for shape 1x65536*(8x2^1)_NN_d
Tuning for shape 1x262144*(8x2^2)_NN_d
Tuning for shape 1x262144*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x2^6  &  1.000 & 1.000 & 1.769 & 0.565
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 262144] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x to produce Y[4, 64]
Matmul: 4 x 64 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(8x2^1)_NN_d
Tuning for shape 4x1024*(8x2^2)_NN_d
Tuning for shape 4x4096*(8x2^3)_NN_d
Tuning for shape 4x16384*(8x2^4)_NN_d
Tuning for shape 4x65536*(8x2^5)_NN_d
Tuning for shape 4x262144*(8x2^6)_NN_d
Tuning for shape 4x1024*(8x2^1)_NN_d
Tuning for shape 4x4096*(8x2^2)_NN_d
Tuning for shape 4x16384*(8x2^3)_NN_d
Tuning for shape 4x65536*(8x2^4)_NN_d
Tuning for shape 4x262144*(8x2^5)_NN_d
Tuning for shape 4x4096*(8x2^1)_NN_d
Tuning for shape 4x16384*(8x2^2)_NN_d
Tuning for shape 4x65536*(8x2^3)_NN_d
Tuning for shape 4x262144*(8x2^4)_NN_d
Tuning for shape 4x16384*(8x2^1)_NN_d
Tuning for shape 4x65536*(8x2^2)_NN_d
Tuning for shape 4x262144*(8x2^3)_NN_d
Tuning for shape 4x65536*(8x2^1)_NN_d
Tuning for shape 4x262144*(8x2^2)_NN_d
Tuning for shape 4x262144*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x2^6  &  1.000 & 1.000 & 6.629 & 0.151
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 262144] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x to produce Y[16, 64]
Matmul: 16 x 64 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(8x2^1)_NN_d
Tuning for shape 16x1024*(8x2^2)_NN_d
Tuning for shape 16x4096*(8x2^3)_NN_d
Tuning for shape 16x16384*(8x2^4)_NN_d
Tuning for shape 16x65536*(8x2^5)_NN_d
Tuning for shape 16x262144*(8x2^6)_NN_d
Tuning for shape 16x1024*(8x2^1)_NN_d
Tuning for shape 16x4096*(8x2^2)_NN_d
Tuning for shape 16x16384*(8x2^3)_NN_d
Tuning for shape 16x65536*(8x2^4)_NN_d
Tuning for shape 16x262144*(8x2^5)_NN_d
Tuning for shape 16x4096*(8x2^1)_NN_d
Tuning for shape 16x16384*(8x2^2)_NN_d
Tuning for shape 16x65536*(8x2^3)_NN_d
Tuning for shape 16x262144*(8x2^4)_NN_d
Tuning for shape 16x16384*(8x2^1)_NN_d
Tuning for shape 16x65536*(8x2^2)_NN_d
Tuning for shape 16x262144*(8x2^3)_NN_d
Tuning for shape 16x65536*(8x2^1)_NN_d
Tuning for shape 16x262144*(8x2^2)_NN_d
Tuning for shape 16x262144*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x2^6  &  1.000 & 1.000 & 26.857 & 0.037
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 6 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 262144] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x to produce Y[64, 64]
Matmul: 64 x 64 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(8x2^1)_NN_d
Tuning for shape 64x1024*(8x2^2)_NN_d
Tuning for shape 64x4096*(8x2^3)_NN_d
Tuning for shape 64x16384*(8x2^4)_NN_d
Tuning for shape 64x65536*(8x2^5)_NN_d
Tuning for shape 64x262144*(8x2^6)_NN_d
Tuning for shape 64x1024*(8x2^1)_NN_d
Tuning for shape 64x4096*(8x2^2)_NN_d
Tuning for shape 64x16384*(8x2^3)_NN_d
Tuning for shape 64x65536*(8x2^4)_NN_d
Tuning for shape 64x262144*(8x2^5)_NN_d
Tuning for shape 64x4096*(8x2^1)_NN_d
Tuning for shape 64x16384*(8x2^2)_NN_d
Tuning for shape 64x65536*(8x2^3)_NN_d
Tuning for shape 64x262144*(8x2^4)_NN_d
Tuning for shape 64x16384*(8x2^1)_NN_d
Tuning for shape 64x65536*(8x2^2)_NN_d
Tuning for shape 64x262144*(8x2^3)_NN_d
Tuning for shape 64x65536*(8x2^1)_NN_d
Tuning for shape 64x262144*(8x2^2)_NN_d
Tuning for shape 64x262144*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x2^6  &  1.000 & 1.000 & 53.918 & 0.019
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 6 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 262144] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x to produce Y[256, 64]
Matmul: 256 x 64 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(8x2^1)_NN_d
Tuning for shape 256x1024*(8x2^2)_NN_d
Tuning for shape 256x4096*(8x2^3)_NN_d
Tuning for shape 256x16384*(8x2^4)_NN_d
Tuning for shape 256x65536*(8x2^5)_NN_d
Tuning for shape 256x262144*(8x2^6)_NN_d
Tuning for shape 256x1024*(8x2^1)_NN_d
Tuning for shape 256x4096*(8x2^2)_NN_d
Tuning for shape 256x16384*(8x2^3)_NN_d
Tuning for shape 256x65536*(8x2^4)_NN_d
Tuning for shape 256x262144*(8x2^5)_NN_d
Tuning for shape 256x4096*(8x2^1)_NN_d
Tuning for shape 256x16384*(8x2^2)_NN_d
Tuning for shape 256x65536*(8x2^3)_NN_d
Tuning for shape 256x262144*(8x2^4)_NN_d
Tuning for shape 256x16384*(8x2^1)_NN_d
Tuning for shape 256x65536*(8x2^2)_NN_d
Tuning for shape 256x262144*(8x2^3)_NN_d
Tuning for shape 256x65536*(8x2^1)_NN_d
Tuning for shape 256x262144*(8x2^2)_NN_d
Tuning for shape 256x262144*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x2^6  &  1.000 & 1.000 & 30.410 & 0.033
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 6 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 262144] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x256*(8x2^1)_NN_d
Tuning for shape 1024x1024*(8x2^2)_NN_d
Tuning for shape 1024x4096*(8x2^3)_NN_d
Tuning for shape 1024x16384*(8x2^4)_NN_d
Tuning for shape 1024x65536*(8x2^5)_NN_d
Tuning for shape 1024x262144*(8x2^6)_NN_d
Tuning for shape 1024x1024*(8x2^1)_NN_d
Tuning for shape 1024x4096*(8x2^2)_NN_d
Tuning for shape 1024x16384*(8x2^3)_NN_d
Tuning for shape 1024x65536*(8x2^4)_NN_d
Tuning for shape 1024x262144*(8x2^5)_NN_d
Tuning for shape 1024x4096*(8x2^1)_NN_d
Tuning for shape 1024x16384*(8x2^2)_NN_d
Tuning for shape 1024x65536*(8x2^3)_NN_d
Tuning for shape 1024x262144*(8x2^4)_NN_d
Tuning for shape 1024x16384*(8x2^1)_NN_d
Tuning for shape 1024x65536*(8x2^2)_NN_d
Tuning for shape 1024x262144*(8x2^3)_NN_d
Tuning for shape 1024x65536*(8x2^1)_NN_d
Tuning for shape 1024x262144*(8x2^2)_NN_d
Tuning for shape 1024x262144*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x2^6  &  1.000 & 1.000 & 29.528 & 0.034
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2097152] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x to produce Y[1, 128]
Matmul: 1 x 128 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(8x2^1)_NN_d
Tuning for shape 1x2048*(8x2^2)_NN_d
Tuning for shape 1x8192*(8x2^3)_NN_d
Tuning for shape 1x32768*(8x2^4)_NN_d
Tuning for shape 1x131072*(8x2^5)_NN_d
Tuning for shape 1x524288*(8x2^6)_NN_d
Tuning for shape 1x2097152*(8x2^7)_NN_d
Tuning for shape 1x2048*(8x2^1)_NN_d
Tuning for shape 1x8192*(8x2^2)_NN_d
Tuning for shape 1x32768*(8x2^3)_NN_d
Tuning for shape 1x131072*(8x2^4)_NN_d
Tuning for shape 1x524288*(8x2^5)_NN_d
Tuning for shape 1x2097152*(8x2^6)_NN_d
Tuning for shape 1x8192*(8x2^1)_NN_d
Tuning for shape 1x32768*(8x2^2)_NN_d
Tuning for shape 1x131072*(8x2^3)_NN_d
Tuning for shape 1x524288*(8x2^4)_NN_d
Tuning for shape 1x2097152*(8x2^5)_NN_d
Tuning for shape 1x32768*(8x2^1)_NN_d
Tuning for shape 1x131072*(8x2^2)_NN_d
Tuning for shape 1x524288*(8x2^3)_NN_d
Tuning for shape 1x2097152*(8x2^4)_NN_d
Tuning for shape 1x131072*(8x2^1)_NN_d
Tuning for shape 1x524288*(8x2^2)_NN_d
Tuning for shape 1x2097152*(8x2^3)_NN_d
Tuning for shape 1x524288*(8x2^1)_NN_d
Tuning for shape 1x2097152*(8x2^2)_NN_d
Tuning for shape 1x2097152*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x2^7  &  1.000 & 1.000 & 12.420 & 0.081
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 7 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2097152] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x to produce Y[4, 128]
Matmul: 4 x 128 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(8x2^1)_NN_d
Tuning for shape 4x2048*(8x2^2)_NN_d
Tuning for shape 4x8192*(8x2^3)_NN_d
Tuning for shape 4x32768*(8x2^4)_NN_d
Tuning for shape 4x131072*(8x2^5)_NN_d
Tuning for shape 4x524288*(8x2^6)_NN_d
Tuning for shape 4x2097152*(8x2^7)_NN_d
Tuning for shape 4x2048*(8x2^1)_NN_d
Tuning for shape 4x8192*(8x2^2)_NN_d
Tuning for shape 4x32768*(8x2^3)_NN_d
Tuning for shape 4x131072*(8x2^4)_NN_d
Tuning for shape 4x524288*(8x2^5)_NN_d
Tuning for shape 4x2097152*(8x2^6)_NN_d
Tuning for shape 4x8192*(8x2^1)_NN_d
Tuning for shape 4x32768*(8x2^2)_NN_d
Tuning for shape 4x131072*(8x2^3)_NN_d
Tuning for shape 4x524288*(8x2^4)_NN_d
Tuning for shape 4x2097152*(8x2^5)_NN_d
Tuning for shape 4x32768*(8x2^1)_NN_d
Tuning for shape 4x131072*(8x2^2)_NN_d
Tuning for shape 4x524288*(8x2^3)_NN_d
Tuning for shape 4x2097152*(8x2^4)_NN_d
Tuning for shape 4x131072*(8x2^1)_NN_d
Tuning for shape 4x524288*(8x2^2)_NN_d
Tuning for shape 4x2097152*(8x2^3)_NN_d
Tuning for shape 4x524288*(8x2^1)_NN_d
Tuning for shape 4x2097152*(8x2^2)_NN_d
Tuning for shape 4x2097152*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x2^7  &  1.000 & 1.000 & 44.813 & 0.022
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 7 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2097152] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x to produce Y[16, 128]
Matmul: 16 x 128 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(8x2^1)_NN_d
Tuning for shape 16x2048*(8x2^2)_NN_d
Tuning for shape 16x8192*(8x2^3)_NN_d
Tuning for shape 16x32768*(8x2^4)_NN_d
Tuning for shape 16x131072*(8x2^5)_NN_d
Tuning for shape 16x524288*(8x2^6)_NN_d
Tuning for shape 16x2097152*(8x2^7)_NN_d
Tuning for shape 16x2048*(8x2^1)_NN_d
Tuning for shape 16x8192*(8x2^2)_NN_d
Tuning for shape 16x32768*(8x2^3)_NN_d
Tuning for shape 16x131072*(8x2^4)_NN_d
Tuning for shape 16x524288*(8x2^5)_NN_d
Tuning for shape 16x2097152*(8x2^6)_NN_d
Tuning for shape 16x8192*(8x2^1)_NN_d
Tuning for shape 16x32768*(8x2^2)_NN_d
Tuning for shape 16x131072*(8x2^3)_NN_d
Tuning for shape 16x524288*(8x2^4)_NN_d
Tuning for shape 16x2097152*(8x2^5)_NN_d
Tuning for shape 16x32768*(8x2^1)_NN_d
Tuning for shape 16x131072*(8x2^2)_NN_d
Tuning for shape 16x524288*(8x2^3)_NN_d
Tuning for shape 16x2097152*(8x2^4)_NN_d
Tuning for shape 16x131072*(8x2^1)_NN_d
Tuning for shape 16x524288*(8x2^2)_NN_d
Tuning for shape 16x2097152*(8x2^3)_NN_d
Tuning for shape 16x524288*(8x2^1)_NN_d
Tuning for shape 16x2097152*(8x2^2)_NN_d
Tuning for shape 16x2097152*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x2^7  &  1.000 & 1.000 & 63.118 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 7 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2097152] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x to produce Y[64, 128]
Matmul: 64 x 128 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x512*(8x2^1)_NN_d
Tuning for shape 64x2048*(8x2^2)_NN_d
Tuning for shape 64x8192*(8x2^3)_NN_d
Tuning for shape 64x32768*(8x2^4)_NN_d
Tuning for shape 64x131072*(8x2^5)_NN_d
Tuning for shape 64x524288*(8x2^6)_NN_d
Tuning for shape 64x2097152*(8x2^7)_NN_d
Tuning for shape 64x2048*(8x2^1)_NN_d
Tuning for shape 64x8192*(8x2^2)_NN_d
Tuning for shape 64x32768*(8x2^3)_NN_d
Tuning for shape 64x131072*(8x2^4)_NN_d
Tuning for shape 64x524288*(8x2^5)_NN_d
Tuning for shape 64x2097152*(8x2^6)_NN_d
Tuning for shape 64x8192*(8x2^1)_NN_d
Tuning for shape 64x32768*(8x2^2)_NN_d
Tuning for shape 64x131072*(8x2^3)_NN_d
Tuning for shape 64x524288*(8x2^4)_NN_d
Tuning for shape 64x2097152*(8x2^5)_NN_d
Tuning for shape 64x32768*(8x2^1)_NN_d
Tuning for shape 64x131072*(8x2^2)_NN_d
Tuning for shape 64x524288*(8x2^3)_NN_d
Tuning for shape 64x2097152*(8x2^4)_NN_d
Tuning for shape 64x131072*(8x2^1)_NN_d
Tuning for shape 64x524288*(8x2^2)_NN_d
Tuning for shape 64x2097152*(8x2^3)_NN_d
Tuning for shape 64x524288*(8x2^1)_NN_d
Tuning for shape 64x2097152*(8x2^2)_NN_d
Tuning for shape 64x2097152*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x2^7  &  1.000 & 1.000 & 55.489 & 0.018
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 7 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2097152] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x to produce Y[256, 128]
Matmul: 256 x 128 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x512*(8x2^1)_NN_d
Tuning for shape 256x2048*(8x2^2)_NN_d
Tuning for shape 256x8192*(8x2^3)_NN_d
Tuning for shape 256x32768*(8x2^4)_NN_d
Tuning for shape 256x131072*(8x2^5)_NN_d
Tuning for shape 256x524288*(8x2^6)_NN_d
Tuning for shape 256x2097152*(8x2^7)_NN_d
Tuning for shape 256x2048*(8x2^1)_NN_d
Tuning for shape 256x8192*(8x2^2)_NN_d
Tuning for shape 256x32768*(8x2^3)_NN_d
Tuning for shape 256x131072*(8x2^4)_NN_d
Tuning for shape 256x524288*(8x2^5)_NN_d
Tuning for shape 256x2097152*(8x2^6)_NN_d
Tuning for shape 256x8192*(8x2^1)_NN_d
Tuning for shape 256x32768*(8x2^2)_NN_d
Tuning for shape 256x131072*(8x2^3)_NN_d
Tuning for shape 256x524288*(8x2^4)_NN_d
Tuning for shape 256x2097152*(8x2^5)_NN_d
Tuning for shape 256x32768*(8x2^1)_NN_d
Tuning for shape 256x131072*(8x2^2)_NN_d
Tuning for shape 256x524288*(8x2^3)_NN_d
Tuning for shape 256x2097152*(8x2^4)_NN_d
Tuning for shape 256x131072*(8x2^1)_NN_d
Tuning for shape 256x524288*(8x2^2)_NN_d
Tuning for shape 256x2097152*(8x2^3)_NN_d
Tuning for shape 256x524288*(8x2^1)_NN_d
Tuning for shape 256x2097152*(8x2^2)_NN_d
Tuning for shape 256x2097152*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x2^7  &  1.000 & 1.000 & 33.825 & 0.030
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 8 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16777216] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x F_7 [8, 2] x to produce Y[1, 256]
Matmul: 1 x 256 x 16777216, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(8x2^1)_NN_d
Tuning for shape 1x4096*(8x2^2)_NN_d
Tuning for shape 1x16384*(8x2^3)_NN_d
Tuning for shape 1x65536*(8x2^4)_NN_d
Tuning for shape 1x262144*(8x2^5)_NN_d
Tuning for shape 1x1048576*(8x2^6)_NN_d
Tuning for shape 1x4194304*(8x2^7)_NN_d
Tuning for shape 1x16777216*(8x2^8)_NN_d
Tuning for shape 1x4096*(8x2^1)_NN_d
Tuning for shape 1x16384*(8x2^2)_NN_d
Tuning for shape 1x65536*(8x2^3)_NN_d
Tuning for shape 1x262144*(8x2^4)_NN_d
Tuning for shape 1x1048576*(8x2^5)_NN_d
Tuning for shape 1x4194304*(8x2^6)_NN_d
Tuning for shape 1x16777216*(8x2^7)_NN_d
Tuning for shape 1x16384*(8x2^1)_NN_d
Tuning for shape 1x65536*(8x2^2)_NN_d
Tuning for shape 1x262144*(8x2^3)_NN_d
Tuning for shape 1x1048576*(8x2^4)_NN_d
Tuning for shape 1x4194304*(8x2^5)_NN_d
Tuning for shape 1x16777216*(8x2^6)_NN_d
Tuning for shape 1x65536*(8x2^1)_NN_d
Tuning for shape 1x262144*(8x2^2)_NN_d
Tuning for shape 1x1048576*(8x2^3)_NN_d
Tuning for shape 1x4194304*(8x2^4)_NN_d
Tuning for shape 1x16777216*(8x2^5)_NN_d
Tuning for shape 1x262144*(8x2^1)_NN_d
Tuning for shape 1x1048576*(8x2^2)_NN_d
Tuning for shape 1x4194304*(8x2^3)_NN_d
Tuning for shape 1x16777216*(8x2^4)_NN_d
Tuning for shape 1x1048576*(8x2^1)_NN_d
Tuning for shape 1x4194304*(8x2^2)_NN_d
Tuning for shape 1x16777216*(8x2^3)_NN_d
Tuning for shape 1x4194304*(8x2^1)_NN_d
Tuning for shape 1x16777216*(8x2^2)_NN_d
Tuning for shape 1x16777216*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x2^8  &  1.000 & 1.000 & 78.559 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 8 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16777216] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x F_7 [8, 2] x to produce Y[4, 256]
Matmul: 4 x 256 x 16777216, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1024*(8x2^1)_NN_d
Tuning for shape 4x4096*(8x2^2)_NN_d
Tuning for shape 4x16384*(8x2^3)_NN_d
Tuning for shape 4x65536*(8x2^4)_NN_d
Tuning for shape 4x262144*(8x2^5)_NN_d
Tuning for shape 4x1048576*(8x2^6)_NN_d
Tuning for shape 4x4194304*(8x2^7)_NN_d
Tuning for shape 4x16777216*(8x2^8)_NN_d
Tuning for shape 4x4096*(8x2^1)_NN_d
Tuning for shape 4x16384*(8x2^2)_NN_d
Tuning for shape 4x65536*(8x2^3)_NN_d
Tuning for shape 4x262144*(8x2^4)_NN_d
Tuning for shape 4x1048576*(8x2^5)_NN_d
Tuning for shape 4x4194304*(8x2^6)_NN_d
Tuning for shape 4x16777216*(8x2^7)_NN_d
Tuning for shape 4x16384*(8x2^1)_NN_d
Tuning for shape 4x65536*(8x2^2)_NN_d
Tuning for shape 4x262144*(8x2^3)_NN_d
Tuning for shape 4x1048576*(8x2^4)_NN_d
Tuning for shape 4x4194304*(8x2^5)_NN_d
Tuning for shape 4x16777216*(8x2^6)_NN_d
Tuning for shape 4x65536*(8x2^1)_NN_d
Tuning for shape 4x262144*(8x2^2)_NN_d
Tuning for shape 4x1048576*(8x2^3)_NN_d
Tuning for shape 4x4194304*(8x2^4)_NN_d
Tuning for shape 4x16777216*(8x2^5)_NN_d
Tuning for shape 4x262144*(8x2^1)_NN_d
Tuning for shape 4x1048576*(8x2^2)_NN_d
Tuning for shape 4x4194304*(8x2^3)_NN_d
Tuning for shape 4x16777216*(8x2^4)_NN_d
Tuning for shape 4x1048576*(8x2^1)_NN_d
Tuning for shape 4x4194304*(8x2^2)_NN_d
Tuning for shape 4x16777216*(8x2^3)_NN_d
Tuning for shape 4x4194304*(8x2^1)_NN_d
Tuning for shape 4x16777216*(8x2^2)_NN_d
Tuning for shape 4x16777216*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x2^8  &  1.000 & 1.000 & 63.985 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 8 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16777216] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x F_7 [8, 2] x to produce Y[16, 256]
Matmul: 16 x 256 x 16777216, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1024*(8x2^1)_NN_d
Tuning for shape 16x4096*(8x2^2)_NN_d
Tuning for shape 16x16384*(8x2^3)_NN_d
Tuning for shape 16x65536*(8x2^4)_NN_d
Tuning for shape 16x262144*(8x2^5)_NN_d
Tuning for shape 16x1048576*(8x2^6)_NN_d
Tuning for shape 16x4194304*(8x2^7)_NN_d
Tuning for shape 16x16777216*(8x2^8)_NN_d
Tuning for shape 16x4096*(8x2^1)_NN_d
Tuning for shape 16x16384*(8x2^2)_NN_d
Tuning for shape 16x65536*(8x2^3)_NN_d
Tuning for shape 16x262144*(8x2^4)_NN_d
Tuning for shape 16x1048576*(8x2^5)_NN_d
Tuning for shape 16x4194304*(8x2^6)_NN_d
Tuning for shape 16x16777216*(8x2^7)_NN_d
Tuning for shape 16x16384*(8x2^1)_NN_d
Tuning for shape 16x65536*(8x2^2)_NN_d
Tuning for shape 16x262144*(8x2^3)_NN_d
Tuning for shape 16x1048576*(8x2^4)_NN_d
Tuning for shape 16x4194304*(8x2^5)_NN_d
Tuning for shape 16x16777216*(8x2^6)_NN_d
Tuning for shape 16x65536*(8x2^1)_NN_d
Tuning for shape 16x262144*(8x2^2)_NN_d
Tuning for shape 16x1048576*(8x2^3)_NN_d
Tuning for shape 16x4194304*(8x2^4)_NN_d
Tuning for shape 16x16777216*(8x2^5)_NN_d
Tuning for shape 16x262144*(8x2^1)_NN_d
Tuning for shape 16x1048576*(8x2^2)_NN_d
Tuning for shape 16x4194304*(8x2^3)_NN_d
Tuning for shape 16x16777216*(8x2^4)_NN_d
Tuning for shape 16x1048576*(8x2^1)_NN_d
Tuning for shape 16x4194304*(8x2^2)_NN_d
Tuning for shape 16x16777216*(8x2^3)_NN_d
Tuning for shape 16x4194304*(8x2^1)_NN_d
Tuning for shape 16x16777216*(8x2^2)_NN_d
Tuning for shape 16x16777216*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x2^8  &  1.000 & 1.000 & 64.182 & 0.016
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 9 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 134217728] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x F_7 [8, 2] x F_8 [8, 2] x to produce Y[1, 512]
Matmul: 1 x 512 x 134217728, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2048*(8x2^1)_NN_d
Tuning for shape 1x8192*(8x2^2)_NN_d
Tuning for shape 1x32768*(8x2^3)_NN_d
Tuning for shape 1x131072*(8x2^4)_NN_d
Tuning for shape 1x524288*(8x2^5)_NN_d
Tuning for shape 1x2097152*(8x2^6)_NN_d
Tuning for shape 1x8388608*(8x2^7)_NN_d
Tuning for shape 1x33554432*(8x2^8)_NN_d
Tuning for shape 1x134217728*(8x2^9)_NN_d
Tuning for shape 1x8192*(8x2^1)_NN_d
Tuning for shape 1x32768*(8x2^2)_NN_d
Tuning for shape 1x131072*(8x2^3)_NN_d
Tuning for shape 1x524288*(8x2^4)_NN_d
Tuning for shape 1x2097152*(8x2^5)_NN_d
Tuning for shape 1x8388608*(8x2^6)_NN_d
Tuning for shape 1x33554432*(8x2^7)_NN_d
Tuning for shape 1x134217728*(8x2^8)_NN_d
Tuning for shape 1x32768*(8x2^1)_NN_d
Tuning for shape 1x131072*(8x2^2)_NN_d
Tuning for shape 1x524288*(8x2^3)_NN_d
Tuning for shape 1x2097152*(8x2^4)_NN_d
Tuning for shape 1x8388608*(8x2^5)_NN_d
Tuning for shape 1x33554432*(8x2^6)_NN_d
Tuning for shape 1x134217728*(8x2^7)_NN_d
Tuning for shape 1x131072*(8x2^1)_NN_d
Tuning for shape 1x524288*(8x2^2)_NN_d
Tuning for shape 1x2097152*(8x2^3)_NN_d
Tuning for shape 1x8388608*(8x2^4)_NN_d
Tuning for shape 1x33554432*(8x2^5)_NN_d
Tuning for shape 1x134217728*(8x2^6)_NN_d
Tuning for shape 1x524288*(8x2^1)_NN_d
Tuning for shape 1x2097152*(8x2^2)_NN_d
Tuning for shape 1x8388608*(8x2^3)_NN_d
Tuning for shape 1x33554432*(8x2^4)_NN_d
Tuning for shape 1x134217728*(8x2^5)_NN_d
Tuning for shape 1x2097152*(8x2^1)_NN_d
Tuning for shape 1x8388608*(8x2^2)_NN_d
Tuning for shape 1x33554432*(8x2^3)_NN_d
Tuning for shape 1x134217728*(8x2^4)_NN_d
Tuning for shape 1x8388608*(8x2^1)_NN_d
Tuning for shape 1x33554432*(8x2^2)_NN_d
Tuning for shape 1x134217728*(8x2^3)_NN_d
Tuning for shape 1x33554432*(8x2^1)_NN_d
Tuning for shape 1x134217728*(8x2^2)_NN_d
Tuning for shape 1x134217728*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x2^9  &  1.000 & 1.000 & 84.296 & 0.012
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 9 -p 8 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 134217728] with F_0 [8, 2] x F_1 [8, 2] x F_2 [8, 2] x F_3 [8, 2] x F_4 [8, 2] x F_5 [8, 2] x F_6 [8, 2] x F_7 [8, 2] x F_8 [8, 2] x to produce Y[4, 512]
Matmul: 4 x 512 x 134217728, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2048*(8x2^1)_NN_d
Tuning for shape 4x8192*(8x2^2)_NN_d
Tuning for shape 4x32768*(8x2^3)_NN_d
Tuning for shape 4x131072*(8x2^4)_NN_d
Tuning for shape 4x524288*(8x2^5)_NN_d
Tuning for shape 4x2097152*(8x2^6)_NN_d
Tuning for shape 4x8388608*(8x2^7)_NN_d
Tuning for shape 4x33554432*(8x2^8)_NN_d
Tuning for shape 4x134217728*(8x2^9)_NN_d
Tuning for shape 4x8192*(8x2^1)_NN_d
Tuning for shape 4x32768*(8x2^2)_NN_d
Tuning for shape 4x131072*(8x2^3)_NN_d
Tuning for shape 4x524288*(8x2^4)_NN_d
Tuning for shape 4x2097152*(8x2^5)_NN_d
Tuning for shape 4x8388608*(8x2^6)_NN_d
Tuning for shape 4x33554432*(8x2^7)_NN_d
Tuning for shape 4x134217728*(8x2^8)_NN_d
Tuning for shape 4x32768*(8x2^1)_NN_d
Tuning for shape 4x131072*(8x2^2)_NN_d
Tuning for shape 4x524288*(8x2^3)_NN_d
Tuning for shape 4x2097152*(8x2^4)_NN_d
Tuning for shape 4x8388608*(8x2^5)_NN_d
Tuning for shape 4x33554432*(8x2^6)_NN_d
Tuning for shape 4x134217728*(8x2^7)_NN_d
Tuning for shape 4x131072*(8x2^1)_NN_d
Tuning for shape 4x524288*(8x2^2)_NN_d
Tuning for shape 4x2097152*(8x2^3)_NN_d
Tuning for shape 4x8388608*(8x2^4)_NN_d
Tuning for shape 4x33554432*(8x2^5)_NN_d
Tuning for shape 4x134217728*(8x2^6)_NN_d
Tuning for shape 4x524288*(8x2^1)_NN_d
Tuning for shape 4x2097152*(8x2^2)_NN_d
Tuning for shape 4x8388608*(8x2^3)_NN_d
Tuning for shape 4x33554432*(8x2^4)_NN_d
Tuning for shape 4x134217728*(8x2^5)_NN_d
Tuning for shape 4x2097152*(8x2^1)_NN_d
Tuning for shape 4x8388608*(8x2^2)_NN_d
Tuning for shape 4x33554432*(8x2^3)_NN_d
Tuning for shape 4x134217728*(8x2^4)_NN_d
Tuning for shape 4x8388608*(8x2^1)_NN_d
Tuning for shape 4x33554432*(8x2^2)_NN_d
Tuning for shape 4x134217728*(8x2^3)_NN_d
Tuning for shape 4x33554432*(8x2^1)_NN_d
Tuning for shape 4x134217728*(8x2^2)_NN_d
Tuning for shape 4x134217728*(8x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x2^9  &  1.000 & 1.000 & 48.406 & 0.021
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [8, 4] x to produce Y[1, 4]
Matmul: 1 x 4 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x4^1  &  1.000 & 1.000 & 0.000 & 4305.318
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [8, 4] x to produce Y[4, 4]
Matmul: 4 x 4 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x4^1  &  1.000 & 1.000 & 0.001 & 1183.664
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [8, 4] x to produce Y[16, 4]
Matmul: 16 x 4 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x4^1  &  1.000 & 1.000 & 0.003 & 303.402
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [8, 4] x to produce Y[64, 4]
Matmul: 64 x 4 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x4^1  &  1.000 & 1.000 & 0.013 & 75.874
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [8, 4] x to produce Y[256, 4]
Matmul: 256 x 4 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x4^1  &  1.000 & 1.000 & 0.053 & 18.796
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [8, 4] x to produce Y[1024, 4]
Matmul: 1024 x 4 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x4^1  &  1.000 & 1.000 & 0.212 & 4.718
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [8, 4] x F_1 [8, 4] x to produce Y[1, 16]
Matmul: 1 x 16 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32*(8x4^1)_NN_d
Tuning for shape 1x64*(8x4^2)_NN_d
Tuning for shape 1x64*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x4^2  &  1.000 & 1.000 & 0.002 & 539.065
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [8, 4] x F_1 [8, 4] x to produce Y[4, 16]
Matmul: 4 x 16 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32*(8x4^1)_NN_d
Tuning for shape 4x64*(8x4^2)_NN_d
Tuning for shape 4x64*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x4^2  &  1.000 & 1.000 & 0.007 & 146.745
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [8, 4] x F_1 [8, 4] x to produce Y[16, 16]
Matmul: 16 x 16 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32*(8x4^1)_NN_d
Tuning for shape 16x64*(8x4^2)_NN_d
Tuning for shape 16x64*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x4^2  &  1.000 & 1.000 & 0.027 & 36.580
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [8, 4] x F_1 [8, 4] x to produce Y[64, 16]
Matmul: 64 x 16 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32*(8x4^1)_NN_d
Tuning for shape 64x64*(8x4^2)_NN_d
Tuning for shape 64x64*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x4^2  &  1.000 & 1.000 & 0.111 & 9.032
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [8, 4] x F_1 [8, 4] x to produce Y[256, 16]
Matmul: 256 x 16 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32*(8x4^1)_NN_d
Tuning for shape 256x64*(8x4^2)_NN_d
Tuning for shape 256x64*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x4^2  &  1.000 & 1.000 & 0.433 & 2.309
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [8, 4] x F_1 [8, 4] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32*(8x4^1)_NN_d
Tuning for shape 1024x64*(8x4^2)_NN_d
Tuning for shape 1024x64*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x4^2  &  1.000 & 1.000 & 1.763 & 0.567
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x to produce Y[1, 64]
Matmul: 1 x 64 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(8x4^1)_NN_d
Tuning for shape 1x256*(8x4^2)_NN_d
Tuning for shape 1x512*(8x4^3)_NN_d
Tuning for shape 1x256*(8x4^1)_NN_d
Tuning for shape 1x512*(8x4^2)_NN_d
Tuning for shape 1x512*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x4^3  &  1.000 & 1.000 & 0.014 & 71.269
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x to produce Y[4, 64]
Matmul: 4 x 64 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(8x4^1)_NN_d
Tuning for shape 4x256*(8x4^2)_NN_d
Tuning for shape 4x512*(8x4^3)_NN_d
Tuning for shape 4x256*(8x4^1)_NN_d
Tuning for shape 4x512*(8x4^2)_NN_d
Tuning for shape 4x512*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x4^3  &  1.000 & 1.000 & 0.053 & 18.832
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 512] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x to produce Y[16, 64]
Matmul: 16 x 64 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(8x4^1)_NN_d
Tuning for shape 16x256*(8x4^2)_NN_d
Tuning for shape 16x512*(8x4^3)_NN_d
Tuning for shape 16x256*(8x4^1)_NN_d
Tuning for shape 16x512*(8x4^2)_NN_d
Tuning for shape 16x512*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x4^3  &  1.000 & 1.000 & 0.212 & 4.708
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 512] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x to produce Y[64, 64]
Matmul: 64 x 64 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(8x4^1)_NN_d
Tuning for shape 64x256*(8x4^2)_NN_d
Tuning for shape 64x512*(8x4^3)_NN_d
Tuning for shape 64x256*(8x4^1)_NN_d
Tuning for shape 64x512*(8x4^2)_NN_d
Tuning for shape 64x512*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x4^3  &  1.000 & 1.000 & 0.826 & 1.211
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 512] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x to produce Y[256, 64]
Matmul: 256 x 64 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(8x4^1)_NN_d
Tuning for shape 256x256*(8x4^2)_NN_d
Tuning for shape 256x512*(8x4^3)_NN_d
Tuning for shape 256x256*(8x4^1)_NN_d
Tuning for shape 256x512*(8x4^2)_NN_d
Tuning for shape 256x512*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x4^3  &  1.000 & 1.000 & 3.409 & 0.293
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 512] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(8x4^1)_NN_d
Tuning for shape 1024x256*(8x4^2)_NN_d
Tuning for shape 1024x512*(8x4^3)_NN_d
Tuning for shape 1024x256*(8x4^1)_NN_d
Tuning for shape 1024x512*(8x4^2)_NN_d
Tuning for shape 1024x512*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x4^3  &  1.000 & 1.000 & 13.630 & 0.073
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x to produce Y[1, 256]
Matmul: 1 x 256 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(8x4^1)_NN_d
Tuning for shape 1x1024*(8x4^2)_NN_d
Tuning for shape 1x2048*(8x4^3)_NN_d
Tuning for shape 1x4096*(8x4^4)_NN_d
Tuning for shape 1x1024*(8x4^1)_NN_d
Tuning for shape 1x2048*(8x4^2)_NN_d
Tuning for shape 1x4096*(8x4^3)_NN_d
Tuning for shape 1x2048*(8x4^1)_NN_d
Tuning for shape 1x4096*(8x4^2)_NN_d
Tuning for shape 1x4096*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x4^4  &  1.000 & 1.000 & 0.102 & 9.768
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x to produce Y[4, 256]
Matmul: 4 x 256 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(8x4^1)_NN_d
Tuning for shape 4x1024*(8x4^2)_NN_d
Tuning for shape 4x2048*(8x4^3)_NN_d
Tuning for shape 4x4096*(8x4^4)_NN_d
Tuning for shape 4x1024*(8x4^1)_NN_d
Tuning for shape 4x2048*(8x4^2)_NN_d
Tuning for shape 4x4096*(8x4^3)_NN_d
Tuning for shape 4x2048*(8x4^1)_NN_d
Tuning for shape 4x4096*(8x4^2)_NN_d
Tuning for shape 4x4096*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x4^4  &  1.000 & 1.000 & 0.382 & 2.620
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x to produce Y[16, 256]
Matmul: 16 x 256 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(8x4^1)_NN_d
Tuning for shape 16x1024*(8x4^2)_NN_d
Tuning for shape 16x2048*(8x4^3)_NN_d
Tuning for shape 16x4096*(8x4^4)_NN_d
Tuning for shape 16x1024*(8x4^1)_NN_d
Tuning for shape 16x2048*(8x4^2)_NN_d
Tuning for shape 16x4096*(8x4^3)_NN_d
Tuning for shape 16x2048*(8x4^1)_NN_d
Tuning for shape 16x4096*(8x4^2)_NN_d
Tuning for shape 16x4096*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x4^4  &  1.000 & 1.000 & 1.475 & 0.678
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x to produce Y[64, 256]
Matmul: 64 x 256 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x512*(8x4^1)_NN_d
Tuning for shape 64x1024*(8x4^2)_NN_d
Tuning for shape 64x2048*(8x4^3)_NN_d
Tuning for shape 64x4096*(8x4^4)_NN_d
Tuning for shape 64x1024*(8x4^1)_NN_d
Tuning for shape 64x2048*(8x4^2)_NN_d
Tuning for shape 64x4096*(8x4^3)_NN_d
Tuning for shape 64x2048*(8x4^1)_NN_d
Tuning for shape 64x4096*(8x4^2)_NN_d
Tuning for shape 64x4096*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x4^4  &  1.000 & 1.000 & 6.091 & 0.164
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x to produce Y[256, 256]
Matmul: 256 x 256 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x512*(8x4^1)_NN_d
Tuning for shape 256x1024*(8x4^2)_NN_d
Tuning for shape 256x2048*(8x4^3)_NN_d
Tuning for shape 256x4096*(8x4^4)_NN_d
Tuning for shape 256x1024*(8x4^1)_NN_d
Tuning for shape 256x2048*(8x4^2)_NN_d
Tuning for shape 256x4096*(8x4^3)_NN_d
Tuning for shape 256x2048*(8x4^1)_NN_d
Tuning for shape 256x4096*(8x4^2)_NN_d
Tuning for shape 256x4096*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x4^4  &  1.000 & 1.000 & 23.150 & 0.043
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x to produce Y[1024, 256]
Matmul: 1024 x 256 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x512*(8x4^1)_NN_d
Tuning for shape 1024x1024*(8x4^2)_NN_d
Tuning for shape 1024x2048*(8x4^3)_NN_d
Tuning for shape 1024x4096*(8x4^4)_NN_d
Tuning for shape 1024x1024*(8x4^1)_NN_d
Tuning for shape 1024x2048*(8x4^2)_NN_d
Tuning for shape 1024x4096*(8x4^3)_NN_d
Tuning for shape 1024x2048*(8x4^1)_NN_d
Tuning for shape 1024x4096*(8x4^2)_NN_d
Tuning for shape 1024x4096*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x4^4  &  1.000 & 1.000 & 94.014 & 0.011
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32768] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x to produce Y[1, 1024]
Matmul: 1 x 1024 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2048*(8x4^1)_NN_d
Tuning for shape 1x4096*(8x4^2)_NN_d
Tuning for shape 1x8192*(8x4^3)_NN_d
Tuning for shape 1x16384*(8x4^4)_NN_d
Tuning for shape 1x32768*(8x4^5)_NN_d
Tuning for shape 1x4096*(8x4^1)_NN_d
Tuning for shape 1x8192*(8x4^2)_NN_d
Tuning for shape 1x16384*(8x4^3)_NN_d
Tuning for shape 1x32768*(8x4^4)_NN_d
Tuning for shape 1x8192*(8x4^1)_NN_d
Tuning for shape 1x16384*(8x4^2)_NN_d
Tuning for shape 1x32768*(8x4^3)_NN_d
Tuning for shape 1x16384*(8x4^1)_NN_d
Tuning for shape 1x32768*(8x4^2)_NN_d
Tuning for shape 1x32768*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x4^5  &  1.000 & 1.000 & 0.702 & 1.425
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32768] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x to produce Y[4, 1024]
Matmul: 4 x 1024 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2048*(8x4^1)_NN_d
Tuning for shape 4x4096*(8x4^2)_NN_d
Tuning for shape 4x8192*(8x4^3)_NN_d
Tuning for shape 4x16384*(8x4^4)_NN_d
Tuning for shape 4x32768*(8x4^5)_NN_d
Tuning for shape 4x4096*(8x4^1)_NN_d
Tuning for shape 4x8192*(8x4^2)_NN_d
Tuning for shape 4x16384*(8x4^3)_NN_d
Tuning for shape 4x32768*(8x4^4)_NN_d
Tuning for shape 4x8192*(8x4^1)_NN_d
Tuning for shape 4x16384*(8x4^2)_NN_d
Tuning for shape 4x32768*(8x4^3)_NN_d
Tuning for shape 4x16384*(8x4^1)_NN_d
Tuning for shape 4x32768*(8x4^2)_NN_d
Tuning for shape 4x32768*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x4^5  &  1.000 & 1.000 & 2.661 & 0.376
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32768] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x to produce Y[16, 1024]
Matmul: 16 x 1024 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2048*(8x4^1)_NN_d
Tuning for shape 16x4096*(8x4^2)_NN_d
Tuning for shape 16x8192*(8x4^3)_NN_d
Tuning for shape 16x16384*(8x4^4)_NN_d
Tuning for shape 16x32768*(8x4^5)_NN_d
Tuning for shape 16x4096*(8x4^1)_NN_d
Tuning for shape 16x8192*(8x4^2)_NN_d
Tuning for shape 16x16384*(8x4^3)_NN_d
Tuning for shape 16x32768*(8x4^4)_NN_d
Tuning for shape 16x8192*(8x4^1)_NN_d
Tuning for shape 16x16384*(8x4^2)_NN_d
Tuning for shape 16x32768*(8x4^3)_NN_d
Tuning for shape 16x16384*(8x4^1)_NN_d
Tuning for shape 16x32768*(8x4^2)_NN_d
Tuning for shape 16x32768*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x4^5  &  1.000 & 1.000 & 10.763 & 0.093
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 32768] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x to produce Y[64, 1024]
Matmul: 64 x 1024 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2048*(8x4^1)_NN_d
Tuning for shape 64x4096*(8x4^2)_NN_d
Tuning for shape 64x8192*(8x4^3)_NN_d
Tuning for shape 64x16384*(8x4^4)_NN_d
Tuning for shape 64x32768*(8x4^5)_NN_d
Tuning for shape 64x4096*(8x4^1)_NN_d
Tuning for shape 64x8192*(8x4^2)_NN_d
Tuning for shape 64x16384*(8x4^3)_NN_d
Tuning for shape 64x32768*(8x4^4)_NN_d
Tuning for shape 64x8192*(8x4^1)_NN_d
Tuning for shape 64x16384*(8x4^2)_NN_d
Tuning for shape 64x32768*(8x4^3)_NN_d
Tuning for shape 64x16384*(8x4^1)_NN_d
Tuning for shape 64x32768*(8x4^2)_NN_d
Tuning for shape 64x32768*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x4^5  &  1.000 & 1.000 & 43.124 & 0.023
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 32768] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x to produce Y[256, 1024]
Matmul: 256 x 1024 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2048*(8x4^1)_NN_d
Tuning for shape 256x4096*(8x4^2)_NN_d
Tuning for shape 256x8192*(8x4^3)_NN_d
Tuning for shape 256x16384*(8x4^4)_NN_d
Tuning for shape 256x32768*(8x4^5)_NN_d
Tuning for shape 256x4096*(8x4^1)_NN_d
Tuning for shape 256x8192*(8x4^2)_NN_d
Tuning for shape 256x16384*(8x4^3)_NN_d
Tuning for shape 256x32768*(8x4^4)_NN_d
Tuning for shape 256x8192*(8x4^1)_NN_d
Tuning for shape 256x16384*(8x4^2)_NN_d
Tuning for shape 256x32768*(8x4^3)_NN_d
Tuning for shape 256x16384*(8x4^1)_NN_d
Tuning for shape 256x32768*(8x4^2)_NN_d
Tuning for shape 256x32768*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x4^5  &  1.000 & 1.000 & 97.227 & 0.010
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 5 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 32768] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x to produce Y[1024, 1024]
Matmul: 1024 x 1024 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2048*(8x4^1)_NN_d
Tuning for shape 1024x4096*(8x4^2)_NN_d
Tuning for shape 1024x8192*(8x4^3)_NN_d
Tuning for shape 1024x16384*(8x4^4)_NN_d
Tuning for shape 1024x32768*(8x4^5)_NN_d
Tuning for shape 1024x4096*(8x4^1)_NN_d
Tuning for shape 1024x8192*(8x4^2)_NN_d
Tuning for shape 1024x16384*(8x4^3)_NN_d
Tuning for shape 1024x32768*(8x4^4)_NN_d
Tuning for shape 1024x8192*(8x4^1)_NN_d
Tuning for shape 1024x16384*(8x4^2)_NN_d
Tuning for shape 1024x32768*(8x4^3)_NN_d
Tuning for shape 1024x16384*(8x4^1)_NN_d
Tuning for shape 1024x32768*(8x4^2)_NN_d
Tuning for shape 1024x32768*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x4^5  &  1.000 & 1.000 & 97.303 & 0.010
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 262144] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8192*(8x4^1)_NN_d
Tuning for shape 1x16384*(8x4^2)_NN_d
Tuning for shape 1x32768*(8x4^3)_NN_d
Tuning for shape 1x65536*(8x4^4)_NN_d
Tuning for shape 1x131072*(8x4^5)_NN_d
Tuning for shape 1x262144*(8x4^6)_NN_d
Tuning for shape 1x16384*(8x4^1)_NN_d
Tuning for shape 1x32768*(8x4^2)_NN_d
Tuning for shape 1x65536*(8x4^3)_NN_d
Tuning for shape 1x131072*(8x4^4)_NN_d
Tuning for shape 1x262144*(8x4^5)_NN_d
Tuning for shape 1x32768*(8x4^1)_NN_d
Tuning for shape 1x65536*(8x4^2)_NN_d
Tuning for shape 1x131072*(8x4^3)_NN_d
Tuning for shape 1x262144*(8x4^4)_NN_d
Tuning for shape 1x65536*(8x4^1)_NN_d
Tuning for shape 1x131072*(8x4^2)_NN_d
Tuning for shape 1x262144*(8x4^3)_NN_d
Tuning for shape 1x131072*(8x4^1)_NN_d
Tuning for shape 1x262144*(8x4^2)_NN_d
Tuning for shape 1x262144*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x4^6  &  1.000 & 1.000 & 5.239 & 0.191
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 262144] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8192*(8x4^1)_NN_d
Tuning for shape 4x16384*(8x4^2)_NN_d
Tuning for shape 4x32768*(8x4^3)_NN_d
Tuning for shape 4x65536*(8x4^4)_NN_d
Tuning for shape 4x131072*(8x4^5)_NN_d
Tuning for shape 4x262144*(8x4^6)_NN_d
Tuning for shape 4x16384*(8x4^1)_NN_d
Tuning for shape 4x32768*(8x4^2)_NN_d
Tuning for shape 4x65536*(8x4^3)_NN_d
Tuning for shape 4x131072*(8x4^4)_NN_d
Tuning for shape 4x262144*(8x4^5)_NN_d
Tuning for shape 4x32768*(8x4^1)_NN_d
Tuning for shape 4x65536*(8x4^2)_NN_d
Tuning for shape 4x131072*(8x4^3)_NN_d
Tuning for shape 4x262144*(8x4^4)_NN_d
Tuning for shape 4x65536*(8x4^1)_NN_d
Tuning for shape 4x131072*(8x4^2)_NN_d
Tuning for shape 4x262144*(8x4^3)_NN_d
Tuning for shape 4x131072*(8x4^1)_NN_d
Tuning for shape 4x262144*(8x4^2)_NN_d
Tuning for shape 4x262144*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x4^6  &  1.000 & 1.000 & 20.038 & 0.050
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 262144] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8192*(8x4^1)_NN_d
Tuning for shape 16x16384*(8x4^2)_NN_d
Tuning for shape 16x32768*(8x4^3)_NN_d
Tuning for shape 16x65536*(8x4^4)_NN_d
Tuning for shape 16x131072*(8x4^5)_NN_d
Tuning for shape 16x262144*(8x4^6)_NN_d
Tuning for shape 16x16384*(8x4^1)_NN_d
Tuning for shape 16x32768*(8x4^2)_NN_d
Tuning for shape 16x65536*(8x4^3)_NN_d
Tuning for shape 16x131072*(8x4^4)_NN_d
Tuning for shape 16x262144*(8x4^5)_NN_d
Tuning for shape 16x32768*(8x4^1)_NN_d
Tuning for shape 16x65536*(8x4^2)_NN_d
Tuning for shape 16x131072*(8x4^3)_NN_d
Tuning for shape 16x262144*(8x4^4)_NN_d
Tuning for shape 16x65536*(8x4^1)_NN_d
Tuning for shape 16x131072*(8x4^2)_NN_d
Tuning for shape 16x262144*(8x4^3)_NN_d
Tuning for shape 16x131072*(8x4^1)_NN_d
Tuning for shape 16x262144*(8x4^2)_NN_d
Tuning for shape 16x262144*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x4^6  &  1.000 & 1.000 & 77.960 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 6 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 262144] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8192*(8x4^1)_NN_d
Tuning for shape 64x16384*(8x4^2)_NN_d
Tuning for shape 64x32768*(8x4^3)_NN_d
Tuning for shape 64x65536*(8x4^4)_NN_d
Tuning for shape 64x131072*(8x4^5)_NN_d
Tuning for shape 64x262144*(8x4^6)_NN_d
Tuning for shape 64x16384*(8x4^1)_NN_d
Tuning for shape 64x32768*(8x4^2)_NN_d
Tuning for shape 64x65536*(8x4^3)_NN_d
Tuning for shape 64x131072*(8x4^4)_NN_d
Tuning for shape 64x262144*(8x4^5)_NN_d
Tuning for shape 64x32768*(8x4^1)_NN_d
Tuning for shape 64x65536*(8x4^2)_NN_d
Tuning for shape 64x131072*(8x4^3)_NN_d
Tuning for shape 64x262144*(8x4^4)_NN_d
Tuning for shape 64x65536*(8x4^1)_NN_d
Tuning for shape 64x131072*(8x4^2)_NN_d
Tuning for shape 64x262144*(8x4^3)_NN_d
Tuning for shape 64x131072*(8x4^1)_NN_d
Tuning for shape 64x262144*(8x4^2)_NN_d
Tuning for shape 64x262144*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x4^6  &  1.000 & 1.000 & 113.065 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 6 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 262144] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8192*(8x4^1)_NN_d
Tuning for shape 256x16384*(8x4^2)_NN_d
Tuning for shape 256x32768*(8x4^3)_NN_d
Tuning for shape 256x65536*(8x4^4)_NN_d
Tuning for shape 256x131072*(8x4^5)_NN_d
Tuning for shape 256x262144*(8x4^6)_NN_d
Tuning for shape 256x16384*(8x4^1)_NN_d
Tuning for shape 256x32768*(8x4^2)_NN_d
Tuning for shape 256x65536*(8x4^3)_NN_d
Tuning for shape 256x131072*(8x4^4)_NN_d
Tuning for shape 256x262144*(8x4^5)_NN_d
Tuning for shape 256x32768*(8x4^1)_NN_d
Tuning for shape 256x65536*(8x4^2)_NN_d
Tuning for shape 256x131072*(8x4^3)_NN_d
Tuning for shape 256x262144*(8x4^4)_NN_d
Tuning for shape 256x65536*(8x4^1)_NN_d
Tuning for shape 256x131072*(8x4^2)_NN_d
Tuning for shape 256x262144*(8x4^3)_NN_d
Tuning for shape 256x131072*(8x4^1)_NN_d
Tuning for shape 256x262144*(8x4^2)_NN_d
Tuning for shape 256x262144*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x4^6  &  1.000 & 1.000 & 73.055 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 6 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 262144] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8192*(8x4^1)_NN_d
Tuning for shape 1024x16384*(8x4^2)_NN_d
Tuning for shape 1024x32768*(8x4^3)_NN_d
Tuning for shape 1024x65536*(8x4^4)_NN_d
Tuning for shape 1024x131072*(8x4^5)_NN_d
Tuning for shape 1024x262144*(8x4^6)_NN_d
Tuning for shape 1024x16384*(8x4^1)_NN_d
Tuning for shape 1024x32768*(8x4^2)_NN_d
Tuning for shape 1024x65536*(8x4^3)_NN_d
Tuning for shape 1024x131072*(8x4^4)_NN_d
Tuning for shape 1024x262144*(8x4^5)_NN_d
Tuning for shape 1024x32768*(8x4^1)_NN_d
Tuning for shape 1024x65536*(8x4^2)_NN_d
Tuning for shape 1024x131072*(8x4^3)_NN_d
Tuning for shape 1024x262144*(8x4^4)_NN_d
Tuning for shape 1024x65536*(8x4^1)_NN_d
Tuning for shape 1024x131072*(8x4^2)_NN_d
Tuning for shape 1024x262144*(8x4^3)_NN_d
Tuning for shape 1024x131072*(8x4^1)_NN_d
Tuning for shape 1024x262144*(8x4^2)_NN_d
Tuning for shape 1024x262144*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x4^6  &  1.000 & 1.000 & 71.312 & 0.014
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2097152] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x to produce Y[1, 16384]
Matmul: 1 x 16384 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32768*(8x4^1)_NN_d
Tuning for shape 1x65536*(8x4^2)_NN_d
Tuning for shape 1x131072*(8x4^3)_NN_d
Tuning for shape 1x262144*(8x4^4)_NN_d
Tuning for shape 1x524288*(8x4^5)_NN_d
Tuning for shape 1x1048576*(8x4^6)_NN_d
Tuning for shape 1x2097152*(8x4^7)_NN_d
Tuning for shape 1x65536*(8x4^1)_NN_d
Tuning for shape 1x131072*(8x4^2)_NN_d
Tuning for shape 1x262144*(8x4^3)_NN_d
Tuning for shape 1x524288*(8x4^4)_NN_d
Tuning for shape 1x1048576*(8x4^5)_NN_d
Tuning for shape 1x2097152*(8x4^6)_NN_d
Tuning for shape 1x131072*(8x4^1)_NN_d
Tuning for shape 1x262144*(8x4^2)_NN_d
Tuning for shape 1x524288*(8x4^3)_NN_d
Tuning for shape 1x1048576*(8x4^4)_NN_d
Tuning for shape 1x2097152*(8x4^5)_NN_d
Tuning for shape 1x262144*(8x4^1)_NN_d
Tuning for shape 1x524288*(8x4^2)_NN_d
Tuning for shape 1x1048576*(8x4^3)_NN_d
Tuning for shape 1x2097152*(8x4^4)_NN_d
Tuning for shape 1x524288*(8x4^1)_NN_d
Tuning for shape 1x1048576*(8x4^2)_NN_d
Tuning for shape 1x2097152*(8x4^3)_NN_d
Tuning for shape 1x1048576*(8x4^1)_NN_d
Tuning for shape 1x2097152*(8x4^2)_NN_d
Tuning for shape 1x2097152*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x4^7  &  1.000 & 1.000 & 37.327 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 7 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2097152] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x to produce Y[4, 16384]
Matmul: 4 x 16384 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32768*(8x4^1)_NN_d
Tuning for shape 4x65536*(8x4^2)_NN_d
Tuning for shape 4x131072*(8x4^3)_NN_d
Tuning for shape 4x262144*(8x4^4)_NN_d
Tuning for shape 4x524288*(8x4^5)_NN_d
Tuning for shape 4x1048576*(8x4^6)_NN_d
Tuning for shape 4x2097152*(8x4^7)_NN_d
Tuning for shape 4x65536*(8x4^1)_NN_d
Tuning for shape 4x131072*(8x4^2)_NN_d
Tuning for shape 4x262144*(8x4^3)_NN_d
Tuning for shape 4x524288*(8x4^4)_NN_d
Tuning for shape 4x1048576*(8x4^5)_NN_d
Tuning for shape 4x2097152*(8x4^6)_NN_d
Tuning for shape 4x131072*(8x4^1)_NN_d
Tuning for shape 4x262144*(8x4^2)_NN_d
Tuning for shape 4x524288*(8x4^3)_NN_d
Tuning for shape 4x1048576*(8x4^4)_NN_d
Tuning for shape 4x2097152*(8x4^5)_NN_d
Tuning for shape 4x262144*(8x4^1)_NN_d
Tuning for shape 4x524288*(8x4^2)_NN_d
Tuning for shape 4x1048576*(8x4^3)_NN_d
Tuning for shape 4x2097152*(8x4^4)_NN_d
Tuning for shape 4x524288*(8x4^1)_NN_d
Tuning for shape 4x1048576*(8x4^2)_NN_d
Tuning for shape 4x2097152*(8x4^3)_NN_d
Tuning for shape 4x1048576*(8x4^1)_NN_d
Tuning for shape 4x2097152*(8x4^2)_NN_d
Tuning for shape 4x2097152*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x4^7  &  1.000 & 1.000 & 119.765 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 7 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2097152] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x to produce Y[16, 16384]
Matmul: 16 x 16384 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32768*(8x4^1)_NN_d
Tuning for shape 16x65536*(8x4^2)_NN_d
Tuning for shape 16x131072*(8x4^3)_NN_d
Tuning for shape 16x262144*(8x4^4)_NN_d
Tuning for shape 16x524288*(8x4^5)_NN_d
Tuning for shape 16x1048576*(8x4^6)_NN_d
Tuning for shape 16x2097152*(8x4^7)_NN_d
Tuning for shape 16x65536*(8x4^1)_NN_d
Tuning for shape 16x131072*(8x4^2)_NN_d
Tuning for shape 16x262144*(8x4^3)_NN_d
Tuning for shape 16x524288*(8x4^4)_NN_d
Tuning for shape 16x1048576*(8x4^5)_NN_d
Tuning for shape 16x2097152*(8x4^6)_NN_d
Tuning for shape 16x131072*(8x4^1)_NN_d
Tuning for shape 16x262144*(8x4^2)_NN_d
Tuning for shape 16x524288*(8x4^3)_NN_d
Tuning for shape 16x1048576*(8x4^4)_NN_d
Tuning for shape 16x2097152*(8x4^5)_NN_d
Tuning for shape 16x262144*(8x4^1)_NN_d
Tuning for shape 16x524288*(8x4^2)_NN_d
Tuning for shape 16x1048576*(8x4^3)_NN_d
Tuning for shape 16x2097152*(8x4^4)_NN_d
Tuning for shape 16x524288*(8x4^1)_NN_d
Tuning for shape 16x1048576*(8x4^2)_NN_d
Tuning for shape 16x2097152*(8x4^3)_NN_d
Tuning for shape 16x1048576*(8x4^1)_NN_d
Tuning for shape 16x2097152*(8x4^2)_NN_d
Tuning for shape 16x2097152*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x4^7  &  1.000 & 1.000 & 126.870 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 7 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2097152] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x to produce Y[64, 16384]
Matmul: 64 x 16384 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32768*(8x4^1)_NN_d
Tuning for shape 64x65536*(8x4^2)_NN_d
Tuning for shape 64x131072*(8x4^3)_NN_d
Tuning for shape 64x262144*(8x4^4)_NN_d
Tuning for shape 64x524288*(8x4^5)_NN_d
Tuning for shape 64x1048576*(8x4^6)_NN_d
Tuning for shape 64x2097152*(8x4^7)_NN_d
Tuning for shape 64x65536*(8x4^1)_NN_d
Tuning for shape 64x131072*(8x4^2)_NN_d
Tuning for shape 64x262144*(8x4^3)_NN_d
Tuning for shape 64x524288*(8x4^4)_NN_d
Tuning for shape 64x1048576*(8x4^5)_NN_d
Tuning for shape 64x2097152*(8x4^6)_NN_d
Tuning for shape 64x131072*(8x4^1)_NN_d
Tuning for shape 64x262144*(8x4^2)_NN_d
Tuning for shape 64x524288*(8x4^3)_NN_d
Tuning for shape 64x1048576*(8x4^4)_NN_d
Tuning for shape 64x2097152*(8x4^5)_NN_d
Tuning for shape 64x262144*(8x4^1)_NN_d
Tuning for shape 64x524288*(8x4^2)_NN_d
Tuning for shape 64x1048576*(8x4^3)_NN_d
Tuning for shape 64x2097152*(8x4^4)_NN_d
Tuning for shape 64x524288*(8x4^1)_NN_d
Tuning for shape 64x1048576*(8x4^2)_NN_d
Tuning for shape 64x2097152*(8x4^3)_NN_d
Tuning for shape 64x1048576*(8x4^1)_NN_d
Tuning for shape 64x2097152*(8x4^2)_NN_d
Tuning for shape 64x2097152*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x4^7  &  1.000 & 1.000 & 115.648 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 7 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2097152] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x to produce Y[256, 16384]
Matmul: 256 x 16384 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32768*(8x4^1)_NN_d
Tuning for shape 256x65536*(8x4^2)_NN_d
Tuning for shape 256x131072*(8x4^3)_NN_d
Tuning for shape 256x262144*(8x4^4)_NN_d
Tuning for shape 256x524288*(8x4^5)_NN_d
Tuning for shape 256x1048576*(8x4^6)_NN_d
Tuning for shape 256x2097152*(8x4^7)_NN_d
Tuning for shape 256x65536*(8x4^1)_NN_d
Tuning for shape 256x131072*(8x4^2)_NN_d
Tuning for shape 256x262144*(8x4^3)_NN_d
Tuning for shape 256x524288*(8x4^4)_NN_d
Tuning for shape 256x1048576*(8x4^5)_NN_d
Tuning for shape 256x2097152*(8x4^6)_NN_d
Tuning for shape 256x131072*(8x4^1)_NN_d
Tuning for shape 256x262144*(8x4^2)_NN_d
Tuning for shape 256x524288*(8x4^3)_NN_d
Tuning for shape 256x1048576*(8x4^4)_NN_d
Tuning for shape 256x2097152*(8x4^5)_NN_d
Tuning for shape 256x262144*(8x4^1)_NN_d
Tuning for shape 256x524288*(8x4^2)_NN_d
Tuning for shape 256x1048576*(8x4^3)_NN_d
Tuning for shape 256x2097152*(8x4^4)_NN_d
Tuning for shape 256x524288*(8x4^1)_NN_d
Tuning for shape 256x1048576*(8x4^2)_NN_d
Tuning for shape 256x2097152*(8x4^3)_NN_d
Tuning for shape 256x1048576*(8x4^1)_NN_d
Tuning for shape 256x2097152*(8x4^2)_NN_d
Tuning for shape 256x2097152*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x4^7  &  1.000 & 1.000 & 79.971 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 8 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16777216] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x F_7 [8, 4] x to produce Y[1, 65536]
Matmul: 1 x 65536 x 16777216, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x131072*(8x4^1)_NN_d
Tuning for shape 1x262144*(8x4^2)_NN_d
Tuning for shape 1x524288*(8x4^3)_NN_d
Tuning for shape 1x1048576*(8x4^4)_NN_d
Tuning for shape 1x2097152*(8x4^5)_NN_d
Tuning for shape 1x4194304*(8x4^6)_NN_d
Tuning for shape 1x8388608*(8x4^7)_NN_d
Tuning for shape 1x16777216*(8x4^8)_NN_d
Tuning for shape 1x262144*(8x4^1)_NN_d
Tuning for shape 1x524288*(8x4^2)_NN_d
Tuning for shape 1x1048576*(8x4^3)_NN_d
Tuning for shape 1x2097152*(8x4^4)_NN_d
Tuning for shape 1x4194304*(8x4^5)_NN_d
Tuning for shape 1x8388608*(8x4^6)_NN_d
Tuning for shape 1x16777216*(8x4^7)_NN_d
Tuning for shape 1x524288*(8x4^1)_NN_d
Tuning for shape 1x1048576*(8x4^2)_NN_d
Tuning for shape 1x2097152*(8x4^3)_NN_d
Tuning for shape 1x4194304*(8x4^4)_NN_d
Tuning for shape 1x8388608*(8x4^5)_NN_d
Tuning for shape 1x16777216*(8x4^6)_NN_d
Tuning for shape 1x1048576*(8x4^1)_NN_d
Tuning for shape 1x2097152*(8x4^2)_NN_d
Tuning for shape 1x4194304*(8x4^3)_NN_d
Tuning for shape 1x8388608*(8x4^4)_NN_d
Tuning for shape 1x16777216*(8x4^5)_NN_d
Tuning for shape 1x2097152*(8x4^1)_NN_d
Tuning for shape 1x4194304*(8x4^2)_NN_d
Tuning for shape 1x8388608*(8x4^3)_NN_d
Tuning for shape 1x16777216*(8x4^4)_NN_d
Tuning for shape 1x4194304*(8x4^1)_NN_d
Tuning for shape 1x8388608*(8x4^2)_NN_d
Tuning for shape 1x16777216*(8x4^3)_NN_d
Tuning for shape 1x8388608*(8x4^1)_NN_d
Tuning for shape 1x16777216*(8x4^2)_NN_d
Tuning for shape 1x16777216*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x4^8  &  1.000 & 1.000 & 146.019 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 8 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16777216] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x F_7 [8, 4] x to produce Y[4, 65536]
Matmul: 4 x 65536 x 16777216, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x131072*(8x4^1)_NN_d
Tuning for shape 4x262144*(8x4^2)_NN_d
Tuning for shape 4x524288*(8x4^3)_NN_d
Tuning for shape 4x1048576*(8x4^4)_NN_d
Tuning for shape 4x2097152*(8x4^5)_NN_d
Tuning for shape 4x4194304*(8x4^6)_NN_d
Tuning for shape 4x8388608*(8x4^7)_NN_d
Tuning for shape 4x16777216*(8x4^8)_NN_d
Tuning for shape 4x262144*(8x4^1)_NN_d
Tuning for shape 4x524288*(8x4^2)_NN_d
Tuning for shape 4x1048576*(8x4^3)_NN_d
Tuning for shape 4x2097152*(8x4^4)_NN_d
Tuning for shape 4x4194304*(8x4^5)_NN_d
Tuning for shape 4x8388608*(8x4^6)_NN_d
Tuning for shape 4x16777216*(8x4^7)_NN_d
Tuning for shape 4x524288*(8x4^1)_NN_d
Tuning for shape 4x1048576*(8x4^2)_NN_d
Tuning for shape 4x2097152*(8x4^3)_NN_d
Tuning for shape 4x4194304*(8x4^4)_NN_d
Tuning for shape 4x8388608*(8x4^5)_NN_d
Tuning for shape 4x16777216*(8x4^6)_NN_d
Tuning for shape 4x1048576*(8x4^1)_NN_d
Tuning for shape 4x2097152*(8x4^2)_NN_d
Tuning for shape 4x4194304*(8x4^3)_NN_d
Tuning for shape 4x8388608*(8x4^4)_NN_d
Tuning for shape 4x16777216*(8x4^5)_NN_d
Tuning for shape 4x2097152*(8x4^1)_NN_d
Tuning for shape 4x4194304*(8x4^2)_NN_d
Tuning for shape 4x8388608*(8x4^3)_NN_d
Tuning for shape 4x16777216*(8x4^4)_NN_d
Tuning for shape 4x4194304*(8x4^1)_NN_d
Tuning for shape 4x8388608*(8x4^2)_NN_d
Tuning for shape 4x16777216*(8x4^3)_NN_d
Tuning for shape 4x8388608*(8x4^1)_NN_d
Tuning for shape 4x16777216*(8x4^2)_NN_d
Tuning for shape 4x16777216*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x4^8  &  1.000 & 1.000 & 127.971 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 8 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16777216] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x F_7 [8, 4] x to produce Y[16, 65536]
Matmul: 16 x 65536 x 16777216, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x131072*(8x4^1)_NN_d
Tuning for shape 16x262144*(8x4^2)_NN_d
Tuning for shape 16x524288*(8x4^3)_NN_d
Tuning for shape 16x1048576*(8x4^4)_NN_d
Tuning for shape 16x2097152*(8x4^5)_NN_d
Tuning for shape 16x4194304*(8x4^6)_NN_d
Tuning for shape 16x8388608*(8x4^7)_NN_d
Tuning for shape 16x16777216*(8x4^8)_NN_d
Tuning for shape 16x262144*(8x4^1)_NN_d
Tuning for shape 16x524288*(8x4^2)_NN_d
Tuning for shape 16x1048576*(8x4^3)_NN_d
Tuning for shape 16x2097152*(8x4^4)_NN_d
Tuning for shape 16x4194304*(8x4^5)_NN_d
Tuning for shape 16x8388608*(8x4^6)_NN_d
Tuning for shape 16x16777216*(8x4^7)_NN_d
Tuning for shape 16x524288*(8x4^1)_NN_d
Tuning for shape 16x1048576*(8x4^2)_NN_d
Tuning for shape 16x2097152*(8x4^3)_NN_d
Tuning for shape 16x4194304*(8x4^4)_NN_d
Tuning for shape 16x8388608*(8x4^5)_NN_d
Tuning for shape 16x16777216*(8x4^6)_NN_d
Tuning for shape 16x1048576*(8x4^1)_NN_d
Tuning for shape 16x2097152*(8x4^2)_NN_d
Tuning for shape 16x4194304*(8x4^3)_NN_d
Tuning for shape 16x8388608*(8x4^4)_NN_d
Tuning for shape 16x16777216*(8x4^5)_NN_d
Tuning for shape 16x2097152*(8x4^1)_NN_d
Tuning for shape 16x4194304*(8x4^2)_NN_d
Tuning for shape 16x8388608*(8x4^3)_NN_d
Tuning for shape 16x16777216*(8x4^4)_NN_d
Tuning for shape 16x4194304*(8x4^1)_NN_d
Tuning for shape 16x8388608*(8x4^2)_NN_d
Tuning for shape 16x16777216*(8x4^3)_NN_d
Tuning for shape 16x8388608*(8x4^1)_NN_d
Tuning for shape 16x16777216*(8x4^2)_NN_d
Tuning for shape 16x16777216*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x4^8  &  1.000 & 1.000 & 128.340 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 9 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 134217728] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x F_7 [8, 4] x F_8 [8, 4] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 134217728, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x524288*(8x4^1)_NN_d
Tuning for shape 1x1048576*(8x4^2)_NN_d
Tuning for shape 1x2097152*(8x4^3)_NN_d
Tuning for shape 1x4194304*(8x4^4)_NN_d
Tuning for shape 1x8388608*(8x4^5)_NN_d
Tuning for shape 1x16777216*(8x4^6)_NN_d
Tuning for shape 1x33554432*(8x4^7)_NN_d
Tuning for shape 1x67108864*(8x4^8)_NN_d
Tuning for shape 1x134217728*(8x4^9)_NN_d
Tuning for shape 1x1048576*(8x4^1)_NN_d
Tuning for shape 1x2097152*(8x4^2)_NN_d
Tuning for shape 1x4194304*(8x4^3)_NN_d
Tuning for shape 1x8388608*(8x4^4)_NN_d
Tuning for shape 1x16777216*(8x4^5)_NN_d
Tuning for shape 1x33554432*(8x4^6)_NN_d
Tuning for shape 1x67108864*(8x4^7)_NN_d
Tuning for shape 1x134217728*(8x4^8)_NN_d
Tuning for shape 1x2097152*(8x4^1)_NN_d
Tuning for shape 1x4194304*(8x4^2)_NN_d
Tuning for shape 1x8388608*(8x4^3)_NN_d
Tuning for shape 1x16777216*(8x4^4)_NN_d
Tuning for shape 1x33554432*(8x4^5)_NN_d
Tuning for shape 1x67108864*(8x4^6)_NN_d
Tuning for shape 1x134217728*(8x4^7)_NN_d
Tuning for shape 1x4194304*(8x4^1)_NN_d
Tuning for shape 1x8388608*(8x4^2)_NN_d
Tuning for shape 1x16777216*(8x4^3)_NN_d
Tuning for shape 1x33554432*(8x4^4)_NN_d
Tuning for shape 1x67108864*(8x4^5)_NN_d
Tuning for shape 1x134217728*(8x4^6)_NN_d
Tuning for shape 1x8388608*(8x4^1)_NN_d
Tuning for shape 1x16777216*(8x4^2)_NN_d
Tuning for shape 1x33554432*(8x4^3)_NN_d
Tuning for shape 1x67108864*(8x4^4)_NN_d
Tuning for shape 1x134217728*(8x4^5)_NN_d
Tuning for shape 1x16777216*(8x4^1)_NN_d
Tuning for shape 1x33554432*(8x4^2)_NN_d
Tuning for shape 1x67108864*(8x4^3)_NN_d
Tuning for shape 1x134217728*(8x4^4)_NN_d
Tuning for shape 1x33554432*(8x4^1)_NN_d
Tuning for shape 1x67108864*(8x4^2)_NN_d
Tuning for shape 1x134217728*(8x4^3)_NN_d
Tuning for shape 1x67108864*(8x4^1)_NN_d
Tuning for shape 1x134217728*(8x4^2)_NN_d
Tuning for shape 1x134217728*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x4^9  &  1.000 & 1.000 & 152.625 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 9 -p 8 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 134217728] with F_0 [8, 4] x F_1 [8, 4] x F_2 [8, 4] x F_3 [8, 4] x F_4 [8, 4] x F_5 [8, 4] x F_6 [8, 4] x F_7 [8, 4] x F_8 [8, 4] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 134217728, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x524288*(8x4^1)_NN_d
Tuning for shape 4x1048576*(8x4^2)_NN_d
Tuning for shape 4x2097152*(8x4^3)_NN_d
Tuning for shape 4x4194304*(8x4^4)_NN_d
Tuning for shape 4x8388608*(8x4^5)_NN_d
Tuning for shape 4x16777216*(8x4^6)_NN_d
Tuning for shape 4x33554432*(8x4^7)_NN_d
Tuning for shape 4x67108864*(8x4^8)_NN_d
Tuning for shape 4x134217728*(8x4^9)_NN_d
Tuning for shape 4x1048576*(8x4^1)_NN_d
Tuning for shape 4x2097152*(8x4^2)_NN_d
Tuning for shape 4x4194304*(8x4^3)_NN_d
Tuning for shape 4x8388608*(8x4^4)_NN_d
Tuning for shape 4x16777216*(8x4^5)_NN_d
Tuning for shape 4x33554432*(8x4^6)_NN_d
Tuning for shape 4x67108864*(8x4^7)_NN_d
Tuning for shape 4x134217728*(8x4^8)_NN_d
Tuning for shape 4x2097152*(8x4^1)_NN_d
Tuning for shape 4x4194304*(8x4^2)_NN_d
Tuning for shape 4x8388608*(8x4^3)_NN_d
Tuning for shape 4x16777216*(8x4^4)_NN_d
Tuning for shape 4x33554432*(8x4^5)_NN_d
Tuning for shape 4x67108864*(8x4^6)_NN_d
Tuning for shape 4x134217728*(8x4^7)_NN_d
Tuning for shape 4x4194304*(8x4^1)_NN_d
Tuning for shape 4x8388608*(8x4^2)_NN_d
Tuning for shape 4x16777216*(8x4^3)_NN_d
Tuning for shape 4x33554432*(8x4^4)_NN_d
Tuning for shape 4x67108864*(8x4^5)_NN_d
Tuning for shape 4x134217728*(8x4^6)_NN_d
Tuning for shape 4x8388608*(8x4^1)_NN_d
Tuning for shape 4x16777216*(8x4^2)_NN_d
Tuning for shape 4x33554432*(8x4^3)_NN_d
Tuning for shape 4x67108864*(8x4^4)_NN_d
Tuning for shape 4x134217728*(8x4^5)_NN_d
Tuning for shape 4x16777216*(8x4^1)_NN_d
Tuning for shape 4x33554432*(8x4^2)_NN_d
Tuning for shape 4x67108864*(8x4^3)_NN_d
Tuning for shape 4x134217728*(8x4^4)_NN_d
Tuning for shape 4x33554432*(8x4^1)_NN_d
Tuning for shape 4x67108864*(8x4^2)_NN_d
Tuning for shape 4x134217728*(8x4^3)_NN_d
Tuning for shape 4x67108864*(8x4^1)_NN_d
Tuning for shape 4x134217728*(8x4^2)_NN_d
Tuning for shape 4x134217728*(8x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x4^9  &  1.000 & 1.000 & 105.421 & 0.009
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [8, 8] x to produce Y[1, 8]
Matmul: 1 x 8 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x8^1  &  1.000 & 1.000 & 0.000 & 2070.423
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [8, 8] x to produce Y[4, 8]
Matmul: 4 x 8 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x8^1  &  1.000 & 1.000 & 0.002 & 601.332
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [8, 8] x to produce Y[16, 8]
Matmul: 16 x 8 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x8^1  &  1.000 & 1.000 & 0.006 & 157.481
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [8, 8] x to produce Y[64, 8]
Matmul: 64 x 8 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x8^1  &  1.000 & 1.000 & 0.025 & 39.379
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [8, 8] x to produce Y[256, 8]
Matmul: 256 x 8 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x8^1  &  1.000 & 1.000 & 0.105 & 9.507
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [8, 8] x to produce Y[1024, 8]
Matmul: 1024 x 8 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x8^1  &  1.000 & 1.000 & 0.423 & 2.363
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [8, 8] x F_1 [8, 8] x to produce Y[1, 64]
Matmul: 1 x 64 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x64*(8x8^1)_NN_d
Tuning for shape 1x64*(8x8^2)_NN_d
Tuning for shape 1x64*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x8^2  &  1.000 & 1.000 & 0.005 & 200.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [8, 8] x F_1 [8, 8] x to produce Y[4, 64]
Matmul: 4 x 64 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x64*(8x8^1)_NN_d
Tuning for shape 4x64*(8x8^2)_NN_d
Tuning for shape 4x64*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x8^2  &  1.000 & 1.000 & 0.018 & 54.663
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [8, 8] x F_1 [8, 8] x to produce Y[16, 64]
Matmul: 16 x 64 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x64*(8x8^1)_NN_d
Tuning for shape 16x64*(8x8^2)_NN_d
Tuning for shape 16x64*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x8^2  &  1.000 & 1.000 & 0.074 & 13.530
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [8, 8] x F_1 [8, 8] x to produce Y[64, 64]
Matmul: 64 x 64 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x64*(8x8^1)_NN_d
Tuning for shape 64x64*(8x8^2)_NN_d
Tuning for shape 64x64*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x8^2  &  1.000 & 1.000 & 0.257 & 3.890
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [8, 8] x F_1 [8, 8] x to produce Y[256, 64]
Matmul: 256 x 64 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x64*(8x8^1)_NN_d
Tuning for shape 256x64*(8x8^2)_NN_d
Tuning for shape 256x64*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x8^2  &  1.000 & 1.000 & 1.167 & 0.857
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [8, 8] x F_1 [8, 8] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x64*(8x8^1)_NN_d
Tuning for shape 1024x64*(8x8^2)_NN_d
Tuning for shape 1024x64*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x8^2  &  1.000 & 1.000 & 4.674 & 0.214
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x to produce Y[1, 512]
Matmul: 1 x 512 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(8x8^1)_NN_d
Tuning for shape 1x512*(8x8^2)_NN_d
Tuning for shape 1x512*(8x8^3)_NN_d
Tuning for shape 1x512*(8x8^1)_NN_d
Tuning for shape 1x512*(8x8^2)_NN_d
Tuning for shape 1x512*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x8^3  &  1.000 & 1.000 & 0.047 & 21.178
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x to produce Y[4, 512]
Matmul: 4 x 512 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(8x8^1)_NN_d
Tuning for shape 4x512*(8x8^2)_NN_d
Tuning for shape 4x512*(8x8^3)_NN_d
Tuning for shape 4x512*(8x8^1)_NN_d
Tuning for shape 4x512*(8x8^2)_NN_d
Tuning for shape 4x512*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x8^3  &  1.000 & 1.000 & 0.177 & 5.661
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 512] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x to produce Y[16, 512]
Matmul: 16 x 512 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(8x8^1)_NN_d
Tuning for shape 16x512*(8x8^2)_NN_d
Tuning for shape 16x512*(8x8^3)_NN_d
Tuning for shape 16x512*(8x8^1)_NN_d
Tuning for shape 16x512*(8x8^2)_NN_d
Tuning for shape 16x512*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x8^3  &  1.000 & 1.000 & 0.735 & 1.361
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 512] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x to produce Y[64, 512]
Matmul: 64 x 512 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x512*(8x8^1)_NN_d
Tuning for shape 64x512*(8x8^2)_NN_d
Tuning for shape 64x512*(8x8^3)_NN_d
Tuning for shape 64x512*(8x8^1)_NN_d
Tuning for shape 64x512*(8x8^2)_NN_d
Tuning for shape 64x512*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x8^3  &  1.000 & 1.000 & 2.891 & 0.346
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 512] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x to produce Y[256, 512]
Matmul: 256 x 512 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x512*(8x8^1)_NN_d
Tuning for shape 256x512*(8x8^2)_NN_d
Tuning for shape 256x512*(8x8^3)_NN_d
Tuning for shape 256x512*(8x8^1)_NN_d
Tuning for shape 256x512*(8x8^2)_NN_d
Tuning for shape 256x512*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x8^3  &  1.000 & 1.000 & 11.473 & 0.087
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 512] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x to produce Y[1024, 512]
Matmul: 1024 x 512 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x512*(8x8^1)_NN_d
Tuning for shape 1024x512*(8x8^2)_NN_d
Tuning for shape 1024x512*(8x8^3)_NN_d
Tuning for shape 1024x512*(8x8^1)_NN_d
Tuning for shape 1024x512*(8x8^2)_NN_d
Tuning for shape 1024x512*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x8^3  &  1.000 & 1.000 & 46.162 & 0.022
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4096*(8x8^1)_NN_d
Tuning for shape 1x4096*(8x8^2)_NN_d
Tuning for shape 1x4096*(8x8^3)_NN_d
Tuning for shape 1x4096*(8x8^4)_NN_d
Tuning for shape 1x4096*(8x8^1)_NN_d
Tuning for shape 1x4096*(8x8^2)_NN_d
Tuning for shape 1x4096*(8x8^3)_NN_d
Tuning for shape 1x4096*(8x8^1)_NN_d
Tuning for shape 1x4096*(8x8^2)_NN_d
Tuning for shape 1x4096*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x8^4  &  1.000 & 1.000 & 0.428 & 2.337
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4096*(8x8^1)_NN_d
Tuning for shape 4x4096*(8x8^2)_NN_d
Tuning for shape 4x4096*(8x8^3)_NN_d
Tuning for shape 4x4096*(8x8^4)_NN_d
Tuning for shape 4x4096*(8x8^1)_NN_d
Tuning for shape 4x4096*(8x8^2)_NN_d
Tuning for shape 4x4096*(8x8^3)_NN_d
Tuning for shape 4x4096*(8x8^1)_NN_d
Tuning for shape 4x4096*(8x8^2)_NN_d
Tuning for shape 4x4096*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x8^4  &  1.000 & 1.000 & 1.609 & 0.622
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4096*(8x8^1)_NN_d
Tuning for shape 16x4096*(8x8^2)_NN_d
Tuning for shape 16x4096*(8x8^3)_NN_d
Tuning for shape 16x4096*(8x8^4)_NN_d
Tuning for shape 16x4096*(8x8^1)_NN_d
Tuning for shape 16x4096*(8x8^2)_NN_d
Tuning for shape 16x4096*(8x8^3)_NN_d
Tuning for shape 16x4096*(8x8^1)_NN_d
Tuning for shape 16x4096*(8x8^2)_NN_d
Tuning for shape 16x4096*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x8^4  &  1.000 & 1.000 & 6.477 & 0.154
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4096*(8x8^1)_NN_d
Tuning for shape 64x4096*(8x8^2)_NN_d
Tuning for shape 64x4096*(8x8^3)_NN_d
Tuning for shape 64x4096*(8x8^4)_NN_d
Tuning for shape 64x4096*(8x8^1)_NN_d
Tuning for shape 64x4096*(8x8^2)_NN_d
Tuning for shape 64x4096*(8x8^3)_NN_d
Tuning for shape 64x4096*(8x8^1)_NN_d
Tuning for shape 64x4096*(8x8^2)_NN_d
Tuning for shape 64x4096*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x8^4  &  1.000 & 1.000 & 26.082 & 0.038
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4096*(8x8^1)_NN_d
Tuning for shape 256x4096*(8x8^2)_NN_d
Tuning for shape 256x4096*(8x8^3)_NN_d
Tuning for shape 256x4096*(8x8^4)_NN_d
Tuning for shape 256x4096*(8x8^1)_NN_d
Tuning for shape 256x4096*(8x8^2)_NN_d
Tuning for shape 256x4096*(8x8^3)_NN_d
Tuning for shape 256x4096*(8x8^1)_NN_d
Tuning for shape 256x4096*(8x8^2)_NN_d
Tuning for shape 256x4096*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x8^4  &  1.000 & 1.000 & 102.336 & 0.010
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x4096*(8x8^1)_NN_d
Tuning for shape 1024x4096*(8x8^2)_NN_d
Tuning for shape 1024x4096*(8x8^3)_NN_d
Tuning for shape 1024x4096*(8x8^4)_NN_d
Tuning for shape 1024x4096*(8x8^1)_NN_d
Tuning for shape 1024x4096*(8x8^2)_NN_d
Tuning for shape 1024x4096*(8x8^3)_NN_d
Tuning for shape 1024x4096*(8x8^1)_NN_d
Tuning for shape 1024x4096*(8x8^2)_NN_d
Tuning for shape 1024x4096*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x8^4  &  1.000 & 1.000 & 208.490 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32768] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x to produce Y[1, 32768]
Matmul: 1 x 32768 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32768*(8x8^1)_NN_d
Tuning for shape 1x32768*(8x8^2)_NN_d
Tuning for shape 1x32768*(8x8^3)_NN_d
Tuning for shape 1x32768*(8x8^4)_NN_d
Tuning for shape 1x32768*(8x8^5)_NN_d
Tuning for shape 1x32768*(8x8^1)_NN_d
Tuning for shape 1x32768*(8x8^2)_NN_d
Tuning for shape 1x32768*(8x8^3)_NN_d
Tuning for shape 1x32768*(8x8^4)_NN_d
Tuning for shape 1x32768*(8x8^1)_NN_d
Tuning for shape 1x32768*(8x8^2)_NN_d
Tuning for shape 1x32768*(8x8^3)_NN_d
Tuning for shape 1x32768*(8x8^1)_NN_d
Tuning for shape 1x32768*(8x8^2)_NN_d
Tuning for shape 1x32768*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x8^5  &  1.000 & 1.000 & 3.814 & 0.262
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32768] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x to produce Y[4, 32768]
Matmul: 4 x 32768 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32768*(8x8^1)_NN_d
Tuning for shape 4x32768*(8x8^2)_NN_d
Tuning for shape 4x32768*(8x8^3)_NN_d
Tuning for shape 4x32768*(8x8^4)_NN_d
Tuning for shape 4x32768*(8x8^5)_NN_d
Tuning for shape 4x32768*(8x8^1)_NN_d
Tuning for shape 4x32768*(8x8^2)_NN_d
Tuning for shape 4x32768*(8x8^3)_NN_d
Tuning for shape 4x32768*(8x8^4)_NN_d
Tuning for shape 4x32768*(8x8^1)_NN_d
Tuning for shape 4x32768*(8x8^2)_NN_d
Tuning for shape 4x32768*(8x8^3)_NN_d
Tuning for shape 4x32768*(8x8^1)_NN_d
Tuning for shape 4x32768*(8x8^2)_NN_d
Tuning for shape 4x32768*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x8^5  &  1.000 & 1.000 & 14.184 & 0.071
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32768] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x to produce Y[16, 32768]
Matmul: 16 x 32768 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32768*(8x8^1)_NN_d
Tuning for shape 16x32768*(8x8^2)_NN_d
Tuning for shape 16x32768*(8x8^3)_NN_d
Tuning for shape 16x32768*(8x8^4)_NN_d
Tuning for shape 16x32768*(8x8^5)_NN_d
Tuning for shape 16x32768*(8x8^1)_NN_d
Tuning for shape 16x32768*(8x8^2)_NN_d
Tuning for shape 16x32768*(8x8^3)_NN_d
Tuning for shape 16x32768*(8x8^4)_NN_d
Tuning for shape 16x32768*(8x8^1)_NN_d
Tuning for shape 16x32768*(8x8^2)_NN_d
Tuning for shape 16x32768*(8x8^3)_NN_d
Tuning for shape 16x32768*(8x8^1)_NN_d
Tuning for shape 16x32768*(8x8^2)_NN_d
Tuning for shape 16x32768*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x8^5  &  1.000 & 1.000 & 55.812 & 0.018
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 32768] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x to produce Y[64, 32768]
Matmul: 64 x 32768 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32768*(8x8^1)_NN_d
Tuning for shape 64x32768*(8x8^2)_NN_d
Tuning for shape 64x32768*(8x8^3)_NN_d
Tuning for shape 64x32768*(8x8^4)_NN_d
Tuning for shape 64x32768*(8x8^5)_NN_d
Tuning for shape 64x32768*(8x8^1)_NN_d
Tuning for shape 64x32768*(8x8^2)_NN_d
Tuning for shape 64x32768*(8x8^3)_NN_d
Tuning for shape 64x32768*(8x8^4)_NN_d
Tuning for shape 64x32768*(8x8^1)_NN_d
Tuning for shape 64x32768*(8x8^2)_NN_d
Tuning for shape 64x32768*(8x8^3)_NN_d
Tuning for shape 64x32768*(8x8^1)_NN_d
Tuning for shape 64x32768*(8x8^2)_NN_d
Tuning for shape 64x32768*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x8^5  &  1.000 & 1.000 & 203.675 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 32768] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x to produce Y[256, 32768]
Matmul: 256 x 32768 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32768*(8x8^1)_NN_d
Tuning for shape 256x32768*(8x8^2)_NN_d
Tuning for shape 256x32768*(8x8^3)_NN_d
Tuning for shape 256x32768*(8x8^4)_NN_d
Tuning for shape 256x32768*(8x8^5)_NN_d
Tuning for shape 256x32768*(8x8^1)_NN_d
Tuning for shape 256x32768*(8x8^2)_NN_d
Tuning for shape 256x32768*(8x8^3)_NN_d
Tuning for shape 256x32768*(8x8^4)_NN_d
Tuning for shape 256x32768*(8x8^1)_NN_d
Tuning for shape 256x32768*(8x8^2)_NN_d
Tuning for shape 256x32768*(8x8^3)_NN_d
Tuning for shape 256x32768*(8x8^1)_NN_d
Tuning for shape 256x32768*(8x8^2)_NN_d
Tuning for shape 256x32768*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x8^5  &  1.000 & 1.000 & 199.728 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 5 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 32768] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x to produce Y[1024, 32768]
Matmul: 1024 x 32768 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32768*(8x8^1)_NN_d
Tuning for shape 1024x32768*(8x8^2)_NN_d
Tuning for shape 1024x32768*(8x8^3)_NN_d
Tuning for shape 1024x32768*(8x8^4)_NN_d
Tuning for shape 1024x32768*(8x8^5)_NN_d
Tuning for shape 1024x32768*(8x8^1)_NN_d
Tuning for shape 1024x32768*(8x8^2)_NN_d
Tuning for shape 1024x32768*(8x8^3)_NN_d
Tuning for shape 1024x32768*(8x8^4)_NN_d
Tuning for shape 1024x32768*(8x8^1)_NN_d
Tuning for shape 1024x32768*(8x8^2)_NN_d
Tuning for shape 1024x32768*(8x8^3)_NN_d
Tuning for shape 1024x32768*(8x8^1)_NN_d
Tuning for shape 1024x32768*(8x8^2)_NN_d
Tuning for shape 1024x32768*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x8^5  &  1.000 & 1.000 & 216.199 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 262144] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x262144*(8x8^1)_NN_d
Tuning for shape 1x262144*(8x8^2)_NN_d
Tuning for shape 1x262144*(8x8^3)_NN_d
Tuning for shape 1x262144*(8x8^4)_NN_d
Tuning for shape 1x262144*(8x8^5)_NN_d
Tuning for shape 1x262144*(8x8^6)_NN_d
Tuning for shape 1x262144*(8x8^1)_NN_d
Tuning for shape 1x262144*(8x8^2)_NN_d
Tuning for shape 1x262144*(8x8^3)_NN_d
Tuning for shape 1x262144*(8x8^4)_NN_d
Tuning for shape 1x262144*(8x8^5)_NN_d
Tuning for shape 1x262144*(8x8^1)_NN_d
Tuning for shape 1x262144*(8x8^2)_NN_d
Tuning for shape 1x262144*(8x8^3)_NN_d
Tuning for shape 1x262144*(8x8^4)_NN_d
Tuning for shape 1x262144*(8x8^1)_NN_d
Tuning for shape 1x262144*(8x8^2)_NN_d
Tuning for shape 1x262144*(8x8^3)_NN_d
Tuning for shape 1x262144*(8x8^1)_NN_d
Tuning for shape 1x262144*(8x8^2)_NN_d
Tuning for shape 1x262144*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x8^6  &  1.000 & 1.000 & 31.575 & 0.032
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 262144] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x262144*(8x8^1)_NN_d
Tuning for shape 4x262144*(8x8^2)_NN_d
Tuning for shape 4x262144*(8x8^3)_NN_d
Tuning for shape 4x262144*(8x8^4)_NN_d
Tuning for shape 4x262144*(8x8^5)_NN_d
Tuning for shape 4x262144*(8x8^6)_NN_d
Tuning for shape 4x262144*(8x8^1)_NN_d
Tuning for shape 4x262144*(8x8^2)_NN_d
Tuning for shape 4x262144*(8x8^3)_NN_d
Tuning for shape 4x262144*(8x8^4)_NN_d
Tuning for shape 4x262144*(8x8^5)_NN_d
Tuning for shape 4x262144*(8x8^1)_NN_d
Tuning for shape 4x262144*(8x8^2)_NN_d
Tuning for shape 4x262144*(8x8^3)_NN_d
Tuning for shape 4x262144*(8x8^4)_NN_d
Tuning for shape 4x262144*(8x8^1)_NN_d
Tuning for shape 4x262144*(8x8^2)_NN_d
Tuning for shape 4x262144*(8x8^3)_NN_d
Tuning for shape 4x262144*(8x8^1)_NN_d
Tuning for shape 4x262144*(8x8^2)_NN_d
Tuning for shape 4x262144*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x8^6  &  1.000 & 1.000 & 118.181 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 262144] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x262144*(8x8^1)_NN_d
Tuning for shape 16x262144*(8x8^2)_NN_d
Tuning for shape 16x262144*(8x8^3)_NN_d
Tuning for shape 16x262144*(8x8^4)_NN_d
Tuning for shape 16x262144*(8x8^5)_NN_d
Tuning for shape 16x262144*(8x8^6)_NN_d
Tuning for shape 16x262144*(8x8^1)_NN_d
Tuning for shape 16x262144*(8x8^2)_NN_d
Tuning for shape 16x262144*(8x8^3)_NN_d
Tuning for shape 16x262144*(8x8^4)_NN_d
Tuning for shape 16x262144*(8x8^5)_NN_d
Tuning for shape 16x262144*(8x8^1)_NN_d
Tuning for shape 16x262144*(8x8^2)_NN_d
Tuning for shape 16x262144*(8x8^3)_NN_d
Tuning for shape 16x262144*(8x8^4)_NN_d
Tuning for shape 16x262144*(8x8^1)_NN_d
Tuning for shape 16x262144*(8x8^2)_NN_d
Tuning for shape 16x262144*(8x8^3)_NN_d
Tuning for shape 16x262144*(8x8^1)_NN_d
Tuning for shape 16x262144*(8x8^2)_NN_d
Tuning for shape 16x262144*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x8^6  &  1.000 & 1.000 & 233.437 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 6 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 262144] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x to produce Y[64, 262144]
Matmul: 64 x 262144 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x262144*(8x8^1)_NN_d
Tuning for shape 64x262144*(8x8^2)_NN_d
Tuning for shape 64x262144*(8x8^3)_NN_d
Tuning for shape 64x262144*(8x8^4)_NN_d
Tuning for shape 64x262144*(8x8^5)_NN_d
Tuning for shape 64x262144*(8x8^6)_NN_d
Tuning for shape 64x262144*(8x8^1)_NN_d
Tuning for shape 64x262144*(8x8^2)_NN_d
Tuning for shape 64x262144*(8x8^3)_NN_d
Tuning for shape 64x262144*(8x8^4)_NN_d
Tuning for shape 64x262144*(8x8^5)_NN_d
Tuning for shape 64x262144*(8x8^1)_NN_d
Tuning for shape 64x262144*(8x8^2)_NN_d
Tuning for shape 64x262144*(8x8^3)_NN_d
Tuning for shape 64x262144*(8x8^4)_NN_d
Tuning for shape 64x262144*(8x8^1)_NN_d
Tuning for shape 64x262144*(8x8^2)_NN_d
Tuning for shape 64x262144*(8x8^3)_NN_d
Tuning for shape 64x262144*(8x8^1)_NN_d
Tuning for shape 64x262144*(8x8^2)_NN_d
Tuning for shape 64x262144*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x8^6  &  1.000 & 1.000 & 233.155 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 6 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 262144] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x to produce Y[256, 262144]
Matmul: 256 x 262144 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x262144*(8x8^1)_NN_d
Tuning for shape 256x262144*(8x8^2)_NN_d
Tuning for shape 256x262144*(8x8^3)_NN_d
Tuning for shape 256x262144*(8x8^4)_NN_d
Tuning for shape 256x262144*(8x8^5)_NN_d
Tuning for shape 256x262144*(8x8^6)_NN_d
Tuning for shape 256x262144*(8x8^1)_NN_d
Tuning for shape 256x262144*(8x8^2)_NN_d
Tuning for shape 256x262144*(8x8^3)_NN_d
Tuning for shape 256x262144*(8x8^4)_NN_d
Tuning for shape 256x262144*(8x8^5)_NN_d
Tuning for shape 256x262144*(8x8^1)_NN_d
Tuning for shape 256x262144*(8x8^2)_NN_d
Tuning for shape 256x262144*(8x8^3)_NN_d
Tuning for shape 256x262144*(8x8^4)_NN_d
Tuning for shape 256x262144*(8x8^1)_NN_d
Tuning for shape 256x262144*(8x8^2)_NN_d
Tuning for shape 256x262144*(8x8^3)_NN_d
Tuning for shape 256x262144*(8x8^1)_NN_d
Tuning for shape 256x262144*(8x8^2)_NN_d
Tuning for shape 256x262144*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x8^6  &  1.000 & 1.000 & 196.756 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 6 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 262144] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x to produce Y[1024, 262144]
Matmul: 1024 x 262144 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x262144*(8x8^1)_NN_d
Tuning for shape 1024x262144*(8x8^2)_NN_d
Tuning for shape 1024x262144*(8x8^3)_NN_d
Tuning for shape 1024x262144*(8x8^4)_NN_d
Tuning for shape 1024x262144*(8x8^5)_NN_d
Tuning for shape 1024x262144*(8x8^6)_NN_d
Tuning for shape 1024x262144*(8x8^1)_NN_d
Tuning for shape 1024x262144*(8x8^2)_NN_d
Tuning for shape 1024x262144*(8x8^3)_NN_d
Tuning for shape 1024x262144*(8x8^4)_NN_d
Tuning for shape 1024x262144*(8x8^5)_NN_d
Tuning for shape 1024x262144*(8x8^1)_NN_d
Tuning for shape 1024x262144*(8x8^2)_NN_d
Tuning for shape 1024x262144*(8x8^3)_NN_d
Tuning for shape 1024x262144*(8x8^4)_NN_d
Tuning for shape 1024x262144*(8x8^1)_NN_d
Tuning for shape 1024x262144*(8x8^2)_NN_d
Tuning for shape 1024x262144*(8x8^3)_NN_d
Tuning for shape 1024x262144*(8x8^1)_NN_d
Tuning for shape 1024x262144*(8x8^2)_NN_d
Tuning for shape 1024x262144*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x8^6  &  1.000 & 1.000 & 194.772 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2097152] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x to produce Y[1, 2097152]
Matmul: 1 x 2097152 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2097152*(8x8^1)_NN_d
Tuning for shape 1x2097152*(8x8^2)_NN_d
Tuning for shape 1x2097152*(8x8^3)_NN_d
Tuning for shape 1x2097152*(8x8^4)_NN_d
Tuning for shape 1x2097152*(8x8^5)_NN_d
Tuning for shape 1x2097152*(8x8^6)_NN_d
Tuning for shape 1x2097152*(8x8^7)_NN_d
Tuning for shape 1x2097152*(8x8^1)_NN_d
Tuning for shape 1x2097152*(8x8^2)_NN_d
Tuning for shape 1x2097152*(8x8^3)_NN_d
Tuning for shape 1x2097152*(8x8^4)_NN_d
Tuning for shape 1x2097152*(8x8^5)_NN_d
Tuning for shape 1x2097152*(8x8^6)_NN_d
Tuning for shape 1x2097152*(8x8^1)_NN_d
Tuning for shape 1x2097152*(8x8^2)_NN_d
Tuning for shape 1x2097152*(8x8^3)_NN_d
Tuning for shape 1x2097152*(8x8^4)_NN_d
Tuning for shape 1x2097152*(8x8^5)_NN_d
Tuning for shape 1x2097152*(8x8^1)_NN_d
Tuning for shape 1x2097152*(8x8^2)_NN_d
Tuning for shape 1x2097152*(8x8^3)_NN_d
Tuning for shape 1x2097152*(8x8^4)_NN_d
Tuning for shape 1x2097152*(8x8^1)_NN_d
Tuning for shape 1x2097152*(8x8^2)_NN_d
Tuning for shape 1x2097152*(8x8^3)_NN_d
Tuning for shape 1x2097152*(8x8^1)_NN_d
Tuning for shape 1x2097152*(8x8^2)_NN_d
Tuning for shape 1x2097152*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x8^7  &  1.000 & 1.000 & 233.841 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 7 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 2097152] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x to produce Y[4, 2097152]
Matmul: 4 x 2097152 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2097152*(8x8^1)_NN_d
Tuning for shape 4x2097152*(8x8^2)_NN_d
Tuning for shape 4x2097152*(8x8^3)_NN_d
Tuning for shape 4x2097152*(8x8^4)_NN_d
Tuning for shape 4x2097152*(8x8^5)_NN_d
Tuning for shape 4x2097152*(8x8^6)_NN_d
Tuning for shape 4x2097152*(8x8^7)_NN_d
Tuning for shape 4x2097152*(8x8^1)_NN_d
Tuning for shape 4x2097152*(8x8^2)_NN_d
Tuning for shape 4x2097152*(8x8^3)_NN_d
Tuning for shape 4x2097152*(8x8^4)_NN_d
Tuning for shape 4x2097152*(8x8^5)_NN_d
Tuning for shape 4x2097152*(8x8^6)_NN_d
Tuning for shape 4x2097152*(8x8^1)_NN_d
Tuning for shape 4x2097152*(8x8^2)_NN_d
Tuning for shape 4x2097152*(8x8^3)_NN_d
Tuning for shape 4x2097152*(8x8^4)_NN_d
Tuning for shape 4x2097152*(8x8^5)_NN_d
Tuning for shape 4x2097152*(8x8^1)_NN_d
Tuning for shape 4x2097152*(8x8^2)_NN_d
Tuning for shape 4x2097152*(8x8^3)_NN_d
Tuning for shape 4x2097152*(8x8^4)_NN_d
Tuning for shape 4x2097152*(8x8^1)_NN_d
Tuning for shape 4x2097152*(8x8^2)_NN_d
Tuning for shape 4x2097152*(8x8^3)_NN_d
Tuning for shape 4x2097152*(8x8^1)_NN_d
Tuning for shape 4x2097152*(8x8^2)_NN_d
Tuning for shape 4x2097152*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x8^7  &  1.000 & 1.000 & 239.602 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 7 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 2097152] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x to produce Y[16, 2097152]
Matmul: 16 x 2097152 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2097152*(8x8^1)_NN_d
Tuning for shape 16x2097152*(8x8^2)_NN_d
Tuning for shape 16x2097152*(8x8^3)_NN_d
Tuning for shape 16x2097152*(8x8^4)_NN_d
Tuning for shape 16x2097152*(8x8^5)_NN_d
Tuning for shape 16x2097152*(8x8^6)_NN_d
Tuning for shape 16x2097152*(8x8^7)_NN_d
Tuning for shape 16x2097152*(8x8^1)_NN_d
Tuning for shape 16x2097152*(8x8^2)_NN_d
Tuning for shape 16x2097152*(8x8^3)_NN_d
Tuning for shape 16x2097152*(8x8^4)_NN_d
Tuning for shape 16x2097152*(8x8^5)_NN_d
Tuning for shape 16x2097152*(8x8^6)_NN_d
Tuning for shape 16x2097152*(8x8^1)_NN_d
Tuning for shape 16x2097152*(8x8^2)_NN_d
Tuning for shape 16x2097152*(8x8^3)_NN_d
Tuning for shape 16x2097152*(8x8^4)_NN_d
Tuning for shape 16x2097152*(8x8^5)_NN_d
Tuning for shape 16x2097152*(8x8^1)_NN_d
Tuning for shape 16x2097152*(8x8^2)_NN_d
Tuning for shape 16x2097152*(8x8^3)_NN_d
Tuning for shape 16x2097152*(8x8^4)_NN_d
Tuning for shape 16x2097152*(8x8^1)_NN_d
Tuning for shape 16x2097152*(8x8^2)_NN_d
Tuning for shape 16x2097152*(8x8^3)_NN_d
Tuning for shape 16x2097152*(8x8^1)_NN_d
Tuning for shape 16x2097152*(8x8^2)_NN_d
Tuning for shape 16x2097152*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x8^7  &  1.000 & 1.000 & 244.957 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 7 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 2097152] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x to produce Y[64, 2097152]
Matmul: 64 x 2097152 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2097152*(8x8^1)_NN_d
Tuning for shape 64x2097152*(8x8^2)_NN_d
Tuning for shape 64x2097152*(8x8^3)_NN_d
Tuning for shape 64x2097152*(8x8^4)_NN_d
Tuning for shape 64x2097152*(8x8^5)_NN_d
Tuning for shape 64x2097152*(8x8^6)_NN_d
Tuning for shape 64x2097152*(8x8^7)_NN_d
Tuning for shape 64x2097152*(8x8^1)_NN_d
Tuning for shape 64x2097152*(8x8^2)_NN_d
Tuning for shape 64x2097152*(8x8^3)_NN_d
Tuning for shape 64x2097152*(8x8^4)_NN_d
Tuning for shape 64x2097152*(8x8^5)_NN_d
Tuning for shape 64x2097152*(8x8^6)_NN_d
Tuning for shape 64x2097152*(8x8^1)_NN_d
Tuning for shape 64x2097152*(8x8^2)_NN_d
Tuning for shape 64x2097152*(8x8^3)_NN_d
Tuning for shape 64x2097152*(8x8^4)_NN_d
Tuning for shape 64x2097152*(8x8^5)_NN_d
Tuning for shape 64x2097152*(8x8^1)_NN_d
Tuning for shape 64x2097152*(8x8^2)_NN_d
Tuning for shape 64x2097152*(8x8^3)_NN_d
Tuning for shape 64x2097152*(8x8^4)_NN_d
Tuning for shape 64x2097152*(8x8^1)_NN_d
Tuning for shape 64x2097152*(8x8^2)_NN_d
Tuning for shape 64x2097152*(8x8^3)_NN_d
Tuning for shape 64x2097152*(8x8^1)_NN_d
Tuning for shape 64x2097152*(8x8^2)_NN_d
Tuning for shape 64x2097152*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x8^7  &  1.000 & 1.000 & 236.785 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 7 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 2097152] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x to produce Y[256, 2097152]
Matmul: 256 x 2097152 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2097152*(8x8^1)_NN_d
Tuning for shape 256x2097152*(8x8^2)_NN_d
Tuning for shape 256x2097152*(8x8^3)_NN_d
Tuning for shape 256x2097152*(8x8^4)_NN_d
Tuning for shape 256x2097152*(8x8^5)_NN_d
Tuning for shape 256x2097152*(8x8^6)_NN_d
Tuning for shape 256x2097152*(8x8^7)_NN_d
Tuning for shape 256x2097152*(8x8^1)_NN_d
Tuning for shape 256x2097152*(8x8^2)_NN_d
Tuning for shape 256x2097152*(8x8^3)_NN_d
Tuning for shape 256x2097152*(8x8^4)_NN_d
Tuning for shape 256x2097152*(8x8^5)_NN_d
Tuning for shape 256x2097152*(8x8^6)_NN_d
Tuning for shape 256x2097152*(8x8^1)_NN_d
Tuning for shape 256x2097152*(8x8^2)_NN_d
Tuning for shape 256x2097152*(8x8^3)_NN_d
Tuning for shape 256x2097152*(8x8^4)_NN_d
Tuning for shape 256x2097152*(8x8^5)_NN_d
Tuning for shape 256x2097152*(8x8^1)_NN_d
Tuning for shape 256x2097152*(8x8^2)_NN_d
Tuning for shape 256x2097152*(8x8^3)_NN_d
Tuning for shape 256x2097152*(8x8^4)_NN_d
Tuning for shape 256x2097152*(8x8^1)_NN_d
Tuning for shape 256x2097152*(8x8^2)_NN_d
Tuning for shape 256x2097152*(8x8^3)_NN_d
Tuning for shape 256x2097152*(8x8^1)_NN_d
Tuning for shape 256x2097152*(8x8^2)_NN_d
Tuning for shape 256x2097152*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x8^7  &  1.000 & 1.000 & -1.000 & -1.000
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 8 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16777216] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x F_7 [8, 8] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 16777216, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16777216*(8x8^1)_NN_d
Tuning for shape 1x16777216*(8x8^2)_NN_d
Tuning for shape 1x16777216*(8x8^3)_NN_d
Tuning for shape 1x16777216*(8x8^4)_NN_d
Tuning for shape 1x16777216*(8x8^5)_NN_d
Tuning for shape 1x16777216*(8x8^6)_NN_d
Tuning for shape 1x16777216*(8x8^7)_NN_d
Tuning for shape 1x16777216*(8x8^8)_NN_d
Tuning for shape 1x16777216*(8x8^1)_NN_d
Tuning for shape 1x16777216*(8x8^2)_NN_d
Tuning for shape 1x16777216*(8x8^3)_NN_d
Tuning for shape 1x16777216*(8x8^4)_NN_d
Tuning for shape 1x16777216*(8x8^5)_NN_d
Tuning for shape 1x16777216*(8x8^6)_NN_d
Tuning for shape 1x16777216*(8x8^7)_NN_d
Tuning for shape 1x16777216*(8x8^1)_NN_d
Tuning for shape 1x16777216*(8x8^2)_NN_d
Tuning for shape 1x16777216*(8x8^3)_NN_d
Tuning for shape 1x16777216*(8x8^4)_NN_d
Tuning for shape 1x16777216*(8x8^5)_NN_d
Tuning for shape 1x16777216*(8x8^6)_NN_d
Tuning for shape 1x16777216*(8x8^1)_NN_d
Tuning for shape 1x16777216*(8x8^2)_NN_d
Tuning for shape 1x16777216*(8x8^3)_NN_d
Tuning for shape 1x16777216*(8x8^4)_NN_d
Tuning for shape 1x16777216*(8x8^5)_NN_d
Tuning for shape 1x16777216*(8x8^1)_NN_d
Tuning for shape 1x16777216*(8x8^2)_NN_d
Tuning for shape 1x16777216*(8x8^3)_NN_d
Tuning for shape 1x16777216*(8x8^4)_NN_d
Tuning for shape 1x16777216*(8x8^1)_NN_d
Tuning for shape 1x16777216*(8x8^2)_NN_d
Tuning for shape 1x16777216*(8x8^3)_NN_d
Tuning for shape 1x16777216*(8x8^1)_NN_d
Tuning for shape 1x16777216*(8x8^2)_NN_d
Tuning for shape 1x16777216*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x8^8  &  1.000 & 1.000 & 253.764 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 8 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16777216] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x F_7 [8, 8] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 16777216, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16777216*(8x8^1)_NN_d
Tuning for shape 4x16777216*(8x8^2)_NN_d
Tuning for shape 4x16777216*(8x8^3)_NN_d
Tuning for shape 4x16777216*(8x8^4)_NN_d
Tuning for shape 4x16777216*(8x8^5)_NN_d
Tuning for shape 4x16777216*(8x8^6)_NN_d
Tuning for shape 4x16777216*(8x8^7)_NN_d
Tuning for shape 4x16777216*(8x8^8)_NN_d
Tuning for shape 4x16777216*(8x8^1)_NN_d
Tuning for shape 4x16777216*(8x8^2)_NN_d
Tuning for shape 4x16777216*(8x8^3)_NN_d
Tuning for shape 4x16777216*(8x8^4)_NN_d
Tuning for shape 4x16777216*(8x8^5)_NN_d
Tuning for shape 4x16777216*(8x8^6)_NN_d
Tuning for shape 4x16777216*(8x8^7)_NN_d
Tuning for shape 4x16777216*(8x8^1)_NN_d
Tuning for shape 4x16777216*(8x8^2)_NN_d
Tuning for shape 4x16777216*(8x8^3)_NN_d
Tuning for shape 4x16777216*(8x8^4)_NN_d
Tuning for shape 4x16777216*(8x8^5)_NN_d
Tuning for shape 4x16777216*(8x8^6)_NN_d
Tuning for shape 4x16777216*(8x8^1)_NN_d
Tuning for shape 4x16777216*(8x8^2)_NN_d
Tuning for shape 4x16777216*(8x8^3)_NN_d
Tuning for shape 4x16777216*(8x8^4)_NN_d
Tuning for shape 4x16777216*(8x8^5)_NN_d
Tuning for shape 4x16777216*(8x8^1)_NN_d
Tuning for shape 4x16777216*(8x8^2)_NN_d
Tuning for shape 4x16777216*(8x8^3)_NN_d
Tuning for shape 4x16777216*(8x8^4)_NN_d
Tuning for shape 4x16777216*(8x8^1)_NN_d
Tuning for shape 4x16777216*(8x8^2)_NN_d
Tuning for shape 4x16777216*(8x8^3)_NN_d
Tuning for shape 4x16777216*(8x8^1)_NN_d
Tuning for shape 4x16777216*(8x8^2)_NN_d
Tuning for shape 4x16777216*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x8^8  &  1.000 & 1.000 & 246.488 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 8 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16777216] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x F_7 [8, 8] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 16777216, Num KP Factors: 8
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16777216*(8x8^1)_NN_d
Tuning for shape 16x16777216*(8x8^2)_NN_d
Tuning for shape 16x16777216*(8x8^3)_NN_d
Tuning for shape 16x16777216*(8x8^4)_NN_d
Tuning for shape 16x16777216*(8x8^5)_NN_d
Tuning for shape 16x16777216*(8x8^6)_NN_d
Tuning for shape 16x16777216*(8x8^7)_NN_d
Tuning for shape 16x16777216*(8x8^8)_NN_d
Tuning for shape 16x16777216*(8x8^1)_NN_d
Tuning for shape 16x16777216*(8x8^2)_NN_d
Tuning for shape 16x16777216*(8x8^3)_NN_d
Tuning for shape 16x16777216*(8x8^4)_NN_d
Tuning for shape 16x16777216*(8x8^5)_NN_d
Tuning for shape 16x16777216*(8x8^6)_NN_d
Tuning for shape 16x16777216*(8x8^7)_NN_d
Tuning for shape 16x16777216*(8x8^1)_NN_d
Tuning for shape 16x16777216*(8x8^2)_NN_d
Tuning for shape 16x16777216*(8x8^3)_NN_d
Tuning for shape 16x16777216*(8x8^4)_NN_d
Tuning for shape 16x16777216*(8x8^5)_NN_d
Tuning for shape 16x16777216*(8x8^6)_NN_d
Tuning for shape 16x16777216*(8x8^1)_NN_d
Tuning for shape 16x16777216*(8x8^2)_NN_d
Tuning for shape 16x16777216*(8x8^3)_NN_d
Tuning for shape 16x16777216*(8x8^4)_NN_d
Tuning for shape 16x16777216*(8x8^5)_NN_d
Tuning for shape 16x16777216*(8x8^1)_NN_d
Tuning for shape 16x16777216*(8x8^2)_NN_d
Tuning for shape 16x16777216*(8x8^3)_NN_d
Tuning for shape 16x16777216*(8x8^4)_NN_d
Tuning for shape 16x16777216*(8x8^1)_NN_d
Tuning for shape 16x16777216*(8x8^2)_NN_d
Tuning for shape 16x16777216*(8x8^3)_NN_d
Tuning for shape 16x16777216*(8x8^1)_NN_d
Tuning for shape 16x16777216*(8x8^2)_NN_d
Tuning for shape 16x16777216*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x8^8  &  1.000 & 1.000 & 246.897 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 9 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 134217728] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x F_7 [8, 8] x F_8 [8, 8] x to produce Y[1, 134217728]
Matmul: 1 x 134217728 x 134217728, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x134217728*(8x8^1)_NN_d
Tuning for shape 1x134217728*(8x8^2)_NN_d
Tuning for shape 1x134217728*(8x8^3)_NN_d
Tuning for shape 1x134217728*(8x8^4)_NN_d
Tuning for shape 1x134217728*(8x8^5)_NN_d
Tuning for shape 1x134217728*(8x8^6)_NN_d
Tuning for shape 1x134217728*(8x8^7)_NN_d
Tuning for shape 1x134217728*(8x8^8)_NN_d
Tuning for shape 1x134217728*(8x8^9)_NN_d
Tuning for shape 1x134217728*(8x8^1)_NN_d
Tuning for shape 1x134217728*(8x8^2)_NN_d
Tuning for shape 1x134217728*(8x8^3)_NN_d
Tuning for shape 1x134217728*(8x8^4)_NN_d
Tuning for shape 1x134217728*(8x8^5)_NN_d
Tuning for shape 1x134217728*(8x8^6)_NN_d
Tuning for shape 1x134217728*(8x8^7)_NN_d
Tuning for shape 1x134217728*(8x8^8)_NN_d
Tuning for shape 1x134217728*(8x8^1)_NN_d
Tuning for shape 1x134217728*(8x8^2)_NN_d
Tuning for shape 1x134217728*(8x8^3)_NN_d
Tuning for shape 1x134217728*(8x8^4)_NN_d
Tuning for shape 1x134217728*(8x8^5)_NN_d
Tuning for shape 1x134217728*(8x8^6)_NN_d
Tuning for shape 1x134217728*(8x8^7)_NN_d
Tuning for shape 1x134217728*(8x8^1)_NN_d
Tuning for shape 1x134217728*(8x8^2)_NN_d
Tuning for shape 1x134217728*(8x8^3)_NN_d
Tuning for shape 1x134217728*(8x8^4)_NN_d
Tuning for shape 1x134217728*(8x8^5)_NN_d
Tuning for shape 1x134217728*(8x8^6)_NN_d
Tuning for shape 1x134217728*(8x8^1)_NN_d
Tuning for shape 1x134217728*(8x8^2)_NN_d
Tuning for shape 1x134217728*(8x8^3)_NN_d
Tuning for shape 1x134217728*(8x8^4)_NN_d
Tuning for shape 1x134217728*(8x8^5)_NN_d
Tuning for shape 1x134217728*(8x8^1)_NN_d
Tuning for shape 1x134217728*(8x8^2)_NN_d
Tuning for shape 1x134217728*(8x8^3)_NN_d
Tuning for shape 1x134217728*(8x8^4)_NN_d
Tuning for shape 1x134217728*(8x8^1)_NN_d
Tuning for shape 1x134217728*(8x8^2)_NN_d
Tuning for shape 1x134217728*(8x8^3)_NN_d
Tuning for shape 1x134217728*(8x8^1)_NN_d
Tuning for shape 1x134217728*(8x8^2)_NN_d
Tuning for shape 1x134217728*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x8^9  &  1.000 & 1.000 & 256.069 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 9 -p 8 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 134217728] with F_0 [8, 8] x F_1 [8, 8] x F_2 [8, 8] x F_3 [8, 8] x F_4 [8, 8] x F_5 [8, 8] x F_6 [8, 8] x F_7 [8, 8] x F_8 [8, 8] x to produce Y[4, 134217728]
Matmul: 4 x 134217728 x 134217728, Num KP Factors: 9
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x134217728*(8x8^1)_NN_d
Tuning for shape 4x134217728*(8x8^2)_NN_d
Tuning for shape 4x134217728*(8x8^3)_NN_d
Tuning for shape 4x134217728*(8x8^4)_NN_d
Tuning for shape 4x134217728*(8x8^5)_NN_d
Tuning for shape 4x134217728*(8x8^6)_NN_d
Tuning for shape 4x134217728*(8x8^7)_NN_d
Tuning for shape 4x134217728*(8x8^8)_NN_d
Tuning for shape 4x134217728*(8x8^9)_NN_d
Tuning for shape 4x134217728*(8x8^1)_NN_d
Tuning for shape 4x134217728*(8x8^2)_NN_d
Tuning for shape 4x134217728*(8x8^3)_NN_d
Tuning for shape 4x134217728*(8x8^4)_NN_d
Tuning for shape 4x134217728*(8x8^5)_NN_d
Tuning for shape 4x134217728*(8x8^6)_NN_d
Tuning for shape 4x134217728*(8x8^7)_NN_d
Tuning for shape 4x134217728*(8x8^8)_NN_d
Tuning for shape 4x134217728*(8x8^1)_NN_d
Tuning for shape 4x134217728*(8x8^2)_NN_d
Tuning for shape 4x134217728*(8x8^3)_NN_d
Tuning for shape 4x134217728*(8x8^4)_NN_d
Tuning for shape 4x134217728*(8x8^5)_NN_d
Tuning for shape 4x134217728*(8x8^6)_NN_d
Tuning for shape 4x134217728*(8x8^7)_NN_d
Tuning for shape 4x134217728*(8x8^1)_NN_d
Tuning for shape 4x134217728*(8x8^2)_NN_d
Tuning for shape 4x134217728*(8x8^3)_NN_d
Tuning for shape 4x134217728*(8x8^4)_NN_d
Tuning for shape 4x134217728*(8x8^5)_NN_d
Tuning for shape 4x134217728*(8x8^6)_NN_d
Tuning for shape 4x134217728*(8x8^1)_NN_d
Tuning for shape 4x134217728*(8x8^2)_NN_d
Tuning for shape 4x134217728*(8x8^3)_NN_d
Tuning for shape 4x134217728*(8x8^4)_NN_d
Tuning for shape 4x134217728*(8x8^5)_NN_d
Tuning for shape 4x134217728*(8x8^1)_NN_d
Tuning for shape 4x134217728*(8x8^2)_NN_d
Tuning for shape 4x134217728*(8x8^3)_NN_d
Tuning for shape 4x134217728*(8x8^4)_NN_d
Tuning for shape 4x134217728*(8x8^1)_NN_d
Tuning for shape 4x134217728*(8x8^2)_NN_d
Tuning for shape 4x134217728*(8x8^3)_NN_d
Tuning for shape 4x134217728*(8x8^1)_NN_d
Tuning for shape 4x134217728*(8x8^2)_NN_d
Tuning for shape 4x134217728*(8x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x8^9  &  1.000 & 1.000 & -1.000 & -1.000
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [8, 16] x to produce Y[1, 16]
Matmul: 1 x 16 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x16^1  &  1.000 & 1.000 & 0.001 & 1029.205
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [8, 16] x to produce Y[4, 16]
Matmul: 4 x 16 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x16^1  &  1.000 & 1.000 & 0.003 & 291.527
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [8, 16] x to produce Y[16, 16]
Matmul: 16 x 16 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x16^1  &  1.000 & 1.000 & 0.013 & 78.764
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [8, 16] x to produce Y[64, 16]
Matmul: 64 x 16 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x16^1  &  1.000 & 1.000 & 0.050 & 20.202
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [8, 16] x to produce Y[256, 16]
Matmul: 256 x 16 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x16^1  &  1.000 & 1.000 & 0.120 & 8.348
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [8, 16] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x16^1  &  1.000 & 1.000 & 0.793 & 1.260
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [8, 16] x F_1 [8, 16] x to produce Y[1, 256]
Matmul: 1 x 256 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(8x16^1)_NN_d
Tuning for shape 1x64*(8x16^2)_NN_d
Tuning for shape 1x64*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x16^2  &  1.000 & 1.000 & 0.015 & 68.437
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [8, 16] x F_1 [8, 16] x to produce Y[4, 256]
Matmul: 4 x 256 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(8x16^1)_NN_d
Tuning for shape 4x64*(8x16^2)_NN_d
Tuning for shape 4x64*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x16^2  &  1.000 & 1.000 & 0.055 & 18.198
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [8, 16] x F_1 [8, 16] x to produce Y[16, 256]
Matmul: 16 x 256 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(8x16^1)_NN_d
Tuning for shape 16x64*(8x16^2)_NN_d
Tuning for shape 16x64*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x16^2  &  1.000 & 1.000 & 0.214 & 4.673
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [8, 16] x F_1 [8, 16] x to produce Y[64, 256]
Matmul: 64 x 256 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(8x16^1)_NN_d
Tuning for shape 64x64*(8x16^2)_NN_d
Tuning for shape 64x64*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x16^2  &  1.000 & 1.000 & 0.846 & 1.182
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [8, 16] x F_1 [8, 16] x to produce Y[256, 256]
Matmul: 256 x 256 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(8x16^1)_NN_d
Tuning for shape 256x64*(8x16^2)_NN_d
Tuning for shape 256x64*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x16^2  &  1.000 & 1.000 & 3.498 & 0.286
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [8, 16] x F_1 [8, 16] x to produce Y[1024, 256]
Matmul: 1024 x 256 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(8x16^1)_NN_d
Tuning for shape 1024x64*(8x16^2)_NN_d
Tuning for shape 1024x64*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x16^2  &  1.000 & 1.000 & 13.298 & 0.075
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2048*(8x16^1)_NN_d
Tuning for shape 1x1024*(8x16^2)_NN_d
Tuning for shape 1x512*(8x16^3)_NN_d
Tuning for shape 1x1024*(8x16^1)_NN_d
Tuning for shape 1x512*(8x16^2)_NN_d
Tuning for shape 1x512*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x16^3  &  1.000 & 1.000 & 0.215 & 4.659
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2048*(8x16^1)_NN_d
Tuning for shape 4x1024*(8x16^2)_NN_d
Tuning for shape 4x512*(8x16^3)_NN_d
Tuning for shape 4x1024*(8x16^1)_NN_d
Tuning for shape 4x512*(8x16^2)_NN_d
Tuning for shape 4x512*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x16^3  &  1.000 & 1.000 & 0.811 & 1.233
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 512] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2048*(8x16^1)_NN_d
Tuning for shape 16x1024*(8x16^2)_NN_d
Tuning for shape 16x512*(8x16^3)_NN_d
Tuning for shape 16x1024*(8x16^1)_NN_d
Tuning for shape 16x512*(8x16^2)_NN_d
Tuning for shape 16x512*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x16^3  &  1.000 & 1.000 & 3.335 & 0.300
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 512] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x2048*(8x16^1)_NN_d
Tuning for shape 64x1024*(8x16^2)_NN_d
Tuning for shape 64x512*(8x16^3)_NN_d
Tuning for shape 64x1024*(8x16^1)_NN_d
Tuning for shape 64x512*(8x16^2)_NN_d
Tuning for shape 64x512*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x16^3  &  1.000 & 1.000 & 13.077 & 0.076
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 512] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x2048*(8x16^1)_NN_d
Tuning for shape 256x1024*(8x16^2)_NN_d
Tuning for shape 256x512*(8x16^3)_NN_d
Tuning for shape 256x1024*(8x16^1)_NN_d
Tuning for shape 256x512*(8x16^2)_NN_d
Tuning for shape 256x512*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x16^3  &  1.000 & 1.000 & 39.349 & 0.025
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 512] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x2048*(8x16^1)_NN_d
Tuning for shape 1024x1024*(8x16^2)_NN_d
Tuning for shape 1024x512*(8x16^3)_NN_d
Tuning for shape 1024x1024*(8x16^1)_NN_d
Tuning for shape 1024x512*(8x16^2)_NN_d
Tuning for shape 1024x512*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x16^3  &  1.000 & 1.000 & 197.226 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x to produce Y[1, 65536]
Matmul: 1 x 65536 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32768*(8x16^1)_NN_d
Tuning for shape 1x16384*(8x16^2)_NN_d
Tuning for shape 1x8192*(8x16^3)_NN_d
Tuning for shape 1x4096*(8x16^4)_NN_d
Tuning for shape 1x16384*(8x16^1)_NN_d
Tuning for shape 1x8192*(8x16^2)_NN_d
Tuning for shape 1x4096*(8x16^3)_NN_d
Tuning for shape 1x8192*(8x16^1)_NN_d
Tuning for shape 1x4096*(8x16^2)_NN_d
Tuning for shape 1x4096*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x16^4  &  1.000 & 1.000 & 3.148 & 0.318
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x to produce Y[4, 65536]
Matmul: 4 x 65536 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32768*(8x16^1)_NN_d
Tuning for shape 4x16384*(8x16^2)_NN_d
Tuning for shape 4x8192*(8x16^3)_NN_d
Tuning for shape 4x4096*(8x16^4)_NN_d
Tuning for shape 4x16384*(8x16^1)_NN_d
Tuning for shape 4x8192*(8x16^2)_NN_d
Tuning for shape 4x4096*(8x16^3)_NN_d
Tuning for shape 4x8192*(8x16^1)_NN_d
Tuning for shape 4x4096*(8x16^2)_NN_d
Tuning for shape 4x4096*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x16^4  &  1.000 & 1.000 & 12.288 & 0.081
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x to produce Y[16, 65536]
Matmul: 16 x 65536 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32768*(8x16^1)_NN_d
Tuning for shape 16x16384*(8x16^2)_NN_d
Tuning for shape 16x8192*(8x16^3)_NN_d
Tuning for shape 16x4096*(8x16^4)_NN_d
Tuning for shape 16x16384*(8x16^1)_NN_d
Tuning for shape 16x8192*(8x16^2)_NN_d
Tuning for shape 16x4096*(8x16^3)_NN_d
Tuning for shape 16x8192*(8x16^1)_NN_d
Tuning for shape 16x4096*(8x16^2)_NN_d
Tuning for shape 16x4096*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x16^4  &  1.000 & 1.000 & 45.327 & 0.022
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x to produce Y[64, 65536]
Matmul: 64 x 65536 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32768*(8x16^1)_NN_d
Tuning for shape 64x16384*(8x16^2)_NN_d
Tuning for shape 64x8192*(8x16^3)_NN_d
Tuning for shape 64x4096*(8x16^4)_NN_d
Tuning for shape 64x16384*(8x16^1)_NN_d
Tuning for shape 64x8192*(8x16^2)_NN_d
Tuning for shape 64x4096*(8x16^3)_NN_d
Tuning for shape 64x8192*(8x16^1)_NN_d
Tuning for shape 64x4096*(8x16^2)_NN_d
Tuning for shape 64x4096*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x16^4  &  1.000 & 1.000 & 174.005 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x to produce Y[256, 65536]
Matmul: 256 x 65536 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32768*(8x16^1)_NN_d
Tuning for shape 256x16384*(8x16^2)_NN_d
Tuning for shape 256x8192*(8x16^3)_NN_d
Tuning for shape 256x4096*(8x16^4)_NN_d
Tuning for shape 256x16384*(8x16^1)_NN_d
Tuning for shape 256x8192*(8x16^2)_NN_d
Tuning for shape 256x4096*(8x16^3)_NN_d
Tuning for shape 256x8192*(8x16^1)_NN_d
Tuning for shape 256x4096*(8x16^2)_NN_d
Tuning for shape 256x4096*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x16^4  &  1.000 & 1.000 & 235.632 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x to produce Y[1024, 65536]
Matmul: 1024 x 65536 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32768*(8x16^1)_NN_d
Tuning for shape 1024x16384*(8x16^2)_NN_d
Tuning for shape 1024x8192*(8x16^3)_NN_d
Tuning for shape 1024x4096*(8x16^4)_NN_d
Tuning for shape 1024x16384*(8x16^1)_NN_d
Tuning for shape 1024x8192*(8x16^2)_NN_d
Tuning for shape 1024x4096*(8x16^3)_NN_d
Tuning for shape 1024x8192*(8x16^1)_NN_d
Tuning for shape 1024x4096*(8x16^2)_NN_d
Tuning for shape 1024x4096*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x16^4  &  1.000 & 1.000 & 251.733 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32768] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x F_4 [8, 16] x to produce Y[1, 1048576]
Matmul: 1 x 1048576 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x524288*(8x16^1)_NN_d
Tuning for shape 1x262144*(8x16^2)_NN_d
Tuning for shape 1x131072*(8x16^3)_NN_d
Tuning for shape 1x65536*(8x16^4)_NN_d
Tuning for shape 1x32768*(8x16^5)_NN_d
Tuning for shape 1x262144*(8x16^1)_NN_d
Tuning for shape 1x131072*(8x16^2)_NN_d
Tuning for shape 1x65536*(8x16^3)_NN_d
Tuning for shape 1x32768*(8x16^4)_NN_d
Tuning for shape 1x131072*(8x16^1)_NN_d
Tuning for shape 1x65536*(8x16^2)_NN_d
Tuning for shape 1x32768*(8x16^3)_NN_d
Tuning for shape 1x65536*(8x16^1)_NN_d
Tuning for shape 1x32768*(8x16^2)_NN_d
Tuning for shape 1x32768*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x16^5  &  1.000 & 1.000 & 43.214 & 0.023
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32768] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x F_4 [8, 16] x to produce Y[4, 1048576]
Matmul: 4 x 1048576 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x524288*(8x16^1)_NN_d
Tuning for shape 4x262144*(8x16^2)_NN_d
Tuning for shape 4x131072*(8x16^3)_NN_d
Tuning for shape 4x65536*(8x16^4)_NN_d
Tuning for shape 4x32768*(8x16^5)_NN_d
Tuning for shape 4x262144*(8x16^1)_NN_d
Tuning for shape 4x131072*(8x16^2)_NN_d
Tuning for shape 4x65536*(8x16^3)_NN_d
Tuning for shape 4x32768*(8x16^4)_NN_d
Tuning for shape 4x131072*(8x16^1)_NN_d
Tuning for shape 4x65536*(8x16^2)_NN_d
Tuning for shape 4x32768*(8x16^3)_NN_d
Tuning for shape 4x65536*(8x16^1)_NN_d
Tuning for shape 4x32768*(8x16^2)_NN_d
Tuning for shape 4x32768*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x16^5  &  1.000 & 1.000 & 156.064 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32768] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x F_4 [8, 16] x to produce Y[16, 1048576]
Matmul: 16 x 1048576 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x524288*(8x16^1)_NN_d
Tuning for shape 16x262144*(8x16^2)_NN_d
Tuning for shape 16x131072*(8x16^3)_NN_d
Tuning for shape 16x65536*(8x16^4)_NN_d
Tuning for shape 16x32768*(8x16^5)_NN_d
Tuning for shape 16x262144*(8x16^1)_NN_d
Tuning for shape 16x131072*(8x16^2)_NN_d
Tuning for shape 16x65536*(8x16^3)_NN_d
Tuning for shape 16x32768*(8x16^4)_NN_d
Tuning for shape 16x131072*(8x16^1)_NN_d
Tuning for shape 16x65536*(8x16^2)_NN_d
Tuning for shape 16x32768*(8x16^3)_NN_d
Tuning for shape 16x65536*(8x16^1)_NN_d
Tuning for shape 16x32768*(8x16^2)_NN_d
Tuning for shape 16x32768*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x16^5  &  1.000 & 1.000 & 246.097 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 32768] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x F_4 [8, 16] x to produce Y[64, 1048576]
Matmul: 64 x 1048576 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x524288*(8x16^1)_NN_d
Tuning for shape 64x262144*(8x16^2)_NN_d
Tuning for shape 64x131072*(8x16^3)_NN_d
Tuning for shape 64x65536*(8x16^4)_NN_d
Tuning for shape 64x32768*(8x16^5)_NN_d
Tuning for shape 64x262144*(8x16^1)_NN_d
Tuning for shape 64x131072*(8x16^2)_NN_d
Tuning for shape 64x65536*(8x16^3)_NN_d
Tuning for shape 64x32768*(8x16^4)_NN_d
Tuning for shape 64x131072*(8x16^1)_NN_d
Tuning for shape 64x65536*(8x16^2)_NN_d
Tuning for shape 64x32768*(8x16^3)_NN_d
Tuning for shape 64x65536*(8x16^1)_NN_d
Tuning for shape 64x32768*(8x16^2)_NN_d
Tuning for shape 64x32768*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x16^5  &  1.000 & 1.000 & 254.315 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 32768] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x F_4 [8, 16] x to produce Y[256, 1048576]
Matmul: 256 x 1048576 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x524288*(8x16^1)_NN_d
Tuning for shape 256x262144*(8x16^2)_NN_d
Tuning for shape 256x131072*(8x16^3)_NN_d
Tuning for shape 256x65536*(8x16^4)_NN_d
Tuning for shape 256x32768*(8x16^5)_NN_d
Tuning for shape 256x262144*(8x16^1)_NN_d
Tuning for shape 256x131072*(8x16^2)_NN_d
Tuning for shape 256x65536*(8x16^3)_NN_d
Tuning for shape 256x32768*(8x16^4)_NN_d
Tuning for shape 256x131072*(8x16^1)_NN_d
Tuning for shape 256x65536*(8x16^2)_NN_d
Tuning for shape 256x32768*(8x16^3)_NN_d
Tuning for shape 256x65536*(8x16^1)_NN_d
Tuning for shape 256x32768*(8x16^2)_NN_d
Tuning for shape 256x32768*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x16^5  &  1.000 & 1.000 & 252.515 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 262144] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x F_4 [8, 16] x F_5 [8, 16] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8388608*(8x16^1)_NN_d
Tuning for shape 1x4194304*(8x16^2)_NN_d
Tuning for shape 1x2097152*(8x16^3)_NN_d
Tuning for shape 1x1048576*(8x16^4)_NN_d
Tuning for shape 1x524288*(8x16^5)_NN_d
Tuning for shape 1x262144*(8x16^6)_NN_d
Tuning for shape 1x4194304*(8x16^1)_NN_d
Tuning for shape 1x2097152*(8x16^2)_NN_d
Tuning for shape 1x1048576*(8x16^3)_NN_d
Tuning for shape 1x524288*(8x16^4)_NN_d
Tuning for shape 1x262144*(8x16^5)_NN_d
Tuning for shape 1x2097152*(8x16^1)_NN_d
Tuning for shape 1x1048576*(8x16^2)_NN_d
Tuning for shape 1x524288*(8x16^3)_NN_d
Tuning for shape 1x262144*(8x16^4)_NN_d
Tuning for shape 1x1048576*(8x16^1)_NN_d
Tuning for shape 1x524288*(8x16^2)_NN_d
Tuning for shape 1x262144*(8x16^3)_NN_d
Tuning for shape 1x524288*(8x16^1)_NN_d
Tuning for shape 1x262144*(8x16^2)_NN_d
Tuning for shape 1x262144*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x16^6  &  1.000 & 1.000 & 247.636 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 262144] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x F_4 [8, 16] x F_5 [8, 16] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8388608*(8x16^1)_NN_d
Tuning for shape 4x4194304*(8x16^2)_NN_d
Tuning for shape 4x2097152*(8x16^3)_NN_d
Tuning for shape 4x1048576*(8x16^4)_NN_d
Tuning for shape 4x524288*(8x16^5)_NN_d
Tuning for shape 4x262144*(8x16^6)_NN_d
Tuning for shape 4x4194304*(8x16^1)_NN_d
Tuning for shape 4x2097152*(8x16^2)_NN_d
Tuning for shape 4x1048576*(8x16^3)_NN_d
Tuning for shape 4x524288*(8x16^4)_NN_d
Tuning for shape 4x262144*(8x16^5)_NN_d
Tuning for shape 4x2097152*(8x16^1)_NN_d
Tuning for shape 4x1048576*(8x16^2)_NN_d
Tuning for shape 4x524288*(8x16^3)_NN_d
Tuning for shape 4x262144*(8x16^4)_NN_d
Tuning for shape 4x1048576*(8x16^1)_NN_d
Tuning for shape 4x524288*(8x16^2)_NN_d
Tuning for shape 4x262144*(8x16^3)_NN_d
Tuning for shape 4x524288*(8x16^1)_NN_d
Tuning for shape 4x262144*(8x16^2)_NN_d
Tuning for shape 4x262144*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x16^6  &  1.000 & 1.000 & 255.533 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 262144] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x F_4 [8, 16] x F_5 [8, 16] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 262144, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8388608*(8x16^1)_NN_d
Tuning for shape 16x4194304*(8x16^2)_NN_d
Tuning for shape 16x2097152*(8x16^3)_NN_d
Tuning for shape 16x1048576*(8x16^4)_NN_d
Tuning for shape 16x524288*(8x16^5)_NN_d
Tuning for shape 16x262144*(8x16^6)_NN_d
Tuning for shape 16x4194304*(8x16^1)_NN_d
Tuning for shape 16x2097152*(8x16^2)_NN_d
Tuning for shape 16x1048576*(8x16^3)_NN_d
Tuning for shape 16x524288*(8x16^4)_NN_d
Tuning for shape 16x262144*(8x16^5)_NN_d
Tuning for shape 16x2097152*(8x16^1)_NN_d
Tuning for shape 16x1048576*(8x16^2)_NN_d
Tuning for shape 16x524288*(8x16^3)_NN_d
Tuning for shape 16x262144*(8x16^4)_NN_d
Tuning for shape 16x1048576*(8x16^1)_NN_d
Tuning for shape 16x524288*(8x16^2)_NN_d
Tuning for shape 16x262144*(8x16^3)_NN_d
Tuning for shape 16x524288*(8x16^1)_NN_d
Tuning for shape 16x262144*(8x16^2)_NN_d
Tuning for shape 16x262144*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x16^6  &  1.000 & 1.000 & 252.273 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 8 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 2097152] with F_0 [8, 16] x F_1 [8, 16] x F_2 [8, 16] x F_3 [8, 16] x F_4 [8, 16] x F_5 [8, 16] x F_6 [8, 16] x to produce Y[1, 268435456]
Matmul: 1 x 268435456 x 2097152, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x134217728*(8x16^1)_NN_d
Tuning for shape 1x67108864*(8x16^2)_NN_d
Tuning for shape 1x33554432*(8x16^3)_NN_d
Tuning for shape 1x16777216*(8x16^4)_NN_d
Tuning for shape 1x8388608*(8x16^5)_NN_d
Tuning for shape 1x4194304*(8x16^6)_NN_d
Tuning for shape 1x2097152*(8x16^7)_NN_d
Tuning for shape 1x67108864*(8x16^1)_NN_d
Tuning for shape 1x33554432*(8x16^2)_NN_d
Tuning for shape 1x16777216*(8x16^3)_NN_d
Tuning for shape 1x8388608*(8x16^4)_NN_d
Tuning for shape 1x4194304*(8x16^5)_NN_d
Tuning for shape 1x2097152*(8x16^6)_NN_d
Tuning for shape 1x33554432*(8x16^1)_NN_d
Tuning for shape 1x16777216*(8x16^2)_NN_d
Tuning for shape 1x8388608*(8x16^3)_NN_d
Tuning for shape 1x4194304*(8x16^4)_NN_d
Tuning for shape 1x2097152*(8x16^5)_NN_d
Tuning for shape 1x16777216*(8x16^1)_NN_d
Tuning for shape 1x8388608*(8x16^2)_NN_d
Tuning for shape 1x4194304*(8x16^3)_NN_d
Tuning for shape 1x2097152*(8x16^4)_NN_d
Tuning for shape 1x8388608*(8x16^1)_NN_d
Tuning for shape 1x4194304*(8x16^2)_NN_d
Tuning for shape 1x2097152*(8x16^3)_NN_d
Tuning for shape 1x4194304*(8x16^1)_NN_d
Tuning for shape 1x2097152*(8x16^2)_NN_d
Tuning for shape 1x2097152*(8x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x16^7  &  1.000 & 1.000 & 252.743 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [8, 32] x to produce Y[1, 32]
Matmul: 1 x 32 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x32^1  &  1.000 & 1.000 & 0.002 & 506.896
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [8, 32] x to produce Y[4, 32]
Matmul: 4 x 32 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x32^1  &  1.000 & 1.000 & 0.003 & 302.063
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [8, 32] x to produce Y[16, 32]
Matmul: 16 x 32 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x32^1  &  1.000 & 1.000 & 0.026 & 38.563
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [8, 32] x to produce Y[64, 32]
Matmul: 64 x 32 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x32^1  &  1.000 & 1.000 & 0.109 & 9.207
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [8, 32] x to produce Y[256, 32]
Matmul: 256 x 32 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x32^1  &  1.000 & 1.000 & 0.410 & 2.441
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [8, 32] x to produce Y[1024, 32]
Matmul: 1024 x 32 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x32^1  &  1.000 & 1.000 & 1.724 & 0.580
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [8, 32] x F_1 [8, 32] x to produce Y[1, 1024]
Matmul: 1 x 1024 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(8x32^1)_NN_d
Tuning for shape 1x64*(8x32^2)_NN_d
Tuning for shape 1x64*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x32^2  &  1.000 & 1.000 & 0.050 & 19.880
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [8, 32] x F_1 [8, 32] x to produce Y[4, 1024]
Matmul: 4 x 1024 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(8x32^1)_NN_d
Tuning for shape 4x64*(8x32^2)_NN_d
Tuning for shape 4x64*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x32^2  &  1.000 & 1.000 & 0.177 & 5.647
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [8, 32] x F_1 [8, 32] x to produce Y[16, 1024]
Matmul: 16 x 1024 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(8x32^1)_NN_d
Tuning for shape 16x64*(8x32^2)_NN_d
Tuning for shape 16x64*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x32^2  &  1.000 & 1.000 & 0.725 & 1.380
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [8, 32] x F_1 [8, 32] x to produce Y[64, 1024]
Matmul: 64 x 1024 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(8x32^1)_NN_d
Tuning for shape 64x64*(8x32^2)_NN_d
Tuning for shape 64x64*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x32^2  &  1.000 & 1.000 & 2.900 & 0.345
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [8, 32] x F_1 [8, 32] x to produce Y[256, 1024]
Matmul: 256 x 1024 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(8x32^1)_NN_d
Tuning for shape 256x64*(8x32^2)_NN_d
Tuning for shape 256x64*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x32^2  &  1.000 & 1.000 & 11.998 & 0.083
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [8, 32] x F_1 [8, 32] x to produce Y[1024, 1024]
Matmul: 1024 x 1024 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x256*(8x32^1)_NN_d
Tuning for shape 1024x64*(8x32^2)_NN_d
Tuning for shape 1024x64*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x32^2  &  1.000 & 1.000 & 45.820 & 0.022
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x to produce Y[1, 32768]
Matmul: 1 x 32768 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8192*(8x32^1)_NN_d
Tuning for shape 1x2048*(8x32^2)_NN_d
Tuning for shape 1x512*(8x32^3)_NN_d
Tuning for shape 1x2048*(8x32^1)_NN_d
Tuning for shape 1x512*(8x32^2)_NN_d
Tuning for shape 1x512*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x32^3  &  1.000 & 1.000 & 1.325 & 0.755
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x to produce Y[4, 32768]
Matmul: 4 x 32768 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8192*(8x32^1)_NN_d
Tuning for shape 4x2048*(8x32^2)_NN_d
Tuning for shape 4x512*(8x32^3)_NN_d
Tuning for shape 4x2048*(8x32^1)_NN_d
Tuning for shape 4x512*(8x32^2)_NN_d
Tuning for shape 4x512*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x32^3  &  1.000 & 1.000 & 5.013 & 0.199
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 512] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x to produce Y[16, 32768]
Matmul: 16 x 32768 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8192*(8x32^1)_NN_d
Tuning for shape 16x2048*(8x32^2)_NN_d
Tuning for shape 16x512*(8x32^3)_NN_d
Tuning for shape 16x2048*(8x32^1)_NN_d
Tuning for shape 16x512*(8x32^2)_NN_d
Tuning for shape 16x512*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x32^3  &  1.000 & 1.000 & 18.881 & 0.053
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 512] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x to produce Y[64, 32768]
Matmul: 64 x 32768 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8192*(8x32^1)_NN_d
Tuning for shape 64x2048*(8x32^2)_NN_d
Tuning for shape 64x512*(8x32^3)_NN_d
Tuning for shape 64x2048*(8x32^1)_NN_d
Tuning for shape 64x512*(8x32^2)_NN_d
Tuning for shape 64x512*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x32^3  &  1.000 & 1.000 & 77.494 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 512] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x to produce Y[256, 32768]
Matmul: 256 x 32768 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8192*(8x32^1)_NN_d
Tuning for shape 256x2048*(8x32^2)_NN_d
Tuning for shape 256x512*(8x32^3)_NN_d
Tuning for shape 256x2048*(8x32^1)_NN_d
Tuning for shape 256x512*(8x32^2)_NN_d
Tuning for shape 256x512*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x32^3  &  1.000 & 1.000 & 308.461 & 0.003
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 512] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x to produce Y[1024, 32768]
Matmul: 1024 x 32768 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8192*(8x32^1)_NN_d
Tuning for shape 1024x2048*(8x32^2)_NN_d
Tuning for shape 1024x512*(8x32^3)_NN_d
Tuning for shape 1024x2048*(8x32^1)_NN_d
Tuning for shape 1024x512*(8x32^2)_NN_d
Tuning for shape 1024x512*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x32^3  &  1.000 & 1.000 & 460.561 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x F_3 [8, 32] x to produce Y[1, 1048576]
Matmul: 1 x 1048576 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x262144*(8x32^1)_NN_d
Tuning for shape 1x65536*(8x32^2)_NN_d
Tuning for shape 1x16384*(8x32^3)_NN_d
Tuning for shape 1x4096*(8x32^4)_NN_d
Tuning for shape 1x65536*(8x32^1)_NN_d
Tuning for shape 1x16384*(8x32^2)_NN_d
Tuning for shape 1x4096*(8x32^3)_NN_d
Tuning for shape 1x16384*(8x32^1)_NN_d
Tuning for shape 1x4096*(8x32^2)_NN_d
Tuning for shape 1x4096*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x32^4  &  1.000 & 1.000 & 34.802 & 0.029
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x F_3 [8, 32] x to produce Y[4, 1048576]
Matmul: 4 x 1048576 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x262144*(8x32^1)_NN_d
Tuning for shape 4x65536*(8x32^2)_NN_d
Tuning for shape 4x16384*(8x32^3)_NN_d
Tuning for shape 4x4096*(8x32^4)_NN_d
Tuning for shape 4x65536*(8x32^1)_NN_d
Tuning for shape 4x16384*(8x32^2)_NN_d
Tuning for shape 4x4096*(8x32^3)_NN_d
Tuning for shape 4x16384*(8x32^1)_NN_d
Tuning for shape 4x4096*(8x32^2)_NN_d
Tuning for shape 4x4096*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x32^4  &  1.000 & 1.000 & 134.795 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x F_3 [8, 32] x to produce Y[16, 1048576]
Matmul: 16 x 1048576 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x262144*(8x32^1)_NN_d
Tuning for shape 16x65536*(8x32^2)_NN_d
Tuning for shape 16x16384*(8x32^3)_NN_d
Tuning for shape 16x4096*(8x32^4)_NN_d
Tuning for shape 16x65536*(8x32^1)_NN_d
Tuning for shape 16x16384*(8x32^2)_NN_d
Tuning for shape 16x4096*(8x32^3)_NN_d
Tuning for shape 16x16384*(8x32^1)_NN_d
Tuning for shape 16x4096*(8x32^2)_NN_d
Tuning for shape 16x4096*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x32^4  &  1.000 & 1.000 & 447.370 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x F_3 [8, 32] x to produce Y[64, 1048576]
Matmul: 64 x 1048576 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x262144*(8x32^1)_NN_d
Tuning for shape 64x65536*(8x32^2)_NN_d
Tuning for shape 64x16384*(8x32^3)_NN_d
Tuning for shape 64x4096*(8x32^4)_NN_d
Tuning for shape 64x65536*(8x32^1)_NN_d
Tuning for shape 64x16384*(8x32^2)_NN_d
Tuning for shape 64x4096*(8x32^3)_NN_d
Tuning for shape 64x16384*(8x32^1)_NN_d
Tuning for shape 64x4096*(8x32^2)_NN_d
Tuning for shape 64x4096*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x32^4  &  1.000 & 1.000 & 475.614 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x F_3 [8, 32] x to produce Y[256, 1048576]
Matmul: 256 x 1048576 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x262144*(8x32^1)_NN_d
Tuning for shape 256x65536*(8x32^2)_NN_d
Tuning for shape 256x16384*(8x32^3)_NN_d
Tuning for shape 256x4096*(8x32^4)_NN_d
Tuning for shape 256x65536*(8x32^1)_NN_d
Tuning for shape 256x16384*(8x32^2)_NN_d
Tuning for shape 256x4096*(8x32^3)_NN_d
Tuning for shape 256x16384*(8x32^1)_NN_d
Tuning for shape 256x4096*(8x32^2)_NN_d
Tuning for shape 256x4096*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x32^4  &  1.000 & 1.000 & 484.622 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 32768] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x F_3 [8, 32] x F_4 [8, 32] x to produce Y[1, 33554432]
Matmul: 1 x 33554432 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8388608*(8x32^1)_NN_d
Tuning for shape 1x2097152*(8x32^2)_NN_d
Tuning for shape 1x524288*(8x32^3)_NN_d
Tuning for shape 1x131072*(8x32^4)_NN_d
Tuning for shape 1x32768*(8x32^5)_NN_d
Tuning for shape 1x2097152*(8x32^1)_NN_d
Tuning for shape 1x524288*(8x32^2)_NN_d
Tuning for shape 1x131072*(8x32^3)_NN_d
Tuning for shape 1x32768*(8x32^4)_NN_d
Tuning for shape 1x524288*(8x32^1)_NN_d
Tuning for shape 1x131072*(8x32^2)_NN_d
Tuning for shape 1x32768*(8x32^3)_NN_d
Tuning for shape 1x131072*(8x32^1)_NN_d
Tuning for shape 1x32768*(8x32^2)_NN_d
Tuning for shape 1x32768*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x32^5  &  1.000 & 1.000 & 390.505 & 0.003
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 32768] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x F_3 [8, 32] x F_4 [8, 32] x to produce Y[4, 33554432]
Matmul: 4 x 33554432 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8388608*(8x32^1)_NN_d
Tuning for shape 4x2097152*(8x32^2)_NN_d
Tuning for shape 4x524288*(8x32^3)_NN_d
Tuning for shape 4x131072*(8x32^4)_NN_d
Tuning for shape 4x32768*(8x32^5)_NN_d
Tuning for shape 4x2097152*(8x32^1)_NN_d
Tuning for shape 4x524288*(8x32^2)_NN_d
Tuning for shape 4x131072*(8x32^3)_NN_d
Tuning for shape 4x32768*(8x32^4)_NN_d
Tuning for shape 4x524288*(8x32^1)_NN_d
Tuning for shape 4x131072*(8x32^2)_NN_d
Tuning for shape 4x32768*(8x32^3)_NN_d
Tuning for shape 4x131072*(8x32^1)_NN_d
Tuning for shape 4x32768*(8x32^2)_NN_d
Tuning for shape 4x32768*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x32^5  &  1.000 & 1.000 & 444.730 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 8 -q 32 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 32768] with F_0 [8, 32] x F_1 [8, 32] x F_2 [8, 32] x F_3 [8, 32] x F_4 [8, 32] x to produce Y[16, 33554432]
Matmul: 16 x 33554432 x 32768, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8388608*(8x32^1)_NN_d
Tuning for shape 16x2097152*(8x32^2)_NN_d
Tuning for shape 16x524288*(8x32^3)_NN_d
Tuning for shape 16x131072*(8x32^4)_NN_d
Tuning for shape 16x32768*(8x32^5)_NN_d
Tuning for shape 16x2097152*(8x32^1)_NN_d
Tuning for shape 16x524288*(8x32^2)_NN_d
Tuning for shape 16x131072*(8x32^3)_NN_d
Tuning for shape 16x32768*(8x32^4)_NN_d
Tuning for shape 16x524288*(8x32^1)_NN_d
Tuning for shape 16x131072*(8x32^2)_NN_d
Tuning for shape 16x32768*(8x32^3)_NN_d
Tuning for shape 16x131072*(8x32^1)_NN_d
Tuning for shape 16x32768*(8x32^2)_NN_d
Tuning for shape 16x32768*(8x32^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x32^5  &  1.000 & 1.000 & 474.251 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [8, 64] x to produce Y[1, 64]
Matmul: 1 x 64 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x64^1  &  1.000 & 1.000 & 0.004 & 253.308
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [8, 64] x to produce Y[4, 64]
Matmul: 4 x 64 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x64^1  &  1.000 & 1.000 & 0.013 & 77.306
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [8, 64] x to produce Y[16, 64]
Matmul: 16 x 64 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x64^1  &  1.000 & 1.000 & 0.053 & 18.754
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [8, 64] x to produce Y[64, 64]
Matmul: 64 x 64 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x64^1  &  1.000 & 1.000 & 0.200 & 4.999
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [8, 64] x to produce Y[256, 64]
Matmul: 256 x 64 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x64^1  &  1.000 & 1.000 & 0.859 & 1.164
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [8, 64] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x64^1  &  1.000 & 1.000 & 3.377 & 0.296
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [8, 64] x F_1 [8, 64] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(8x64^1)_NN_d
Tuning for shape 1x64*(8x64^2)_NN_d
Tuning for shape 1x64*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x64^2  &  1.000 & 1.000 & 0.173 & 5.768
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [8, 64] x F_1 [8, 64] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(8x64^1)_NN_d
Tuning for shape 4x64*(8x64^2)_NN_d
Tuning for shape 4x64*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x64^2  &  1.000 & 1.000 & 0.659 & 1.518
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [8, 64] x F_1 [8, 64] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(8x64^1)_NN_d
Tuning for shape 16x64*(8x64^2)_NN_d
Tuning for shape 16x64*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x64^2  &  1.000 & 1.000 & 2.541 & 0.394
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [8, 64] x F_1 [8, 64] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x512*(8x64^1)_NN_d
Tuning for shape 64x64*(8x64^2)_NN_d
Tuning for shape 64x64*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x64^2  &  1.000 & 1.000 & 9.939 & 0.101
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [8, 64] x F_1 [8, 64] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x512*(8x64^1)_NN_d
Tuning for shape 256x64*(8x64^2)_NN_d
Tuning for shape 256x64*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x64^2  &  1.000 & 1.000 & 36.005 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [8, 64] x F_1 [8, 64] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x512*(8x64^1)_NN_d
Tuning for shape 1024x64*(8x64^2)_NN_d
Tuning for shape 1024x64*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x64^2  &  1.000 & 1.000 & 165.738 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [8, 64] x F_1 [8, 64] x F_2 [8, 64] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32768*(8x64^1)_NN_d
Tuning for shape 1x4096*(8x64^2)_NN_d
Tuning for shape 1x512*(8x64^3)_NN_d
Tuning for shape 1x4096*(8x64^1)_NN_d
Tuning for shape 1x512*(8x64^2)_NN_d
Tuning for shape 1x512*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x64^3  &  1.000 & 1.000 & 9.235 & 0.108
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [8, 64] x F_1 [8, 64] x F_2 [8, 64] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32768*(8x64^1)_NN_d
Tuning for shape 4x4096*(8x64^2)_NN_d
Tuning for shape 4x512*(8x64^3)_NN_d
Tuning for shape 4x4096*(8x64^1)_NN_d
Tuning for shape 4x512*(8x64^2)_NN_d
Tuning for shape 4x512*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x64^3  &  1.000 & 1.000 & 32.972 & 0.030
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 512] with F_0 [8, 64] x F_1 [8, 64] x F_2 [8, 64] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32768*(8x64^1)_NN_d
Tuning for shape 16x4096*(8x64^2)_NN_d
Tuning for shape 16x512*(8x64^3)_NN_d
Tuning for shape 16x4096*(8x64^1)_NN_d
Tuning for shape 16x512*(8x64^2)_NN_d
Tuning for shape 16x512*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x64^3  &  1.000 & 1.000 & 133.752 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 512] with F_0 [8, 64] x F_1 [8, 64] x F_2 [8, 64] x to produce Y[64, 262144]
Matmul: 64 x 262144 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32768*(8x64^1)_NN_d
Tuning for shape 64x4096*(8x64^2)_NN_d
Tuning for shape 64x512*(8x64^3)_NN_d
Tuning for shape 64x4096*(8x64^1)_NN_d
Tuning for shape 64x512*(8x64^2)_NN_d
Tuning for shape 64x512*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x64^3  &  1.000 & 1.000 & 470.586 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 512] with F_0 [8, 64] x F_1 [8, 64] x F_2 [8, 64] x to produce Y[256, 262144]
Matmul: 256 x 262144 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32768*(8x64^1)_NN_d
Tuning for shape 256x4096*(8x64^2)_NN_d
Tuning for shape 256x512*(8x64^3)_NN_d
Tuning for shape 256x4096*(8x64^1)_NN_d
Tuning for shape 256x512*(8x64^2)_NN_d
Tuning for shape 256x512*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x64^3  &  1.000 & 1.000 & 492.611 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 512] with F_0 [8, 64] x F_1 [8, 64] x F_2 [8, 64] x to produce Y[1024, 262144]
Matmul: 1024 x 262144 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32768*(8x64^1)_NN_d
Tuning for shape 1024x4096*(8x64^2)_NN_d
Tuning for shape 1024x512*(8x64^3)_NN_d
Tuning for shape 1024x4096*(8x64^1)_NN_d
Tuning for shape 1024x512*(8x64^2)_NN_d
Tuning for shape 1024x512*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x64^3  &  1.000 & 1.000 & 504.580 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [8, 64] x F_1 [8, 64] x F_2 [8, 64] x F_3 [8, 64] x to produce Y[1, 16777216]
Matmul: 1 x 16777216 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x2097152*(8x64^1)_NN_d
Tuning for shape 1x262144*(8x64^2)_NN_d
Tuning for shape 1x32768*(8x64^3)_NN_d
Tuning for shape 1x4096*(8x64^4)_NN_d
Tuning for shape 1x262144*(8x64^1)_NN_d
Tuning for shape 1x32768*(8x64^2)_NN_d
Tuning for shape 1x4096*(8x64^3)_NN_d
Tuning for shape 1x32768*(8x64^1)_NN_d
Tuning for shape 1x4096*(8x64^2)_NN_d
Tuning for shape 1x4096*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x64^4  &  1.000 & 1.000 & 348.470 & 0.003
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [8, 64] x F_1 [8, 64] x F_2 [8, 64] x F_3 [8, 64] x to produce Y[4, 16777216]
Matmul: 4 x 16777216 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x2097152*(8x64^1)_NN_d
Tuning for shape 4x262144*(8x64^2)_NN_d
Tuning for shape 4x32768*(8x64^3)_NN_d
Tuning for shape 4x4096*(8x64^4)_NN_d
Tuning for shape 4x262144*(8x64^1)_NN_d
Tuning for shape 4x32768*(8x64^2)_NN_d
Tuning for shape 4x4096*(8x64^3)_NN_d
Tuning for shape 4x32768*(8x64^1)_NN_d
Tuning for shape 4x4096*(8x64^2)_NN_d
Tuning for shape 4x4096*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x64^4  &  1.000 & 1.000 & 427.658 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 8 -q 64 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [8, 64] x F_1 [8, 64] x F_2 [8, 64] x F_3 [8, 64] x to produce Y[16, 16777216]
Matmul: 16 x 16777216 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x2097152*(8x64^1)_NN_d
Tuning for shape 16x262144*(8x64^2)_NN_d
Tuning for shape 16x32768*(8x64^3)_NN_d
Tuning for shape 16x4096*(8x64^4)_NN_d
Tuning for shape 16x262144*(8x64^1)_NN_d
Tuning for shape 16x32768*(8x64^2)_NN_d
Tuning for shape 16x4096*(8x64^3)_NN_d
Tuning for shape 16x32768*(8x64^1)_NN_d
Tuning for shape 16x4096*(8x64^2)_NN_d
Tuning for shape 16x4096*(8x64^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x64^4  &  1.000 & 1.000 & 503.185 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 8] with F_0 [8, 128] x to produce Y[1, 128]
Matmul: 1 x 128 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x128^1  &  1.000 & 1.000 & 0.008 & 126.264
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 8] with F_0 [8, 128] x to produce Y[4, 128]
Matmul: 4 x 128 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x128^1  &  1.000 & 1.000 & 0.027 & 37.449
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 8] with F_0 [8, 128] x to produce Y[16, 128]
Matmul: 16 x 128 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x128^1  &  1.000 & 1.000 & 0.110 & 9.065
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 8] with F_0 [8, 128] x to produce Y[64, 128]
Matmul: 64 x 128 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x128^1  &  1.000 & 1.000 & 0.424 & 2.356
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 8] with F_0 [8, 128] x to produce Y[256, 128]
Matmul: 256 x 128 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x128^1  &  1.000 & 1.000 & 1.755 & 0.570
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 8] with F_0 [8, 128] x to produce Y[1024, 128]
Matmul: 1024 x 128 x 8, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x128^1  &  1.000 & 1.000 & 6.897 & 0.145
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 64] with F_0 [8, 128] x F_1 [8, 128] x to produce Y[1, 16384]
Matmul: 1 x 16384 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(8x128^1)_NN_d
Tuning for shape 1x64*(8x128^2)_NN_d
Tuning for shape 1x64*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x128^2  &  1.000 & 1.000 & 0.678 & 1.476
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 64] with F_0 [8, 128] x F_1 [8, 128] x to produce Y[4, 16384]
Matmul: 4 x 16384 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1024*(8x128^1)_NN_d
Tuning for shape 4x64*(8x128^2)_NN_d
Tuning for shape 4x64*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x128^2  &  1.000 & 1.000 & 2.476 & 0.404
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 64] with F_0 [8, 128] x F_1 [8, 128] x to produce Y[16, 16384]
Matmul: 16 x 16384 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1024*(8x128^1)_NN_d
Tuning for shape 16x64*(8x128^2)_NN_d
Tuning for shape 16x64*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x128^2  &  1.000 & 1.000 & 9.825 & 0.102
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 64] with F_0 [8, 128] x F_1 [8, 128] x to produce Y[64, 16384]
Matmul: 64 x 16384 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1024*(8x128^1)_NN_d
Tuning for shape 64x64*(8x128^2)_NN_d
Tuning for shape 64x64*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x128^2  &  1.000 & 1.000 & 38.252 & 0.026
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 64] with F_0 [8, 128] x F_1 [8, 128] x to produce Y[256, 16384]
Matmul: 256 x 16384 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1024*(8x128^1)_NN_d
Tuning for shape 256x64*(8x128^2)_NN_d
Tuning for shape 256x64*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x128^2  &  1.000 & 1.000 & 154.529 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 64] with F_0 [8, 128] x F_1 [8, 128] x to produce Y[1024, 16384]
Matmul: 1024 x 16384 x 64, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x1024*(8x128^1)_NN_d
Tuning for shape 1024x64*(8x128^2)_NN_d
Tuning for shape 1024x64*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_8x128^2  &  1.000 & 1.000 & 491.978 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 512] with F_0 [8, 128] x F_1 [8, 128] x F_2 [8, 128] x to produce Y[1, 2097152]
Matmul: 1 x 2097152 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x131072*(8x128^1)_NN_d
Tuning for shape 1x8192*(8x128^2)_NN_d
Tuning for shape 1x512*(8x128^3)_NN_d
Tuning for shape 1x8192*(8x128^1)_NN_d
Tuning for shape 1x512*(8x128^2)_NN_d
Tuning for shape 1x512*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x128^3  &  1.000 & 1.000 & 64.670 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 512] with F_0 [8, 128] x F_1 [8, 128] x F_2 [8, 128] x to produce Y[4, 2097152]
Matmul: 4 x 2097152 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x131072*(8x128^1)_NN_d
Tuning for shape 4x8192*(8x128^2)_NN_d
Tuning for shape 4x512*(8x128^3)_NN_d
Tuning for shape 4x8192*(8x128^1)_NN_d
Tuning for shape 4x512*(8x128^2)_NN_d
Tuning for shape 4x512*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_8x128^3  &  1.000 & 1.000 & 252.644 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 512] with F_0 [8, 128] x F_1 [8, 128] x F_2 [8, 128] x to produce Y[16, 2097152]
Matmul: 16 x 2097152 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x131072*(8x128^1)_NN_d
Tuning for shape 16x8192*(8x128^2)_NN_d
Tuning for shape 16x512*(8x128^3)_NN_d
Tuning for shape 16x8192*(8x128^1)_NN_d
Tuning for shape 16x512*(8x128^2)_NN_d
Tuning for shape 16x512*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_8x128^3  &  1.000 & 1.000 & 492.460 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 512] with F_0 [8, 128] x F_1 [8, 128] x F_2 [8, 128] x to produce Y[64, 2097152]
Matmul: 64 x 2097152 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x131072*(8x128^1)_NN_d
Tuning for shape 64x8192*(8x128^2)_NN_d
Tuning for shape 64x512*(8x128^3)_NN_d
Tuning for shape 64x8192*(8x128^1)_NN_d
Tuning for shape 64x512*(8x128^2)_NN_d
Tuning for shape 64x512*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_8x128^3  &  1.000 & 1.000 & 507.380 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 512] with F_0 [8, 128] x F_1 [8, 128] x F_2 [8, 128] x to produce Y[256, 2097152]
Matmul: 256 x 2097152 x 512, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x131072*(8x128^1)_NN_d
Tuning for shape 256x8192*(8x128^2)_NN_d
Tuning for shape 256x512*(8x128^3)_NN_d
Tuning for shape 256x8192*(8x128^1)_NN_d
Tuning for shape 256x512*(8x128^2)_NN_d
Tuning for shape 256x512*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_8x128^3  &  1.000 & 1.000 & 506.728 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 8 -q 128 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [8, 128] x F_1 [8, 128] x F_2 [8, 128] x F_3 [8, 128] x to produce Y[1, 268435456]
Matmul: 1 x 268435456 x 4096, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16777216*(8x128^1)_NN_d
Tuning for shape 1x1048576*(8x128^2)_NN_d
Tuning for shape 1x65536*(8x128^3)_NN_d
Tuning for shape 1x4096*(8x128^4)_NN_d
Tuning for shape 1x1048576*(8x128^1)_NN_d
Tuning for shape 1x65536*(8x128^2)_NN_d
Tuning for shape 1x4096*(8x128^3)_NN_d
Tuning for shape 1x65536*(8x128^1)_NN_d
Tuning for shape 1x4096*(8x128^2)_NN_d
Tuning for shape 1x4096*(8x128^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_8x128^4  &  1.000 & 1.000 & 151.623 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [16, 2] x to produce Y[1, 2]
Matmul: 1 x 2 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x2^1  &  1.000 & 1.000 & 0.000 & 4099.868
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [16, 2] x to produce Y[4, 2]
Matmul: 4 x 2 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x2^1  &  1.000 & 1.000 & 0.001 & 1245.830
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [16, 2] x to produce Y[16, 2]
Matmul: 16 x 2 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x2^1  &  1.000 & 1.000 & 0.003 & 302.447
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [16, 2] x to produce Y[64, 2]
Matmul: 64 x 2 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x2^1  &  1.000 & 1.000 & 0.014 & 73.510
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [16, 2] x to produce Y[256, 2]
Matmul: 256 x 2 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x2^1  &  1.000 & 1.000 & 0.055 & 18.231
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [16, 2] x to produce Y[1024, 2]
Matmul: 1024 x 2 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x2^1  &  1.000 & 1.000 & 0.214 & 4.663
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [16, 2] x F_1 [16, 2] x to produce Y[1, 4]
Matmul: 1 x 4 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x32*(16x2^1)_NN_d
Tuning for shape 1x256*(16x2^2)_NN_d
Tuning for shape 1x256*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x2^2  &  1.000 & 1.000 & 0.003 & 352.909
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [16, 2] x F_1 [16, 2] x to produce Y[4, 4]
Matmul: 4 x 4 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x32*(16x2^1)_NN_d
Tuning for shape 4x256*(16x2^2)_NN_d
Tuning for shape 4x256*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x2^2  &  1.000 & 1.000 & 0.010 & 95.983
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [16, 2] x F_1 [16, 2] x to produce Y[16, 4]
Matmul: 16 x 4 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x32*(16x2^1)_NN_d
Tuning for shape 16x256*(16x2^2)_NN_d
Tuning for shape 16x256*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x2^2  &  1.000 & 1.000 & 0.040 & 24.720
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [16, 2] x F_1 [16, 2] x to produce Y[64, 4]
Matmul: 64 x 4 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x32*(16x2^1)_NN_d
Tuning for shape 64x256*(16x2^2)_NN_d
Tuning for shape 64x256*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x2^2  &  1.000 & 1.000 & 0.164 & 6.111
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [16, 2] x F_1 [16, 2] x to produce Y[256, 4]
Matmul: 256 x 4 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x32*(16x2^1)_NN_d
Tuning for shape 256x256*(16x2^2)_NN_d
Tuning for shape 256x256*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x2^2  &  1.000 & 1.000 & 0.645 & 1.549
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 256] with F_0 [16, 2] x F_1 [16, 2] x to produce Y[1024, 4]
Matmul: 1024 x 4 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x32*(16x2^1)_NN_d
Tuning for shape 1024x256*(16x2^2)_NN_d
Tuning for shape 1024x256*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x2^2  &  1.000 & 1.000 & 2.513 & 0.398
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x to produce Y[1, 8]
Matmul: 1 x 8 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x64*(16x2^1)_NN_d
Tuning for shape 1x512*(16x2^2)_NN_d
Tuning for shape 1x4096*(16x2^3)_NN_d
Tuning for shape 1x512*(16x2^1)_NN_d
Tuning for shape 1x4096*(16x2^2)_NN_d
Tuning for shape 1x4096*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x2^3  &  1.000 & 1.000 & 0.037 & 27.161
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x to produce Y[4, 8]
Matmul: 4 x 8 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x64*(16x2^1)_NN_d
Tuning for shape 4x512*(16x2^2)_NN_d
Tuning for shape 4x4096*(16x2^3)_NN_d
Tuning for shape 4x512*(16x2^1)_NN_d
Tuning for shape 4x4096*(16x2^2)_NN_d
Tuning for shape 4x4096*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x2^3  &  1.000 & 1.000 & 0.133 & 7.529
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x to produce Y[16, 8]
Matmul: 16 x 8 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x64*(16x2^1)_NN_d
Tuning for shape 16x512*(16x2^2)_NN_d
Tuning for shape 16x4096*(16x2^3)_NN_d
Tuning for shape 16x512*(16x2^1)_NN_d
Tuning for shape 16x4096*(16x2^2)_NN_d
Tuning for shape 16x4096*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x2^3  &  1.000 & 1.000 & 0.555 & 1.803
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x to produce Y[64, 8]
Matmul: 64 x 8 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x64*(16x2^1)_NN_d
Tuning for shape 64x512*(16x2^2)_NN_d
Tuning for shape 64x4096*(16x2^3)_NN_d
Tuning for shape 64x512*(16x2^1)_NN_d
Tuning for shape 64x4096*(16x2^2)_NN_d
Tuning for shape 64x4096*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x2^3  &  1.000 & 1.000 & 2.190 & 0.457
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x to produce Y[256, 8]
Matmul: 256 x 8 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x64*(16x2^1)_NN_d
Tuning for shape 256x512*(16x2^2)_NN_d
Tuning for shape 256x4096*(16x2^3)_NN_d
Tuning for shape 256x512*(16x2^1)_NN_d
Tuning for shape 256x4096*(16x2^2)_NN_d
Tuning for shape 256x4096*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x2^3  &  1.000 & 1.000 & 8.581 & 0.117
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x to produce Y[1024, 8]
Matmul: 1024 x 8 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x64*(16x2^1)_NN_d
Tuning for shape 1024x512*(16x2^2)_NN_d
Tuning for shape 1024x4096*(16x2^3)_NN_d
Tuning for shape 1024x512*(16x2^1)_NN_d
Tuning for shape 1024x4096*(16x2^2)_NN_d
Tuning for shape 1024x4096*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x2^3  &  1.000 & 1.000 & 35.288 & 0.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 65536] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x to produce Y[1, 16]
Matmul: 1 x 16 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(16x2^1)_NN_d
Tuning for shape 1x1024*(16x2^2)_NN_d
Tuning for shape 1x8192*(16x2^3)_NN_d
Tuning for shape 1x65536*(16x2^4)_NN_d
Tuning for shape 1x1024*(16x2^1)_NN_d
Tuning for shape 1x8192*(16x2^2)_NN_d
Tuning for shape 1x65536*(16x2^3)_NN_d
Tuning for shape 1x8192*(16x2^1)_NN_d
Tuning for shape 1x65536*(16x2^2)_NN_d
Tuning for shape 1x65536*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x2^4  &  1.000 & 1.000 & 0.493 & 2.028
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 65536] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x to produce Y[4, 16]
Matmul: 4 x 16 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(16x2^1)_NN_d
Tuning for shape 4x1024*(16x2^2)_NN_d
Tuning for shape 4x8192*(16x2^3)_NN_d
Tuning for shape 4x65536*(16x2^4)_NN_d
Tuning for shape 4x1024*(16x2^1)_NN_d
Tuning for shape 4x8192*(16x2^2)_NN_d
Tuning for shape 4x65536*(16x2^3)_NN_d
Tuning for shape 4x8192*(16x2^1)_NN_d
Tuning for shape 4x65536*(16x2^2)_NN_d
Tuning for shape 4x65536*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x2^4  &  1.000 & 1.000 & 1.859 & 0.538
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 65536] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x to produce Y[16, 16]
Matmul: 16 x 16 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(16x2^1)_NN_d
Tuning for shape 16x1024*(16x2^2)_NN_d
Tuning for shape 16x8192*(16x2^3)_NN_d
Tuning for shape 16x65536*(16x2^4)_NN_d
Tuning for shape 16x1024*(16x2^1)_NN_d
Tuning for shape 16x8192*(16x2^2)_NN_d
Tuning for shape 16x65536*(16x2^3)_NN_d
Tuning for shape 16x8192*(16x2^1)_NN_d
Tuning for shape 16x65536*(16x2^2)_NN_d
Tuning for shape 16x65536*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x2^4  &  1.000 & 1.000 & 7.346 & 0.136
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 65536] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x to produce Y[64, 16]
Matmul: 64 x 16 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(16x2^1)_NN_d
Tuning for shape 64x1024*(16x2^2)_NN_d
Tuning for shape 64x8192*(16x2^3)_NN_d
Tuning for shape 64x65536*(16x2^4)_NN_d
Tuning for shape 64x1024*(16x2^1)_NN_d
Tuning for shape 64x8192*(16x2^2)_NN_d
Tuning for shape 64x65536*(16x2^3)_NN_d
Tuning for shape 64x8192*(16x2^1)_NN_d
Tuning for shape 64x65536*(16x2^2)_NN_d
Tuning for shape 64x65536*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x2^4  &  1.000 & 1.000 & 26.859 & 0.037
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 65536] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x to produce Y[256, 16]
Matmul: 256 x 16 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(16x2^1)_NN_d
Tuning for shape 256x1024*(16x2^2)_NN_d
Tuning for shape 256x8192*(16x2^3)_NN_d
Tuning for shape 256x65536*(16x2^4)_NN_d
Tuning for shape 256x1024*(16x2^1)_NN_d
Tuning for shape 256x8192*(16x2^2)_NN_d
Tuning for shape 256x65536*(16x2^3)_NN_d
Tuning for shape 256x8192*(16x2^1)_NN_d
Tuning for shape 256x65536*(16x2^2)_NN_d
Tuning for shape 256x65536*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x2^4  &  1.000 & 1.000 & 37.235 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 65536] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(16x2^1)_NN_d
Tuning for shape 1024x1024*(16x2^2)_NN_d
Tuning for shape 1024x8192*(16x2^3)_NN_d
Tuning for shape 1024x65536*(16x2^4)_NN_d
Tuning for shape 1024x1024*(16x2^1)_NN_d
Tuning for shape 1024x8192*(16x2^2)_NN_d
Tuning for shape 1024x65536*(16x2^3)_NN_d
Tuning for shape 1024x8192*(16x2^1)_NN_d
Tuning for shape 1024x65536*(16x2^2)_NN_d
Tuning for shape 1024x65536*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x2^4  &  1.000 & 1.000 & 36.473 & 0.027
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1048576] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x F_4 [16, 2] x to produce Y[1, 32]
Matmul: 1 x 32 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(16x2^1)_NN_d
Tuning for shape 1x2048*(16x2^2)_NN_d
Tuning for shape 1x16384*(16x2^3)_NN_d
Tuning for shape 1x131072*(16x2^4)_NN_d
Tuning for shape 1x1048576*(16x2^5)_NN_d
Tuning for shape 1x2048*(16x2^1)_NN_d
Tuning for shape 1x16384*(16x2^2)_NN_d
Tuning for shape 1x131072*(16x2^3)_NN_d
Tuning for shape 1x1048576*(16x2^4)_NN_d
Tuning for shape 1x16384*(16x2^1)_NN_d
Tuning for shape 1x131072*(16x2^2)_NN_d
Tuning for shape 1x1048576*(16x2^3)_NN_d
Tuning for shape 1x131072*(16x2^1)_NN_d
Tuning for shape 1x1048576*(16x2^2)_NN_d
Tuning for shape 1x1048576*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x2^5  &  1.000 & 1.000 & 6.645 & 0.150
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1048576] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x F_4 [16, 2] x to produce Y[4, 32]
Matmul: 4 x 32 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(16x2^1)_NN_d
Tuning for shape 4x2048*(16x2^2)_NN_d
Tuning for shape 4x16384*(16x2^3)_NN_d
Tuning for shape 4x131072*(16x2^4)_NN_d
Tuning for shape 4x1048576*(16x2^5)_NN_d
Tuning for shape 4x2048*(16x2^1)_NN_d
Tuning for shape 4x16384*(16x2^2)_NN_d
Tuning for shape 4x131072*(16x2^3)_NN_d
Tuning for shape 4x1048576*(16x2^4)_NN_d
Tuning for shape 4x16384*(16x2^1)_NN_d
Tuning for shape 4x131072*(16x2^2)_NN_d
Tuning for shape 4x1048576*(16x2^3)_NN_d
Tuning for shape 4x131072*(16x2^1)_NN_d
Tuning for shape 4x1048576*(16x2^2)_NN_d
Tuning for shape 4x1048576*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x2^5  &  1.000 & 1.000 & 24.715 & 0.040
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1048576] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x F_4 [16, 2] x to produce Y[16, 32]
Matmul: 16 x 32 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(16x2^1)_NN_d
Tuning for shape 16x2048*(16x2^2)_NN_d
Tuning for shape 16x16384*(16x2^3)_NN_d
Tuning for shape 16x131072*(16x2^4)_NN_d
Tuning for shape 16x1048576*(16x2^5)_NN_d
Tuning for shape 16x2048*(16x2^1)_NN_d
Tuning for shape 16x16384*(16x2^2)_NN_d
Tuning for shape 16x131072*(16x2^3)_NN_d
Tuning for shape 16x1048576*(16x2^4)_NN_d
Tuning for shape 16x16384*(16x2^1)_NN_d
Tuning for shape 16x131072*(16x2^2)_NN_d
Tuning for shape 16x1048576*(16x2^3)_NN_d
Tuning for shape 16x131072*(16x2^1)_NN_d
Tuning for shape 16x1048576*(16x2^2)_NN_d
Tuning for shape 16x1048576*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x2^5  &  1.000 & 1.000 & 78.943 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1048576] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x F_4 [16, 2] x to produce Y[64, 32]
Matmul: 64 x 32 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(16x2^1)_NN_d
Tuning for shape 64x2048*(16x2^2)_NN_d
Tuning for shape 64x16384*(16x2^3)_NN_d
Tuning for shape 64x131072*(16x2^4)_NN_d
Tuning for shape 64x1048576*(16x2^5)_NN_d
Tuning for shape 64x2048*(16x2^1)_NN_d
Tuning for shape 64x16384*(16x2^2)_NN_d
Tuning for shape 64x131072*(16x2^3)_NN_d
Tuning for shape 64x1048576*(16x2^4)_NN_d
Tuning for shape 64x16384*(16x2^1)_NN_d
Tuning for shape 64x131072*(16x2^2)_NN_d
Tuning for shape 64x1048576*(16x2^3)_NN_d
Tuning for shape 64x131072*(16x2^1)_NN_d
Tuning for shape 64x1048576*(16x2^2)_NN_d
Tuning for shape 64x1048576*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x2^5  &  1.000 & 1.000 & 65.684 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1048576] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x F_4 [16, 2] x to produce Y[256, 32]
Matmul: 256 x 32 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(16x2^1)_NN_d
Tuning for shape 256x2048*(16x2^2)_NN_d
Tuning for shape 256x16384*(16x2^3)_NN_d
Tuning for shape 256x131072*(16x2^4)_NN_d
Tuning for shape 256x1048576*(16x2^5)_NN_d
Tuning for shape 256x2048*(16x2^1)_NN_d
Tuning for shape 256x16384*(16x2^2)_NN_d
Tuning for shape 256x131072*(16x2^3)_NN_d
Tuning for shape 256x1048576*(16x2^4)_NN_d
Tuning for shape 256x16384*(16x2^1)_NN_d
Tuning for shape 256x131072*(16x2^2)_NN_d
Tuning for shape 256x1048576*(16x2^3)_NN_d
Tuning for shape 256x131072*(16x2^1)_NN_d
Tuning for shape 256x1048576*(16x2^2)_NN_d
Tuning for shape 256x1048576*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x2^5  &  1.000 & 1.000 & 31.181 & 0.032
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16777216] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x F_4 [16, 2] x F_5 [16, 2] x to produce Y[1, 64]
Matmul: 1 x 64 x 16777216, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x512*(16x2^1)_NN_d
Tuning for shape 1x4096*(16x2^2)_NN_d
Tuning for shape 1x32768*(16x2^3)_NN_d
Tuning for shape 1x262144*(16x2^4)_NN_d
Tuning for shape 1x2097152*(16x2^5)_NN_d
Tuning for shape 1x16777216*(16x2^6)_NN_d
Tuning for shape 1x4096*(16x2^1)_NN_d
Tuning for shape 1x32768*(16x2^2)_NN_d
Tuning for shape 1x262144*(16x2^3)_NN_d
Tuning for shape 1x2097152*(16x2^4)_NN_d
Tuning for shape 1x16777216*(16x2^5)_NN_d
Tuning for shape 1x32768*(16x2^1)_NN_d
Tuning for shape 1x262144*(16x2^2)_NN_d
Tuning for shape 1x2097152*(16x2^3)_NN_d
Tuning for shape 1x16777216*(16x2^4)_NN_d
Tuning for shape 1x262144*(16x2^1)_NN_d
Tuning for shape 1x2097152*(16x2^2)_NN_d
Tuning for shape 1x16777216*(16x2^3)_NN_d
Tuning for shape 1x2097152*(16x2^1)_NN_d
Tuning for shape 1x16777216*(16x2^2)_NN_d
Tuning for shape 1x16777216*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x2^6  &  1.000 & 1.000 & 54.527 & 0.018
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16777216] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x F_4 [16, 2] x F_5 [16, 2] x to produce Y[4, 64]
Matmul: 4 x 64 x 16777216, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x512*(16x2^1)_NN_d
Tuning for shape 4x4096*(16x2^2)_NN_d
Tuning for shape 4x32768*(16x2^3)_NN_d
Tuning for shape 4x262144*(16x2^4)_NN_d
Tuning for shape 4x2097152*(16x2^5)_NN_d
Tuning for shape 4x16777216*(16x2^6)_NN_d
Tuning for shape 4x4096*(16x2^1)_NN_d
Tuning for shape 4x32768*(16x2^2)_NN_d
Tuning for shape 4x262144*(16x2^3)_NN_d
Tuning for shape 4x2097152*(16x2^4)_NN_d
Tuning for shape 4x16777216*(16x2^5)_NN_d
Tuning for shape 4x32768*(16x2^1)_NN_d
Tuning for shape 4x262144*(16x2^2)_NN_d
Tuning for shape 4x2097152*(16x2^3)_NN_d
Tuning for shape 4x16777216*(16x2^4)_NN_d
Tuning for shape 4x262144*(16x2^1)_NN_d
Tuning for shape 4x2097152*(16x2^2)_NN_d
Tuning for shape 4x16777216*(16x2^3)_NN_d
Tuning for shape 4x2097152*(16x2^1)_NN_d
Tuning for shape 4x16777216*(16x2^2)_NN_d
Tuning for shape 4x16777216*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x2^6  &  1.000 & 1.000 & 83.190 & 0.012
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16777216] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x F_4 [16, 2] x F_5 [16, 2] x to produce Y[16, 64]
Matmul: 16 x 64 x 16777216, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x512*(16x2^1)_NN_d
Tuning for shape 16x4096*(16x2^2)_NN_d
Tuning for shape 16x32768*(16x2^3)_NN_d
Tuning for shape 16x262144*(16x2^4)_NN_d
Tuning for shape 16x2097152*(16x2^5)_NN_d
Tuning for shape 16x16777216*(16x2^6)_NN_d
Tuning for shape 16x4096*(16x2^1)_NN_d
Tuning for shape 16x32768*(16x2^2)_NN_d
Tuning for shape 16x262144*(16x2^3)_NN_d
Tuning for shape 16x2097152*(16x2^4)_NN_d
Tuning for shape 16x16777216*(16x2^5)_NN_d
Tuning for shape 16x32768*(16x2^1)_NN_d
Tuning for shape 16x262144*(16x2^2)_NN_d
Tuning for shape 16x2097152*(16x2^3)_NN_d
Tuning for shape 16x16777216*(16x2^4)_NN_d
Tuning for shape 16x262144*(16x2^1)_NN_d
Tuning for shape 16x2097152*(16x2^2)_NN_d
Tuning for shape 16x16777216*(16x2^3)_NN_d
Tuning for shape 16x2097152*(16x2^1)_NN_d
Tuning for shape 16x16777216*(16x2^2)_NN_d
Tuning for shape 16x16777216*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x2^6  &  1.000 & 1.000 & 83.434 & 0.012
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 16 -q 2 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 268435456] with F_0 [16, 2] x F_1 [16, 2] x F_2 [16, 2] x F_3 [16, 2] x F_4 [16, 2] x F_5 [16, 2] x F_6 [16, 2] x to produce Y[1, 128]
Matmul: 1 x 128 x 268435456, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(16x2^1)_NN_d
Tuning for shape 1x8192*(16x2^2)_NN_d
Tuning for shape 1x65536*(16x2^3)_NN_d
Tuning for shape 1x524288*(16x2^4)_NN_d
Tuning for shape 1x4194304*(16x2^5)_NN_d
Tuning for shape 1x33554432*(16x2^6)_NN_d
Tuning for shape 1x268435456*(16x2^7)_NN_d
Tuning for shape 1x8192*(16x2^1)_NN_d
Tuning for shape 1x65536*(16x2^2)_NN_d
Tuning for shape 1x524288*(16x2^3)_NN_d
Tuning for shape 1x4194304*(16x2^4)_NN_d
Tuning for shape 1x33554432*(16x2^5)_NN_d
Tuning for shape 1x268435456*(16x2^6)_NN_d
Tuning for shape 1x65536*(16x2^1)_NN_d
Tuning for shape 1x524288*(16x2^2)_NN_d
Tuning for shape 1x4194304*(16x2^3)_NN_d
Tuning for shape 1x33554432*(16x2^4)_NN_d
Tuning for shape 1x268435456*(16x2^5)_NN_d
Tuning for shape 1x524288*(16x2^1)_NN_d
Tuning for shape 1x4194304*(16x2^2)_NN_d
Tuning for shape 1x33554432*(16x2^3)_NN_d
Tuning for shape 1x268435456*(16x2^4)_NN_d
Tuning for shape 1x4194304*(16x2^1)_NN_d
Tuning for shape 1x33554432*(16x2^2)_NN_d
Tuning for shape 1x268435456*(16x2^3)_NN_d
Tuning for shape 1x33554432*(16x2^1)_NN_d
Tuning for shape 1x268435456*(16x2^2)_NN_d
Tuning for shape 1x268435456*(16x2^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x2^7  &  1.000 & 1.000 & 132.058 & 0.008
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [16, 4] x to produce Y[1, 4]
Matmul: 1 x 4 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x4^1  &  1.000 & 1.000 & 0.000 & 2113.357
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [16, 4] x to produce Y[4, 4]
Matmul: 4 x 4 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x4^1  &  1.000 & 1.000 & 0.002 & 600.354
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [16, 4] x to produce Y[16, 4]
Matmul: 16 x 4 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x4^1  &  1.000 & 1.000 & 0.007 & 146.544
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [16, 4] x to produce Y[64, 4]
Matmul: 64 x 4 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x4^1  &  1.000 & 1.000 & 0.026 & 37.813
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [16, 4] x to produce Y[256, 4]
Matmul: 256 x 4 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x4^1  &  1.000 & 1.000 & 0.103 & 9.685
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [16, 4] x to produce Y[1024, 4]
Matmul: 1024 x 4 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x4^1  &  1.000 & 1.000 & 0.433 & 2.307
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [16, 4] x F_1 [16, 4] x to produce Y[1, 16]
Matmul: 1 x 16 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x64*(16x4^1)_NN_d
Tuning for shape 1x256*(16x4^2)_NN_d
Tuning for shape 1x256*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x4^2  &  1.000 & 1.000 & 0.006 & 162.437
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [16, 4] x F_1 [16, 4] x to produce Y[4, 16]
Matmul: 4 x 16 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x64*(16x4^1)_NN_d
Tuning for shape 4x256*(16x4^2)_NN_d
Tuning for shape 4x256*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x4^2  &  1.000 & 1.000 & 0.023 & 43.820
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [16, 4] x F_1 [16, 4] x to produce Y[16, 16]
Matmul: 16 x 16 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x64*(16x4^1)_NN_d
Tuning for shape 16x256*(16x4^2)_NN_d
Tuning for shape 16x256*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x4^2  &  1.000 & 1.000 & 0.090 & 11.075
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [16, 4] x F_1 [16, 4] x to produce Y[64, 16]
Matmul: 64 x 16 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x64*(16x4^1)_NN_d
Tuning for shape 64x256*(16x4^2)_NN_d
Tuning for shape 64x256*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x4^2  &  1.000 & 1.000 & 0.373 & 2.681
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [16, 4] x F_1 [16, 4] x to produce Y[256, 16]
Matmul: 256 x 16 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x64*(16x4^1)_NN_d
Tuning for shape 256x256*(16x4^2)_NN_d
Tuning for shape 256x256*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x4^2  &  1.000 & 1.000 & 1.429 & 0.700
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 256] with F_0 [16, 4] x F_1 [16, 4] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x64*(16x4^1)_NN_d
Tuning for shape 1024x256*(16x4^2)_NN_d
Tuning for shape 1024x256*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x4^2  &  1.000 & 1.000 & 5.766 & 0.173
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x to produce Y[1, 64]
Matmul: 1 x 64 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(16x4^1)_NN_d
Tuning for shape 1x1024*(16x4^2)_NN_d
Tuning for shape 1x4096*(16x4^3)_NN_d
Tuning for shape 1x1024*(16x4^1)_NN_d
Tuning for shape 1x4096*(16x4^2)_NN_d
Tuning for shape 1x4096*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x4^3  &  1.000 & 1.000 & 0.081 & 12.315
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x to produce Y[4, 64]
Matmul: 4 x 64 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x256*(16x4^1)_NN_d
Tuning for shape 4x1024*(16x4^2)_NN_d
Tuning for shape 4x4096*(16x4^3)_NN_d
Tuning for shape 4x1024*(16x4^1)_NN_d
Tuning for shape 4x4096*(16x4^2)_NN_d
Tuning for shape 4x4096*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x4^3  &  1.000 & 1.000 & 0.309 & 3.234
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x to produce Y[16, 64]
Matmul: 16 x 64 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x256*(16x4^1)_NN_d
Tuning for shape 16x1024*(16x4^2)_NN_d
Tuning for shape 16x4096*(16x4^3)_NN_d
Tuning for shape 16x1024*(16x4^1)_NN_d
Tuning for shape 16x4096*(16x4^2)_NN_d
Tuning for shape 16x4096*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x4^3  &  1.000 & 1.000 & 1.251 & 0.800
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x to produce Y[64, 64]
Matmul: 64 x 64 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x256*(16x4^1)_NN_d
Tuning for shape 64x1024*(16x4^2)_NN_d
Tuning for shape 64x4096*(16x4^3)_NN_d
Tuning for shape 64x1024*(16x4^1)_NN_d
Tuning for shape 64x4096*(16x4^2)_NN_d
Tuning for shape 64x4096*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x4^3  &  1.000 & 1.000 & 4.924 & 0.203
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x to produce Y[256, 64]
Matmul: 256 x 64 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x256*(16x4^1)_NN_d
Tuning for shape 256x1024*(16x4^2)_NN_d
Tuning for shape 256x4096*(16x4^3)_NN_d
Tuning for shape 256x1024*(16x4^1)_NN_d
Tuning for shape 256x4096*(16x4^2)_NN_d
Tuning for shape 256x4096*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x4^3  &  1.000 & 1.000 & 20.269 & 0.049
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x256*(16x4^1)_NN_d
Tuning for shape 1024x1024*(16x4^2)_NN_d
Tuning for shape 1024x4096*(16x4^3)_NN_d
Tuning for shape 1024x1024*(16x4^1)_NN_d
Tuning for shape 1024x4096*(16x4^2)_NN_d
Tuning for shape 1024x4096*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x4^3  &  1.000 & 1.000 & 79.059 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 65536] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x to produce Y[1, 256]
Matmul: 1 x 256 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(16x4^1)_NN_d
Tuning for shape 1x4096*(16x4^2)_NN_d
Tuning for shape 1x16384*(16x4^3)_NN_d
Tuning for shape 1x65536*(16x4^4)_NN_d
Tuning for shape 1x4096*(16x4^1)_NN_d
Tuning for shape 1x16384*(16x4^2)_NN_d
Tuning for shape 1x65536*(16x4^3)_NN_d
Tuning for shape 1x16384*(16x4^1)_NN_d
Tuning for shape 1x65536*(16x4^2)_NN_d
Tuning for shape 1x65536*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x4^4  &  1.000 & 1.000 & 1.114 & 0.897
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 65536] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x to produce Y[4, 256]
Matmul: 4 x 256 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1024*(16x4^1)_NN_d
Tuning for shape 4x4096*(16x4^2)_NN_d
Tuning for shape 4x16384*(16x4^3)_NN_d
Tuning for shape 4x65536*(16x4^4)_NN_d
Tuning for shape 4x4096*(16x4^1)_NN_d
Tuning for shape 4x16384*(16x4^2)_NN_d
Tuning for shape 4x65536*(16x4^3)_NN_d
Tuning for shape 4x16384*(16x4^1)_NN_d
Tuning for shape 4x65536*(16x4^2)_NN_d
Tuning for shape 4x65536*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x4^4  &  1.000 & 1.000 & 4.335 & 0.231
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 65536] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x to produce Y[16, 256]
Matmul: 16 x 256 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1024*(16x4^1)_NN_d
Tuning for shape 16x4096*(16x4^2)_NN_d
Tuning for shape 16x16384*(16x4^3)_NN_d
Tuning for shape 16x65536*(16x4^4)_NN_d
Tuning for shape 16x4096*(16x4^1)_NN_d
Tuning for shape 16x16384*(16x4^2)_NN_d
Tuning for shape 16x65536*(16x4^3)_NN_d
Tuning for shape 16x16384*(16x4^1)_NN_d
Tuning for shape 16x65536*(16x4^2)_NN_d
Tuning for shape 16x65536*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x4^4  &  1.000 & 1.000 & 17.098 & 0.058
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 65536] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x to produce Y[64, 256]
Matmul: 64 x 256 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1024*(16x4^1)_NN_d
Tuning for shape 64x4096*(16x4^2)_NN_d
Tuning for shape 64x16384*(16x4^3)_NN_d
Tuning for shape 64x65536*(16x4^4)_NN_d
Tuning for shape 64x4096*(16x4^1)_NN_d
Tuning for shape 64x16384*(16x4^2)_NN_d
Tuning for shape 64x65536*(16x4^3)_NN_d
Tuning for shape 64x16384*(16x4^1)_NN_d
Tuning for shape 64x65536*(16x4^2)_NN_d
Tuning for shape 64x65536*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x4^4  &  1.000 & 1.000 & 67.828 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 65536] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x to produce Y[256, 256]
Matmul: 256 x 256 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1024*(16x4^1)_NN_d
Tuning for shape 256x4096*(16x4^2)_NN_d
Tuning for shape 256x16384*(16x4^3)_NN_d
Tuning for shape 256x65536*(16x4^4)_NN_d
Tuning for shape 256x4096*(16x4^1)_NN_d
Tuning for shape 256x16384*(16x4^2)_NN_d
Tuning for shape 256x65536*(16x4^3)_NN_d
Tuning for shape 256x16384*(16x4^1)_NN_d
Tuning for shape 256x65536*(16x4^2)_NN_d
Tuning for shape 256x65536*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x4^4  &  1.000 & 1.000 & 80.569 & 0.012
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 65536] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x to produce Y[1024, 256]
Matmul: 1024 x 256 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x1024*(16x4^1)_NN_d
Tuning for shape 1024x4096*(16x4^2)_NN_d
Tuning for shape 1024x16384*(16x4^3)_NN_d
Tuning for shape 1024x65536*(16x4^4)_NN_d
Tuning for shape 1024x4096*(16x4^1)_NN_d
Tuning for shape 1024x16384*(16x4^2)_NN_d
Tuning for shape 1024x65536*(16x4^3)_NN_d
Tuning for shape 1024x16384*(16x4^1)_NN_d
Tuning for shape 1024x65536*(16x4^2)_NN_d
Tuning for shape 1024x65536*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x4^4  &  1.000 & 1.000 & 79.056 & 0.013
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1048576] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x F_4 [16, 4] x to produce Y[1, 1024]
Matmul: 1 x 1024 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4096*(16x4^1)_NN_d
Tuning for shape 1x16384*(16x4^2)_NN_d
Tuning for shape 1x65536*(16x4^3)_NN_d
Tuning for shape 1x262144*(16x4^4)_NN_d
Tuning for shape 1x1048576*(16x4^5)_NN_d
Tuning for shape 1x16384*(16x4^1)_NN_d
Tuning for shape 1x65536*(16x4^2)_NN_d
Tuning for shape 1x262144*(16x4^3)_NN_d
Tuning for shape 1x1048576*(16x4^4)_NN_d
Tuning for shape 1x65536*(16x4^1)_NN_d
Tuning for shape 1x262144*(16x4^2)_NN_d
Tuning for shape 1x1048576*(16x4^3)_NN_d
Tuning for shape 1x262144*(16x4^1)_NN_d
Tuning for shape 1x1048576*(16x4^2)_NN_d
Tuning for shape 1x1048576*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x4^5  &  1.000 & 1.000 & 15.619 & 0.064
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1048576] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x F_4 [16, 4] x to produce Y[4, 1024]
Matmul: 4 x 1024 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x4096*(16x4^1)_NN_d
Tuning for shape 4x16384*(16x4^2)_NN_d
Tuning for shape 4x65536*(16x4^3)_NN_d
Tuning for shape 4x262144*(16x4^4)_NN_d
Tuning for shape 4x1048576*(16x4^5)_NN_d
Tuning for shape 4x16384*(16x4^1)_NN_d
Tuning for shape 4x65536*(16x4^2)_NN_d
Tuning for shape 4x262144*(16x4^3)_NN_d
Tuning for shape 4x1048576*(16x4^4)_NN_d
Tuning for shape 4x65536*(16x4^1)_NN_d
Tuning for shape 4x262144*(16x4^2)_NN_d
Tuning for shape 4x1048576*(16x4^3)_NN_d
Tuning for shape 4x262144*(16x4^1)_NN_d
Tuning for shape 4x1048576*(16x4^2)_NN_d
Tuning for shape 4x1048576*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x4^5  &  1.000 & 1.000 & 58.808 & 0.017
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1048576] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x F_4 [16, 4] x to produce Y[16, 1024]
Matmul: 16 x 1024 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x4096*(16x4^1)_NN_d
Tuning for shape 16x16384*(16x4^2)_NN_d
Tuning for shape 16x65536*(16x4^3)_NN_d
Tuning for shape 16x262144*(16x4^4)_NN_d
Tuning for shape 16x1048576*(16x4^5)_NN_d
Tuning for shape 16x16384*(16x4^1)_NN_d
Tuning for shape 16x65536*(16x4^2)_NN_d
Tuning for shape 16x262144*(16x4^3)_NN_d
Tuning for shape 16x1048576*(16x4^4)_NN_d
Tuning for shape 16x65536*(16x4^1)_NN_d
Tuning for shape 16x262144*(16x4^2)_NN_d
Tuning for shape 16x1048576*(16x4^3)_NN_d
Tuning for shape 16x262144*(16x4^1)_NN_d
Tuning for shape 16x1048576*(16x4^2)_NN_d
Tuning for shape 16x1048576*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x4^5  &  1.000 & 1.000 & 158.845 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1048576] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x F_4 [16, 4] x to produce Y[64, 1024]
Matmul: 64 x 1024 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x4096*(16x4^1)_NN_d
Tuning for shape 64x16384*(16x4^2)_NN_d
Tuning for shape 64x65536*(16x4^3)_NN_d
Tuning for shape 64x262144*(16x4^4)_NN_d
Tuning for shape 64x1048576*(16x4^5)_NN_d
Tuning for shape 64x16384*(16x4^1)_NN_d
Tuning for shape 64x65536*(16x4^2)_NN_d
Tuning for shape 64x262144*(16x4^3)_NN_d
Tuning for shape 64x1048576*(16x4^4)_NN_d
Tuning for shape 64x65536*(16x4^1)_NN_d
Tuning for shape 64x262144*(16x4^2)_NN_d
Tuning for shape 64x1048576*(16x4^3)_NN_d
Tuning for shape 64x262144*(16x4^1)_NN_d
Tuning for shape 64x1048576*(16x4^2)_NN_d
Tuning for shape 64x1048576*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x4^5  &  1.000 & 1.000 & 139.238 & 0.007
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1048576] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x F_4 [16, 4] x to produce Y[256, 1024]
Matmul: 256 x 1024 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x4096*(16x4^1)_NN_d
Tuning for shape 256x16384*(16x4^2)_NN_d
Tuning for shape 256x65536*(16x4^3)_NN_d
Tuning for shape 256x262144*(16x4^4)_NN_d
Tuning for shape 256x1048576*(16x4^5)_NN_d
Tuning for shape 256x16384*(16x4^1)_NN_d
Tuning for shape 256x65536*(16x4^2)_NN_d
Tuning for shape 256x262144*(16x4^3)_NN_d
Tuning for shape 256x1048576*(16x4^4)_NN_d
Tuning for shape 256x65536*(16x4^1)_NN_d
Tuning for shape 256x262144*(16x4^2)_NN_d
Tuning for shape 256x1048576*(16x4^3)_NN_d
Tuning for shape 256x262144*(16x4^1)_NN_d
Tuning for shape 256x1048576*(16x4^2)_NN_d
Tuning for shape 256x1048576*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x4^5  &  1.000 & 1.000 & 68.376 & 0.015
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16777216] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x F_4 [16, 4] x F_5 [16, 4] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 16777216, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16384*(16x4^1)_NN_d
Tuning for shape 1x65536*(16x4^2)_NN_d
Tuning for shape 1x262144*(16x4^3)_NN_d
Tuning for shape 1x1048576*(16x4^4)_NN_d
Tuning for shape 1x4194304*(16x4^5)_NN_d
Tuning for shape 1x16777216*(16x4^6)_NN_d
Tuning for shape 1x65536*(16x4^1)_NN_d
Tuning for shape 1x262144*(16x4^2)_NN_d
Tuning for shape 1x1048576*(16x4^3)_NN_d
Tuning for shape 1x4194304*(16x4^4)_NN_d
Tuning for shape 1x16777216*(16x4^5)_NN_d
Tuning for shape 1x262144*(16x4^1)_NN_d
Tuning for shape 1x1048576*(16x4^2)_NN_d
Tuning for shape 1x4194304*(16x4^3)_NN_d
Tuning for shape 1x16777216*(16x4^4)_NN_d
Tuning for shape 1x1048576*(16x4^1)_NN_d
Tuning for shape 1x4194304*(16x4^2)_NN_d
Tuning for shape 1x16777216*(16x4^3)_NN_d
Tuning for shape 1x4194304*(16x4^1)_NN_d
Tuning for shape 1x16777216*(16x4^2)_NN_d
Tuning for shape 1x16777216*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x4^6  &  1.000 & 1.000 & 211.937 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16777216] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x F_4 [16, 4] x F_5 [16, 4] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 16777216, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16384*(16x4^1)_NN_d
Tuning for shape 4x65536*(16x4^2)_NN_d
Tuning for shape 4x262144*(16x4^3)_NN_d
Tuning for shape 4x1048576*(16x4^4)_NN_d
Tuning for shape 4x4194304*(16x4^5)_NN_d
Tuning for shape 4x16777216*(16x4^6)_NN_d
Tuning for shape 4x65536*(16x4^1)_NN_d
Tuning for shape 4x262144*(16x4^2)_NN_d
Tuning for shape 4x1048576*(16x4^3)_NN_d
Tuning for shape 4x4194304*(16x4^4)_NN_d
Tuning for shape 4x16777216*(16x4^5)_NN_d
Tuning for shape 4x262144*(16x4^1)_NN_d
Tuning for shape 4x1048576*(16x4^2)_NN_d
Tuning for shape 4x4194304*(16x4^3)_NN_d
Tuning for shape 4x16777216*(16x4^4)_NN_d
Tuning for shape 4x1048576*(16x4^1)_NN_d
Tuning for shape 4x4194304*(16x4^2)_NN_d
Tuning for shape 4x16777216*(16x4^3)_NN_d
Tuning for shape 4x4194304*(16x4^1)_NN_d
Tuning for shape 4x16777216*(16x4^2)_NN_d
Tuning for shape 4x16777216*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x4^6  &  1.000 & 1.000 & 166.088 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16777216] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x F_4 [16, 4] x F_5 [16, 4] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 16777216, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16384*(16x4^1)_NN_d
Tuning for shape 16x65536*(16x4^2)_NN_d
Tuning for shape 16x262144*(16x4^3)_NN_d
Tuning for shape 16x1048576*(16x4^4)_NN_d
Tuning for shape 16x4194304*(16x4^5)_NN_d
Tuning for shape 16x16777216*(16x4^6)_NN_d
Tuning for shape 16x65536*(16x4^1)_NN_d
Tuning for shape 16x262144*(16x4^2)_NN_d
Tuning for shape 16x1048576*(16x4^3)_NN_d
Tuning for shape 16x4194304*(16x4^4)_NN_d
Tuning for shape 16x16777216*(16x4^5)_NN_d
Tuning for shape 16x262144*(16x4^1)_NN_d
Tuning for shape 16x1048576*(16x4^2)_NN_d
Tuning for shape 16x4194304*(16x4^3)_NN_d
Tuning for shape 16x16777216*(16x4^4)_NN_d
Tuning for shape 16x1048576*(16x4^1)_NN_d
Tuning for shape 16x4194304*(16x4^2)_NN_d
Tuning for shape 16x16777216*(16x4^3)_NN_d
Tuning for shape 16x4194304*(16x4^1)_NN_d
Tuning for shape 16x16777216*(16x4^2)_NN_d
Tuning for shape 16x16777216*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x4^6  &  1.000 & 1.000 & 166.696 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 16 -q 4 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 268435456] with F_0 [16, 4] x F_1 [16, 4] x F_2 [16, 4] x F_3 [16, 4] x F_4 [16, 4] x F_5 [16, 4] x F_6 [16, 4] x to produce Y[1, 16384]
Matmul: 1 x 16384 x 268435456, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x65536*(16x4^1)_NN_d
Tuning for shape 1x262144*(16x4^2)_NN_d
Tuning for shape 1x1048576*(16x4^3)_NN_d
Tuning for shape 1x4194304*(16x4^4)_NN_d
Tuning for shape 1x16777216*(16x4^5)_NN_d
Tuning for shape 1x67108864*(16x4^6)_NN_d
Tuning for shape 1x268435456*(16x4^7)_NN_d
Tuning for shape 1x262144*(16x4^1)_NN_d
Tuning for shape 1x1048576*(16x4^2)_NN_d
Tuning for shape 1x4194304*(16x4^3)_NN_d
Tuning for shape 1x16777216*(16x4^4)_NN_d
Tuning for shape 1x67108864*(16x4^5)_NN_d
Tuning for shape 1x268435456*(16x4^6)_NN_d
Tuning for shape 1x1048576*(16x4^1)_NN_d
Tuning for shape 1x4194304*(16x4^2)_NN_d
Tuning for shape 1x16777216*(16x4^3)_NN_d
Tuning for shape 1x67108864*(16x4^4)_NN_d
Tuning for shape 1x268435456*(16x4^5)_NN_d
Tuning for shape 1x4194304*(16x4^1)_NN_d
Tuning for shape 1x16777216*(16x4^2)_NN_d
Tuning for shape 1x67108864*(16x4^3)_NN_d
Tuning for shape 1x268435456*(16x4^4)_NN_d
Tuning for shape 1x16777216*(16x4^1)_NN_d
Tuning for shape 1x67108864*(16x4^2)_NN_d
Tuning for shape 1x268435456*(16x4^3)_NN_d
Tuning for shape 1x67108864*(16x4^1)_NN_d
Tuning for shape 1x268435456*(16x4^2)_NN_d
Tuning for shape 1x268435456*(16x4^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x4^7  &  1.000 & 1.000 & 243.522 & 0.004
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [16, 8] x to produce Y[1, 8]
Matmul: 1 x 8 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x8^1  &  1.000 & 1.000 & 0.001 & 1002.150
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [16, 8] x to produce Y[4, 8]
Matmul: 4 x 8 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x8^1  &  1.000 & 1.000 & 0.003 & 308.023
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [16, 8] x to produce Y[16, 8]
Matmul: 16 x 8 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x8^1  &  1.000 & 1.000 & 0.013 & 75.277
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [16, 8] x to produce Y[64, 8]
Matmul: 64 x 8 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x8^1  &  1.000 & 1.000 & 0.053 & 19.030
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [16, 8] x to produce Y[256, 8]
Matmul: 256 x 8 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x8^1  &  1.000 & 1.000 & 0.215 & 4.650
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [16, 8] x to produce Y[1024, 8]
Matmul: 1024 x 8 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x8^1  &  1.000 & 1.000 & 0.826 & 1.211
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [16, 8] x F_1 [16, 8] x to produce Y[1, 64]
Matmul: 1 x 64 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x128*(16x8^1)_NN_d
Tuning for shape 1x256*(16x8^2)_NN_d
Tuning for shape 1x256*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x8^2  &  1.000 & 1.000 & 0.015 & 68.528
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 2 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 256] with F_0 [16, 8] x F_1 [16, 8] x to produce Y[4, 64]
Matmul: 4 x 64 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x128*(16x8^1)_NN_d
Tuning for shape 4x256*(16x8^2)_NN_d
Tuning for shape 4x256*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x8^2  &  1.000 & 1.000 & 0.055 & 18.109
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 2 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 256] with F_0 [16, 8] x F_1 [16, 8] x to produce Y[16, 64]
Matmul: 16 x 64 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x128*(16x8^1)_NN_d
Tuning for shape 16x256*(16x8^2)_NN_d
Tuning for shape 16x256*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x8^2  &  1.000 & 1.000 & 0.223 & 4.474
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 2 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 256] with F_0 [16, 8] x F_1 [16, 8] x to produce Y[64, 64]
Matmul: 64 x 64 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x128*(16x8^1)_NN_d
Tuning for shape 64x256*(16x8^2)_NN_d
Tuning for shape 64x256*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x8^2  &  1.000 & 1.000 & 0.886 & 1.128
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 2 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 256] with F_0 [16, 8] x F_1 [16, 8] x to produce Y[256, 64]
Matmul: 256 x 64 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x128*(16x8^1)_NN_d
Tuning for shape 256x256*(16x8^2)_NN_d
Tuning for shape 256x256*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x8^2  &  1.000 & 1.000 & 3.282 & 0.305
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 2 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 256] with F_0 [16, 8] x F_1 [16, 8] x to produce Y[1024, 64]
Matmul: 1024 x 64 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x128*(16x8^1)_NN_d
Tuning for shape 1024x256*(16x8^2)_NN_d
Tuning for shape 1024x256*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x8^2  &  1.000 & 1.000 & 13.733 & 0.073
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 3 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 4096] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x to produce Y[1, 512]
Matmul: 1 x 512 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x1024*(16x8^1)_NN_d
Tuning for shape 1x2048*(16x8^2)_NN_d
Tuning for shape 1x4096*(16x8^3)_NN_d
Tuning for shape 1x2048*(16x8^1)_NN_d
Tuning for shape 1x4096*(16x8^2)_NN_d
Tuning for shape 1x4096*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x8^3  &  1.000 & 1.000 & 0.222 & 4.500
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 3 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 4096] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x to produce Y[4, 512]
Matmul: 4 x 512 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x1024*(16x8^1)_NN_d
Tuning for shape 4x2048*(16x8^2)_NN_d
Tuning for shape 4x4096*(16x8^3)_NN_d
Tuning for shape 4x2048*(16x8^1)_NN_d
Tuning for shape 4x4096*(16x8^2)_NN_d
Tuning for shape 4x4096*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x8^3  &  1.000 & 1.000 & 0.825 & 1.212
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 3 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 4096] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x to produce Y[16, 512]
Matmul: 16 x 512 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x1024*(16x8^1)_NN_d
Tuning for shape 16x2048*(16x8^2)_NN_d
Tuning for shape 16x4096*(16x8^3)_NN_d
Tuning for shape 16x2048*(16x8^1)_NN_d
Tuning for shape 16x4096*(16x8^2)_NN_d
Tuning for shape 16x4096*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x8^3  &  1.000 & 1.000 & 3.309 & 0.302
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 3 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 4096] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x to produce Y[64, 512]
Matmul: 64 x 512 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x1024*(16x8^1)_NN_d
Tuning for shape 64x2048*(16x8^2)_NN_d
Tuning for shape 64x4096*(16x8^3)_NN_d
Tuning for shape 64x2048*(16x8^1)_NN_d
Tuning for shape 64x4096*(16x8^2)_NN_d
Tuning for shape 64x4096*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x8^3  &  1.000 & 1.000 & 12.957 & 0.077
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 3 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 4096] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x to produce Y[256, 512]
Matmul: 256 x 512 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x1024*(16x8^1)_NN_d
Tuning for shape 256x2048*(16x8^2)_NN_d
Tuning for shape 256x4096*(16x8^3)_NN_d
Tuning for shape 256x2048*(16x8^1)_NN_d
Tuning for shape 256x4096*(16x8^2)_NN_d
Tuning for shape 256x4096*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x8^3  &  1.000 & 1.000 & 53.912 & 0.019
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 3 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 4096] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x to produce Y[1024, 512]
Matmul: 1024 x 512 x 4096, Num KP Factors: 3
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x1024*(16x8^1)_NN_d
Tuning for shape 1024x2048*(16x8^2)_NN_d
Tuning for shape 1024x4096*(16x8^3)_NN_d
Tuning for shape 1024x2048*(16x8^1)_NN_d
Tuning for shape 1024x4096*(16x8^2)_NN_d
Tuning for shape 1024x4096*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x8^3  &  1.000 & 1.000 & 208.840 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 4 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 65536] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x to produce Y[1, 4096]
Matmul: 1 x 4096 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x8192*(16x8^1)_NN_d
Tuning for shape 1x16384*(16x8^2)_NN_d
Tuning for shape 1x32768*(16x8^3)_NN_d
Tuning for shape 1x65536*(16x8^4)_NN_d
Tuning for shape 1x16384*(16x8^1)_NN_d
Tuning for shape 1x32768*(16x8^2)_NN_d
Tuning for shape 1x65536*(16x8^3)_NN_d
Tuning for shape 1x32768*(16x8^1)_NN_d
Tuning for shape 1x65536*(16x8^2)_NN_d
Tuning for shape 1x65536*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x8^4  &  1.000 & 1.000 & 3.159 & 0.317
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 4 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 65536] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x to produce Y[4, 4096]
Matmul: 4 x 4096 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x8192*(16x8^1)_NN_d
Tuning for shape 4x16384*(16x8^2)_NN_d
Tuning for shape 4x32768*(16x8^3)_NN_d
Tuning for shape 4x65536*(16x8^4)_NN_d
Tuning for shape 4x16384*(16x8^1)_NN_d
Tuning for shape 4x32768*(16x8^2)_NN_d
Tuning for shape 4x65536*(16x8^3)_NN_d
Tuning for shape 4x32768*(16x8^1)_NN_d
Tuning for shape 4x65536*(16x8^2)_NN_d
Tuning for shape 4x65536*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x8^4  &  1.000 & 1.000 & 12.209 & 0.082
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 4 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 65536] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x to produce Y[16, 4096]
Matmul: 16 x 4096 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x8192*(16x8^1)_NN_d
Tuning for shape 16x16384*(16x8^2)_NN_d
Tuning for shape 16x32768*(16x8^3)_NN_d
Tuning for shape 16x65536*(16x8^4)_NN_d
Tuning for shape 16x16384*(16x8^1)_NN_d
Tuning for shape 16x32768*(16x8^2)_NN_d
Tuning for shape 16x65536*(16x8^3)_NN_d
Tuning for shape 16x32768*(16x8^1)_NN_d
Tuning for shape 16x65536*(16x8^2)_NN_d
Tuning for shape 16x65536*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x8^4  &  1.000 & 1.000 & 47.564 & 0.021
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 4 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 65536] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x to produce Y[64, 4096]
Matmul: 64 x 4096 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x8192*(16x8^1)_NN_d
Tuning for shape 64x16384*(16x8^2)_NN_d
Tuning for shape 64x32768*(16x8^3)_NN_d
Tuning for shape 64x65536*(16x8^4)_NN_d
Tuning for shape 64x16384*(16x8^1)_NN_d
Tuning for shape 64x32768*(16x8^2)_NN_d
Tuning for shape 64x65536*(16x8^3)_NN_d
Tuning for shape 64x32768*(16x8^1)_NN_d
Tuning for shape 64x65536*(16x8^2)_NN_d
Tuning for shape 64x65536*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x8^4  &  1.000 & 1.000 & 183.881 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 4 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 65536] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x to produce Y[256, 4096]
Matmul: 256 x 4096 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x8192*(16x8^1)_NN_d
Tuning for shape 256x16384*(16x8^2)_NN_d
Tuning for shape 256x32768*(16x8^3)_NN_d
Tuning for shape 256x65536*(16x8^4)_NN_d
Tuning for shape 256x16384*(16x8^1)_NN_d
Tuning for shape 256x32768*(16x8^2)_NN_d
Tuning for shape 256x65536*(16x8^3)_NN_d
Tuning for shape 256x32768*(16x8^1)_NN_d
Tuning for shape 256x65536*(16x8^2)_NN_d
Tuning for shape 256x65536*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x8^4  &  1.000 & 1.000 & 177.198 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 4 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 65536] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x to produce Y[1024, 4096]
Matmul: 1024 x 4096 x 65536, Num KP Factors: 4
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x8192*(16x8^1)_NN_d
Tuning for shape 1024x16384*(16x8^2)_NN_d
Tuning for shape 1024x32768*(16x8^3)_NN_d
Tuning for shape 1024x65536*(16x8^4)_NN_d
Tuning for shape 1024x16384*(16x8^1)_NN_d
Tuning for shape 1024x32768*(16x8^2)_NN_d
Tuning for shape 1024x65536*(16x8^3)_NN_d
Tuning for shape 1024x32768*(16x8^1)_NN_d
Tuning for shape 1024x65536*(16x8^2)_NN_d
Tuning for shape 1024x65536*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x8^4  &  1.000 & 1.000 & 184.952 & 0.005
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 5 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 1048576] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x F_4 [16, 8] x to produce Y[1, 32768]
Matmul: 1 x 32768 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x65536*(16x8^1)_NN_d
Tuning for shape 1x131072*(16x8^2)_NN_d
Tuning for shape 1x262144*(16x8^3)_NN_d
Tuning for shape 1x524288*(16x8^4)_NN_d
Tuning for shape 1x1048576*(16x8^5)_NN_d
Tuning for shape 1x131072*(16x8^1)_NN_d
Tuning for shape 1x262144*(16x8^2)_NN_d
Tuning for shape 1x524288*(16x8^3)_NN_d
Tuning for shape 1x1048576*(16x8^4)_NN_d
Tuning for shape 1x262144*(16x8^1)_NN_d
Tuning for shape 1x524288*(16x8^2)_NN_d
Tuning for shape 1x1048576*(16x8^3)_NN_d
Tuning for shape 1x524288*(16x8^1)_NN_d
Tuning for shape 1x1048576*(16x8^2)_NN_d
Tuning for shape 1x1048576*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x8^5  &  1.000 & 1.000 & 45.190 & 0.022
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 5 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 1048576] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x F_4 [16, 8] x to produce Y[4, 32768]
Matmul: 4 x 32768 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x65536*(16x8^1)_NN_d
Tuning for shape 4x131072*(16x8^2)_NN_d
Tuning for shape 4x262144*(16x8^3)_NN_d
Tuning for shape 4x524288*(16x8^4)_NN_d
Tuning for shape 4x1048576*(16x8^5)_NN_d
Tuning for shape 4x131072*(16x8^1)_NN_d
Tuning for shape 4x262144*(16x8^2)_NN_d
Tuning for shape 4x524288*(16x8^3)_NN_d
Tuning for shape 4x1048576*(16x8^4)_NN_d
Tuning for shape 4x262144*(16x8^1)_NN_d
Tuning for shape 4x524288*(16x8^2)_NN_d
Tuning for shape 4x1048576*(16x8^3)_NN_d
Tuning for shape 4x524288*(16x8^1)_NN_d
Tuning for shape 4x1048576*(16x8^2)_NN_d
Tuning for shape 4x1048576*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x8^5  &  1.000 & 1.000 & 155.224 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 5 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 1048576] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x F_4 [16, 8] x to produce Y[16, 32768]
Matmul: 16 x 32768 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x65536*(16x8^1)_NN_d
Tuning for shape 16x131072*(16x8^2)_NN_d
Tuning for shape 16x262144*(16x8^3)_NN_d
Tuning for shape 16x524288*(16x8^4)_NN_d
Tuning for shape 16x1048576*(16x8^5)_NN_d
Tuning for shape 16x131072*(16x8^1)_NN_d
Tuning for shape 16x262144*(16x8^2)_NN_d
Tuning for shape 16x524288*(16x8^3)_NN_d
Tuning for shape 16x1048576*(16x8^4)_NN_d
Tuning for shape 16x262144*(16x8^1)_NN_d
Tuning for shape 16x524288*(16x8^2)_NN_d
Tuning for shape 16x1048576*(16x8^3)_NN_d
Tuning for shape 16x524288*(16x8^1)_NN_d
Tuning for shape 16x1048576*(16x8^2)_NN_d
Tuning for shape 16x1048576*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x8^5  &  1.000 & 1.000 & 319.191 & 0.003
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 5 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 1048576] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x F_4 [16, 8] x to produce Y[64, 32768]
Matmul: 64 x 32768 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x65536*(16x8^1)_NN_d
Tuning for shape 64x131072*(16x8^2)_NN_d
Tuning for shape 64x262144*(16x8^3)_NN_d
Tuning for shape 64x524288*(16x8^4)_NN_d
Tuning for shape 64x1048576*(16x8^5)_NN_d
Tuning for shape 64x131072*(16x8^1)_NN_d
Tuning for shape 64x262144*(16x8^2)_NN_d
Tuning for shape 64x524288*(16x8^3)_NN_d
Tuning for shape 64x1048576*(16x8^4)_NN_d
Tuning for shape 64x262144*(16x8^1)_NN_d
Tuning for shape 64x524288*(16x8^2)_NN_d
Tuning for shape 64x1048576*(16x8^3)_NN_d
Tuning for shape 64x524288*(16x8^1)_NN_d
Tuning for shape 64x1048576*(16x8^2)_NN_d
Tuning for shape 64x1048576*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x8^5  &  1.000 & 1.000 & 290.016 & 0.003
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 5 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 1048576] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x F_4 [16, 8] x to produce Y[256, 32768]
Matmul: 256 x 32768 x 1048576, Num KP Factors: 5
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x65536*(16x8^1)_NN_d
Tuning for shape 256x131072*(16x8^2)_NN_d
Tuning for shape 256x262144*(16x8^3)_NN_d
Tuning for shape 256x524288*(16x8^4)_NN_d
Tuning for shape 256x1048576*(16x8^5)_NN_d
Tuning for shape 256x131072*(16x8^1)_NN_d
Tuning for shape 256x262144*(16x8^2)_NN_d
Tuning for shape 256x524288*(16x8^3)_NN_d
Tuning for shape 256x1048576*(16x8^4)_NN_d
Tuning for shape 256x262144*(16x8^1)_NN_d
Tuning for shape 256x524288*(16x8^2)_NN_d
Tuning for shape 256x1048576*(16x8^3)_NN_d
Tuning for shape 256x524288*(16x8^1)_NN_d
Tuning for shape 256x1048576*(16x8^2)_NN_d
Tuning for shape 256x1048576*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x8^5  &  1.000 & 1.000 & 166.585 & 0.006
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 6 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16777216] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x F_4 [16, 8] x F_5 [16, 8] x to produce Y[1, 262144]
Matmul: 1 x 262144 x 16777216, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x524288*(16x8^1)_NN_d
Tuning for shape 1x1048576*(16x8^2)_NN_d
Tuning for shape 1x2097152*(16x8^3)_NN_d
Tuning for shape 1x4194304*(16x8^4)_NN_d
Tuning for shape 1x8388608*(16x8^5)_NN_d
Tuning for shape 1x16777216*(16x8^6)_NN_d
Tuning for shape 1x1048576*(16x8^1)_NN_d
Tuning for shape 1x2097152*(16x8^2)_NN_d
Tuning for shape 1x4194304*(16x8^3)_NN_d
Tuning for shape 1x8388608*(16x8^4)_NN_d
Tuning for shape 1x16777216*(16x8^5)_NN_d
Tuning for shape 1x2097152*(16x8^1)_NN_d
Tuning for shape 1x4194304*(16x8^2)_NN_d
Tuning for shape 1x8388608*(16x8^3)_NN_d
Tuning for shape 1x16777216*(16x8^4)_NN_d
Tuning for shape 1x4194304*(16x8^1)_NN_d
Tuning for shape 1x8388608*(16x8^2)_NN_d
Tuning for shape 1x16777216*(16x8^3)_NN_d
Tuning for shape 1x8388608*(16x8^1)_NN_d
Tuning for shape 1x16777216*(16x8^2)_NN_d
Tuning for shape 1x16777216*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x8^6  &  1.000 & 1.000 & 395.369 & 0.003
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 6 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16777216] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x F_4 [16, 8] x F_5 [16, 8] x to produce Y[4, 262144]
Matmul: 4 x 262144 x 16777216, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x524288*(16x8^1)_NN_d
Tuning for shape 4x1048576*(16x8^2)_NN_d
Tuning for shape 4x2097152*(16x8^3)_NN_d
Tuning for shape 4x4194304*(16x8^4)_NN_d
Tuning for shape 4x8388608*(16x8^5)_NN_d
Tuning for shape 4x16777216*(16x8^6)_NN_d
Tuning for shape 4x1048576*(16x8^1)_NN_d
Tuning for shape 4x2097152*(16x8^2)_NN_d
Tuning for shape 4x4194304*(16x8^3)_NN_d
Tuning for shape 4x8388608*(16x8^4)_NN_d
Tuning for shape 4x16777216*(16x8^5)_NN_d
Tuning for shape 4x2097152*(16x8^1)_NN_d
Tuning for shape 4x4194304*(16x8^2)_NN_d
Tuning for shape 4x8388608*(16x8^3)_NN_d
Tuning for shape 4x16777216*(16x8^4)_NN_d
Tuning for shape 4x4194304*(16x8^1)_NN_d
Tuning for shape 4x8388608*(16x8^2)_NN_d
Tuning for shape 4x16777216*(16x8^3)_NN_d
Tuning for shape 4x8388608*(16x8^1)_NN_d
Tuning for shape 4x16777216*(16x8^2)_NN_d
Tuning for shape 4x16777216*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x8^6  &  1.000 & 1.000 & 330.426 & 0.003
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 6 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16777216] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x F_4 [16, 8] x F_5 [16, 8] x to produce Y[16, 262144]
Matmul: 16 x 262144 x 16777216, Num KP Factors: 6
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x524288*(16x8^1)_NN_d
Tuning for shape 16x1048576*(16x8^2)_NN_d
Tuning for shape 16x2097152*(16x8^3)_NN_d
Tuning for shape 16x4194304*(16x8^4)_NN_d
Tuning for shape 16x8388608*(16x8^5)_NN_d
Tuning for shape 16x16777216*(16x8^6)_NN_d
Tuning for shape 16x1048576*(16x8^1)_NN_d
Tuning for shape 16x2097152*(16x8^2)_NN_d
Tuning for shape 16x4194304*(16x8^3)_NN_d
Tuning for shape 16x8388608*(16x8^4)_NN_d
Tuning for shape 16x16777216*(16x8^5)_NN_d
Tuning for shape 16x2097152*(16x8^1)_NN_d
Tuning for shape 16x4194304*(16x8^2)_NN_d
Tuning for shape 16x8388608*(16x8^3)_NN_d
Tuning for shape 16x16777216*(16x8^4)_NN_d
Tuning for shape 16x4194304*(16x8^1)_NN_d
Tuning for shape 16x8388608*(16x8^2)_NN_d
Tuning for shape 16x16777216*(16x8^3)_NN_d
Tuning for shape 16x8388608*(16x8^1)_NN_d
Tuning for shape 16x16777216*(16x8^2)_NN_d
Tuning for shape 16x16777216*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x8^6  &  1.000 & 1.000 & 331.629 & 0.003
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 7 -p 16 -q 8 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 268435456] with F_0 [16, 8] x F_1 [16, 8] x F_2 [16, 8] x F_3 [16, 8] x F_4 [16, 8] x F_5 [16, 8] x F_6 [16, 8] x to produce Y[1, 2097152]
Matmul: 1 x 2097152 x 268435456, Num KP Factors: 7
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x4194304*(16x8^1)_NN_d
Tuning for shape 1x8388608*(16x8^2)_NN_d
Tuning for shape 1x16777216*(16x8^3)_NN_d
Tuning for shape 1x33554432*(16x8^4)_NN_d
Tuning for shape 1x67108864*(16x8^5)_NN_d
Tuning for shape 1x134217728*(16x8^6)_NN_d
Tuning for shape 1x268435456*(16x8^7)_NN_d
Tuning for shape 1x8388608*(16x8^1)_NN_d
Tuning for shape 1x16777216*(16x8^2)_NN_d
Tuning for shape 1x33554432*(16x8^3)_NN_d
Tuning for shape 1x67108864*(16x8^4)_NN_d
Tuning for shape 1x134217728*(16x8^5)_NN_d
Tuning for shape 1x268435456*(16x8^6)_NN_d
Tuning for shape 1x16777216*(16x8^1)_NN_d
Tuning for shape 1x33554432*(16x8^2)_NN_d
Tuning for shape 1x67108864*(16x8^3)_NN_d
Tuning for shape 1x134217728*(16x8^4)_NN_d
Tuning for shape 1x268435456*(16x8^5)_NN_d
Tuning for shape 1x33554432*(16x8^1)_NN_d
Tuning for shape 1x67108864*(16x8^2)_NN_d
Tuning for shape 1x134217728*(16x8^3)_NN_d
Tuning for shape 1x268435456*(16x8^4)_NN_d
Tuning for shape 1x67108864*(16x8^1)_NN_d
Tuning for shape 1x134217728*(16x8^2)_NN_d
Tuning for shape 1x268435456*(16x8^3)_NN_d
Tuning for shape 1x134217728*(16x8^1)_NN_d
Tuning for shape 1x268435456*(16x8^2)_NN_d
Tuning for shape 1x268435456*(16x8^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x8^7  &  1.000 & 1.000 & 420.265 & 0.002
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 1 -p 16 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 16] with F_0 [16, 16] x to produce Y[1, 16]
Matmul: 1 x 16 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x16*(16x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x16^1  &  1.000 & 1.000 & 0.002 & 534.044
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 4 -n 1 -p 16 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[4, 16] with F_0 [16, 16] x to produce Y[4, 16]
Matmul: 4 x 16 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 4x16*(16x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
4_16x16^1  &  1.000 & 1.000 & 0.007 & 150.851
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 16 -n 1 -p 16 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[16, 16] with F_0 [16, 16] x to produce Y[16, 16]
Matmul: 16 x 16 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 16x16*(16x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
16_16x16^1  &  1.000 & 1.000 & 0.023 & 42.796
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 64 -n 1 -p 16 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[64, 16] with F_0 [16, 16] x to produce Y[64, 16]
Matmul: 64 x 16 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 64x16*(16x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
64_16x16^1  &  1.000 & 1.000 & 0.105 & 9.513
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 256 -n 1 -p 16 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[256, 16] with F_0 [16, 16] x to produce Y[256, 16]
Matmul: 256 x 16 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 256x16*(16x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
256_16x16^1  &  1.000 & 1.000 & 0.414 & 2.413
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1024 -n 1 -p 16 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1024, 16] with F_0 [16, 16] x to produce Y[1024, 16]
Matmul: 1024 x 16 x 16, Num KP Factors: 1
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1024x16*(16x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1024_16x16^1  &  1.000 & 1.000 & 1.722 & 0.581
Running cd build &&  ./tests/benchmarks/benchmark_cuda -m 1 -n 2 -p 16 -q 16 -r 10 -w 20 -t double --tune --opx N --opf N --backend cuda --fuse
 Doing KronMatmul of X[1, 256] with F_0 [16, 16] x F_1 [16, 16] x to produce Y[1, 256]
Matmul: 1 x 256 x 256, Num KP Factors: 2
setting values on host
values set
allocating
Found GPU 0
Tesla V100-PCIE-16GB
    Compute Capability      : 70
    SMs                     : 80
    Max Blocks per SM       : 32
    Max Threads per SM      : 2048
    Registers Per SM        : 65536
    Shared Memory per SM    : 98304
    Shared Memory Per Block : 49152
    Warp Size               : 32
allocated
memcpy
Tuning for shape 1x256*(16x16^1)_NN_d
Tuning for shape 1x256*(16x16^2)_NN_d
Tuning for shape 1x256*(16x16^1)_NN_d
Finding min execution time of the series
benchmark_cuda: /home/parasail/fastkron/src/autotuner/autotuner.cpp:50: float minExecTimeOfSeries(KMMProblem, uint, bool, TunedKernelsSeries&, TunedKernelsMap): Assertion `minTime < std::numeric_limits<float>::max()' failed.
Aborted (core dumped)
1_16x16^2  &  1.000 & 1.000 & 0.039 & 25.795
